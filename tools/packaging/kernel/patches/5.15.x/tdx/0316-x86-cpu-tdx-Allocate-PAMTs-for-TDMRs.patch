From a48814096f836d0eed325f7ecef9b2dde2f240c9 Mon Sep 17 00:00:00 2001
From: Kai Huang <kai.huang@intel.com>
Date: Tue, 28 Sep 2021 09:55:42 +1300
Subject: [PATCH 0316/1418] x86/cpu/tdx: Allocate PAMTs for TDMRs

TDX uses PAMT as metadata to track each TDMR page's status.  PAMTs are
allocated from TDX convertible memory and need to be put into TDMR's
reserved areas when they overlap with TDMR's address range.  However, TDX
only supports a limited number of reserved areas for each TDMR (currently
16), which makes reserved areas a precious resource.  PAMTs should take as
few reserved areas as possible (TDX memory block holes also occupy reserved
areas).

After distributing TDMRs across all TDMR ranges (finalizing address ranges
for all TDMRs), a common case is one large TDX memory block is covered by
multiple TDMRs.  It means PAMTs for those TDMRs will likely be allocated
from the same TDX memory block.  To minimize PAMT's usage of TDMR reserved
areas, allocate a big PAMT pool for those TDMRs.  PAMT for each TDMR can be
divided from the pool.

Specifically, allocate PAMT in the below steps:

  1) After distributing TDMRs, set up one TDX memory block as a candidate
     for PAMT allocation.
  2) For each TDX memory block, find all TDMRs that use it for PAMT
     allocation.  Then calculate the total PAMT size for those TDMRs, and
     allocate a large PAMT pool from this TDX memory block.
  3) For each TDMR, find the PAMT pool via TDX memory block, and divide
     PAMT from the pool.

Signed-off-by: Kai Huang <kai.huang@intel.com>
Signed-off-by: Isaku Yamahata <isaku.yamahata@intel.com>
---
 arch/x86/kernel/cpu/tdx/tdmr-common.c | 320 ++++++++++++++++++++++++++
 arch/x86/kernel/cpu/tdx/tdmr-common.h |  28 +++
 2 files changed, 348 insertions(+)

diff --git a/arch/x86/kernel/cpu/tdx/tdmr-common.c b/arch/x86/kernel/cpu/tdx/tdmr-common.c
index c427c401f8cf..ec07f0d0678f 100644
--- a/arch/x86/kernel/cpu/tdx/tdmr-common.c
+++ b/arch/x86/kernel/cpu/tdx/tdmr-common.c
@@ -439,6 +439,21 @@ static int __init construct_tdmrs_prepare(struct tdx_memory *tmem,
 
 static void __init construct_tdmrs_cleanup(struct tdx_memory *tmem)
 {
+	/*
+	 * Destroy PAMTs before destroying all TDX memory blocks, since
+	 * how PAMTs are allocated from TDX memory blocks are memory type
+	 * specific.
+	 */
+	while (!list_empty(&tmem->pamt_list)) {
+		struct tdx_pamt *pamt = list_first_entry(&tmem->pamt_list,
+				struct tdx_pamt, list);
+
+		list_del(&pamt->list);
+		pamt->tmb->ops->pamt_free(pamt->tmb, pamt->pamt_pfn,
+				pamt->total_pages);
+		kfree(pamt);
+	}
+
 	/* kfree() is NULL safe */
 	kfree(tmem->tdmr_array);
 	tmem->tdmr_array = NULL;
@@ -598,6 +613,301 @@ static int __init distribute_tdmrs_across_tdmr_ranges(
 	return 0;
 }
 
+/***************************** PAMT allocation *****************************/
+
+/*
+ * For given TDMR, among all TDX memory blocks (full or part) that are within
+ * the TDMR, find one TDX memory block as candidate for PAMT allocation.  So
+ * far just find the largest block as candidate.
+ */
+static void __init tdmr_setup_pamt_candidate(struct tdx_memory *tmem,
+		struct tdx_tdmr *tdmr)
+{
+	struct tdx_memblock *tmb, *largest_tmb = NULL;
+	unsigned long largest_tmb_pfn = 0;
+
+	list_for_each_entry(tmb, &tmem->tmb_list, list) {
+		unsigned long start_pfn = tmb->start_pfn;
+		unsigned long end_pfn = tmb->end_pfn;
+		unsigned long tmb_pfn;
+
+		/* Skip those fully below @tdmr */
+		if (TDX_MEMBLOCK_TDMR_END(tmb) <= tdmr->start_1g)
+			continue;
+
+		/* Skip those fully above @tdmr */
+		if (TDX_MEMBLOCK_TDMR_START(tmb) >= tdmr->end_1g)
+			break;
+
+		/* Only calculate size of the part that is within TDMR */
+		if (start_pfn < (tdmr->start_1g >> PAGE_SHIFT))
+			start_pfn = (tdmr->start_1g >> PAGE_SHIFT);
+		if (end_pfn > (tdmr->end_1g >> PAGE_SHIFT))
+			end_pfn = (tdmr->end_1g >> PAGE_SHIFT);
+
+		tmb_pfn = end_pfn - start_pfn;
+		if (largest_tmb_pfn < tmb_pfn) {
+			largest_tmb_pfn = tmb_pfn;
+			largest_tmb = tmb;
+		}
+	}
+
+	/*
+	 * There must be at least one block (or part of it) within one TDMR,
+	 * otherwise it is a bug.
+	 */
+	if (WARN_ON_ONCE(!largest_tmb))
+		largest_tmb = list_first_entry(&tmem->tmb_list,
+				struct tdx_memblock, list);
+
+	tdmr->tmb = largest_tmb;
+}
+
+/*
+ * First step of allocating PAMTs for TDMRs:
+ *
+ * Find one TDX memory block for each TDMR as candidate for PAMT allocation.
+ * After this, each TDMR will have one block for PAMT allocation, but the same
+ * block may be used by multiple TDMRs for PAMT allocation.
+ */
+static void __init tmem_setup_pamt_candidates(struct tdx_memory *tmem)
+{
+	int i;
+
+	for (i = 0; i < tmem->tdmr_num; i++)
+		tdmr_setup_pamt_candidate(tmem, &tmem->tdmr_array[i]);
+}
+
+/* Calculate PAMT size of one page size for one TDMR */
+static unsigned long __init tdmr_range_to_pamt_sz(phys_addr_t start_1g,
+		phys_addr_t end_1g, enum tdx_page_sz pgsz, int pamt_entry_sz)
+{
+	unsigned long pamt_sz;
+
+	pamt_sz = ((end_1g - start_1g) >> ((9 * pgsz) + PAGE_SHIFT)) *
+		pamt_entry_sz;
+	/* PAMT size must be 4K aligned */
+	pamt_sz = ALIGN(pamt_sz, PAGE_SIZE);
+
+	return pamt_sz;
+}
+
+/*
+ * Calculate PAMT size for one TDMR.  PAMTs for all supported page sizes are
+ * calculated together as a whole size, so caller can just allocate all PAMTs
+ * in one pamt_alloc() call.
+ */
+static unsigned long __init tdmr_get_pamt_sz(struct tdx_tdmr *tdmr,
+		int pamt_entry_sz_array[TDX_PG_MAX])
+{
+	enum tdx_page_sz pgsz;
+	unsigned long pamt_sz;
+
+	pamt_sz = 0;
+	for (pgsz = TDX_PG_4K; pgsz < TDX_PG_MAX; pgsz++) {
+		pamt_sz += tdmr_range_to_pamt_sz(tdmr->start_1g, tdmr->end_1g,
+				pgsz, pamt_entry_sz_array[pgsz]);
+	}
+
+	return pamt_sz;
+}
+
+/* Allocate one PAMT pool for all TDMRs that use given TDX memory block. */
+static int __init tmb_alloc_pamt_pool(struct tdx_memory *tmem,
+		struct tdx_memblock *tmb, int pamt_entry_sz_array[TDX_PG_MAX])
+{
+	struct tdx_pamt *pamt;
+	unsigned long pamt_pfn, pamt_sz;
+	int i;
+
+	/* Get all TDMRs that use the same @tmb as PAMT allocation */
+	pamt_sz = 0;
+	for (i = 0; i < tmem->tdmr_num; i++)  {
+		struct tdx_tdmr *tdmr = &tmem->tdmr_array[i];
+
+		if (tdmr->tmb != tmb)
+			continue;
+
+		pamt_sz += tdmr_get_pamt_sz(tdmr, pamt_entry_sz_array);
+	}
+
+	/*
+	 * If one TDMR range has multiple TDX memory blocks, it's possible
+	 * all TDMRs within this range use one block as PAMT candidate, in
+	 * which case other blocks won't be PAMT candidate for any TDMR.
+	 * Just skip in this case.
+	 */
+	if (!pamt_sz)
+		return 0;
+
+	pamt = kzalloc(sizeof(*pamt), GFP_KERNEL);
+	if (!pamt)
+		return -ENOMEM;
+
+	pamt_pfn = tmb->ops->pamt_alloc(tmb, pamt_sz >> PAGE_SHIFT);
+	if (!pamt_pfn) {
+		kfree(pamt);
+		return -ENOMEM;
+	}
+
+	INIT_LIST_HEAD(&pamt->list);
+	pamt->pamt_pfn = pamt_pfn;
+	pamt->total_pages = pamt_sz >> PAGE_SHIFT;
+	pamt->free_pages = pamt_sz >> PAGE_SHIFT;
+	/* In order to use tmb->ops->pamt_free() */
+	pamt->tmb = tmb;
+	/* Setup TDX memory block's PAMT pool */
+	tmb->pamt = pamt;
+	/*
+	 * Add PAMT to @tmem->pamt_list, so they can be easily freed before
+	 * freeing any TDX memory block.
+	 */
+	list_add_tail(&pamt->list, &tmem->pamt_list);
+
+	return 0;
+}
+
+/*
+ * Second step of allocating PAMTs for TDMRs:
+ *
+ * Allocate one PAMT pool for all TDMRs that use the same TDX memory block for
+ * PAMT allocation.  PAMT for each TDMR will later be divided from the pool.
+ * This helps to minimize number of PAMTs and reduce consumption of TDMR's
+ * reserved areas for PAMTs.
+ */
+static int __init tmem_alloc_pamt_pools(struct tdx_memory *tmem,
+		int pamt_entry_sz_array[TDX_PG_MAX])
+{
+	struct tdx_memblock *tmb;
+
+	list_for_each_entry(tmb, &tmem->tmb_list, list) {
+		int ret;
+
+		ret = tmb_alloc_pamt_pool(tmem, tmb, pamt_entry_sz_array);
+		/*
+		 * Just return in case of error.  PAMTs are freed in
+		 * tdx_memory_destroy() before freeing any TDX memory
+		 * blocks.
+		 */
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+/* Simple helper for getting PAMT from TDMR's large PAMT pool. */
+static unsigned long __init pamt_pool_alloc(struct tdx_pamt *pamt,
+		unsigned long npages)
+{
+	unsigned long pamt_pfn;
+
+	if (WARN_ON_ONCE(npages > pamt->free_pages))
+		return 0;
+
+	pamt_pfn = pamt->pamt_pfn + (pamt->total_pages - pamt->free_pages);
+	pamt->free_pages -= npages;
+
+	return pamt_pfn;
+}
+
+/* Set up PAMT for given TDMR from PAMT pool. */
+static void __init tdmr_setup_pamt(struct tdx_tdmr *tdmr,
+		int pamt_entry_sz_array[TDX_PG_MAX])
+{
+	unsigned long npages =
+		tdmr_get_pamt_sz(tdmr, pamt_entry_sz_array) >> PAGE_SHIFT;
+
+	tdmr->pamt_pfn = pamt_pool_alloc(tdmr->tmb->pamt, npages);
+}
+
+/*
+ * Third step of allocating PAMTs for TDMRs:
+ *
+ * Set up PAMTs for all TDMRs by dividing PAMTs from PAMT pools.
+ */
+static void __init tmem_setup_pamts(struct tdx_memory *tmem,
+		int pamt_entry_sz_array[TDX_PG_MAX])
+{
+	int i;
+
+	for (i = 0; i < tmem->tdmr_num; i++)
+		tdmr_setup_pamt(&tmem->tdmr_array[i], pamt_entry_sz_array);
+}
+
+/* Set up PAMT info in TDMR_INFO, which is used by TDX module. */
+static void __init tdmr_info_setup_pamt(struct tdx_tdmr *tdmr,
+		int pamt_entry_sz_array[TDX_PG_MAX],
+		struct tdmr_info *tdmr_info)
+{
+	unsigned long pamt_base_pgsz = tdmr->pamt_pfn << PAGE_SHIFT;
+	unsigned long pamt_base[TDX_PG_MAX];
+	unsigned long pamt_sz[TDX_PG_MAX];
+	enum tdx_page_sz pgsz;
+
+	for (pgsz = TDX_PG_4K; pgsz < TDX_PG_MAX; pgsz++) {
+		unsigned long sz = tdmr_range_to_pamt_sz(tdmr->start_1g,
+				tdmr->end_1g, pgsz, pamt_entry_sz_array[pgsz]);
+
+		pamt_base[pgsz] = pamt_base_pgsz;
+		pamt_sz[pgsz] = sz;
+
+		pamt_base_pgsz += sz;
+	}
+
+	tdmr_info->pamt_4k_base = pamt_base[TDX_PG_4K];
+	tdmr_info->pamt_4k_size = pamt_sz[TDX_PG_4K];
+	tdmr_info->pamt_2m_base = pamt_base[TDX_PG_2M];
+	tdmr_info->pamt_2m_size = pamt_sz[TDX_PG_2M];
+	tdmr_info->pamt_1g_base = pamt_base[TDX_PG_1G];
+	tdmr_info->pamt_1g_size = pamt_sz[TDX_PG_1G];
+}
+
+/*
+ * Final step of allocating PAMTs for TDMRs:
+ *
+ * Set up PAMT info for all TDMR_INFO structures.
+ */
+static void __init tmem_setup_tdmr_info_pamts(struct tdx_memory *tmem,
+		int pamt_entry_sz_array[TDX_PG_MAX],
+		struct tdmr_info *tdmr_info_array)
+{
+	int i;
+
+	for (i = 0; i < tmem->tdmr_num; i++)
+		tdmr_info_setup_pamt(&tmem->tdmr_array[i],
+				pamt_entry_sz_array,
+				&tdmr_info_array[i]);
+}
+
+/*
+ * Third step of constructing final TDMRs:
+ *
+ * Allocate PAMTs for distributed TDMRs in previous step, and set up PAMT info
+ * to TDMR_INFO array, which is used by TDX module.  Allocating PAMTs must be
+ * done after distributing all TDMRs on final TDX memory, since PAMT size
+ * depends on this.
+ */
+static int __init setup_pamts_across_tdmrs(struct tdx_memory *tmem,
+		int pamt_entry_sz_array[TDX_PG_MAX],
+		struct tdmr_info *tdmr_info_array)
+{
+	int ret;
+
+	tmem_setup_pamt_candidates(tmem);
+
+	ret = tmem_alloc_pamt_pools(tmem, pamt_entry_sz_array);
+	if (ret)
+		return ret;
+
+	tmem_setup_pamts(tmem, pamt_entry_sz_array);
+
+	tmem_setup_tdmr_info_pamts(tmem, pamt_entry_sz_array,
+			tdmr_info_array);
+
+	return 0;
+}
+
 /******************************* External APIs *****************************/
 
 /**
@@ -652,6 +962,7 @@ void __init tdx_memblock_free(struct tdx_memblock *tmb)
 void __init tdx_memory_init(struct tdx_memory *tmem)
 {
 	INIT_LIST_HEAD(&tmem->tmb_list);
+	INIT_LIST_HEAD(&tmem->pamt_list);
 }
 
 /**
@@ -827,6 +1138,15 @@ int __init tdx_memory_construct_tdmrs(struct tdx_memory *tmem,
 	if (ret)
 		goto construct_tdmrs_err;
 
+	/*
+	 * Allocate PAMTs for all TDMRs, and set up PAMT info in
+	 * all TDMR_INFO entries.
+	 */
+	ret = setup_pamts_across_tdmrs(tmem, desc->pamt_entry_size,
+			tdmr_info_array);
+	if (ret)
+		goto construct_tdmrs_err;
+
 	return 0;
 
 construct_tdmrs_err:
diff --git a/arch/x86/kernel/cpu/tdx/tdmr-common.h b/arch/x86/kernel/cpu/tdx/tdmr-common.h
index a2f7dca0190e..449bc0622c30 100644
--- a/arch/x86/kernel/cpu/tdx/tdmr-common.h
+++ b/arch/x86/kernel/cpu/tdx/tdmr-common.h
@@ -27,8 +27,32 @@ struct tdx_module_descriptor {
 struct tdx_memblock;
 struct tdx_memory;
 
+/*
+ * Structure to describe TDX PAMT.
+ *
+ * PAMT is physical contiguous memory used by TDX module to track each page in
+ * TDMR and crypto-protected by TDX module, and it (or part) needs to be put
+ * into TDMR's reserved area when it (or part) falls into TDMR.
+ */
+struct tdx_pamt {
+	struct list_head list;
+	unsigned long pamt_pfn;
+	unsigned long total_pages;
+	unsigned long free_pages;
+	struct tdx_memblock *tmb;
+};
+
 struct tdx_memblock_ops {
 	void (*tmb_free)(struct tdx_memblock *tmb);
+	/*
+	 * Allocate @npages TDX memory as PAMT.  @tmb can be where PAMT is
+	 * allocated from, or just a hit.
+	 */
+	unsigned long (*pamt_alloc)(struct tdx_memblock *tmb,
+			unsigned long npages);
+	/* Free PAMT allocated by pamt_alloc(). */
+	void (*pamt_free)(struct tdx_memblock *tmb, unsigned long pamt_pfn,
+			unsigned long npages);
 };
 
 /*
@@ -43,12 +67,15 @@ struct tdx_memblock {
 	int nid;
 	void *data;	/* TDX memory block type specific data */
 	struct tdx_memblock_ops *ops;
+	struct tdx_pamt *pamt;
 };
 
 /* Structure to describe one TDX TDMR. */
 struct tdx_tdmr {
 	phys_addr_t start_1g;
 	phys_addr_t end_1g;
+	unsigned long pamt_pfn;
+	struct tdx_memblock *tmb;	/* For PAMT allocation */
 };
 
 /*
@@ -60,6 +87,7 @@ struct tdx_memory {
 	struct list_head tmb_list;
 	struct tdx_tdmr *tdmr_array;
 	int tdmr_num;
+	struct list_head pamt_list;
 };
 
 struct tdx_memblock * __init tdx_memblock_create(unsigned long start_pfn,
-- 
2.31.1

