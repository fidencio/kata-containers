From 73bdc2d46f7cc247d867a37dd4958da195c5ddb4 Mon Sep 17 00:00:00 2001
From: Dave Jiang <dave.jiang@intel.com>
Date: Mon, 29 Nov 2021 14:08:53 -0700
Subject: [PATCH 0906/1418] vfio: idxd: wholesale copy over 5.12 PO kernel mdev
 driver

Signed-off-by: Dave Jiang <dave.jiang@intel.com>
---
 drivers/dma/idxd/cdev.c            |    4 +-
 drivers/dma/idxd/device.c          |  157 +-
 drivers/dma/idxd/idxd.h            |   45 +-
 drivers/dma/idxd/init.c            |   20 +
 drivers/dma/idxd/irq.c             |    2 +-
 drivers/dma/idxd/registers.h       |   28 +-
 drivers/dma/idxd/sysfs.c           |    7 +
 drivers/vfio/mdev/Kconfig          |    9 +
 drivers/vfio/mdev/Makefile         |    1 +
 drivers/vfio/mdev/idxd/Makefile    |    4 +
 drivers/vfio/mdev/idxd/mdev.c      | 2410 ++++++++++++++++++++++++++++
 drivers/vfio/mdev/idxd/mdev.h      |  179 +++
 drivers/vfio/mdev/idxd/mdev_host.c |   94 ++
 drivers/vfio/mdev/idxd/vdev.c      | 1397 ++++++++++++++++
 include/uapi/linux/idxd.h          |    2 +
 15 files changed, 4320 insertions(+), 39 deletions(-)
 create mode 100644 drivers/vfio/mdev/idxd/Makefile
 create mode 100644 drivers/vfio/mdev/idxd/mdev.c
 create mode 100644 drivers/vfio/mdev/idxd/mdev.h
 create mode 100644 drivers/vfio/mdev/idxd/mdev_host.c
 create mode 100644 drivers/vfio/mdev/idxd/vdev.c

diff --git a/drivers/dma/idxd/cdev.c b/drivers/dma/idxd/cdev.c
index b9b2b4a4124e..1f6a456149b8 100644
--- a/drivers/dma/idxd/cdev.c
+++ b/drivers/dma/idxd/cdev.c
@@ -158,7 +158,7 @@ static int idxd_cdev_release(struct inode *node, struct file *filep)
 			if (rc < 0)
 				dev_err(dev, "wq disable pasid failed.\n");
 		} else {
-			idxd_wq_drain(wq);
+			idxd_wq_drain(wq, NULL);
 		}
 	}
 
@@ -204,7 +204,7 @@ static int idxd_cdev_mmap(struct file *filp, struct vm_area_struct *vma)
 
 	vma->vm_flags |= VM_DONTCOPY;
 	pfn = (base + idxd_get_wq_portal_full_offset(wq->id,
-				IDXD_PORTAL_LIMITED)) >> PAGE_SHIFT;
+				IDXD_PORTAL_LIMITED, IDXD_IRQ_MSIX)) >> PAGE_SHIFT;
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	vma->vm_private_data = ctx;
 
diff --git a/drivers/dma/idxd/device.c b/drivers/dma/idxd/device.c
index e10d276f28fe..b381e7b7de1f 100644
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@ -178,22 +178,28 @@ void idxd_wq_free_resources(struct idxd_wq *wq)
 }
 EXPORT_SYMBOL_GPL(idxd_wq_free_resources);
 
-int idxd_wq_enable(struct idxd_wq *wq)
+int idxd_wq_enable(struct idxd_wq *wq, u32 *status)
 {
 	struct idxd_device *idxd = wq->idxd;
 	struct device *dev = &idxd->pdev->dev;
-	u32 status;
+	u32 stat;
+
+	if (status)
+		*status = 0;
 
 	if (wq->state == IDXD_WQ_ENABLED) {
 		dev_dbg(dev, "WQ %d already enabled\n", wq->id);
 		return -ENXIO;
 	}
 
-	idxd_cmd_exec(idxd, IDXD_CMD_ENABLE_WQ, wq->id, &status);
+	idxd_cmd_exec(idxd, IDXD_CMD_ENABLE_WQ, wq->id, &stat);
 
-	if (status != IDXD_CMDSTS_SUCCESS &&
-	    status != IDXD_CMDSTS_ERR_WQ_ENABLED) {
-		dev_dbg(dev, "WQ enable failed: %#x\n", status);
+	if (status)
+		*status = stat;
+
+	if (stat != IDXD_CMDSTS_SUCCESS &&
+	    stat != IDXD_CMDSTS_ERR_WQ_ENABLED) {
+		dev_dbg(dev, "WQ enable failed: %#x\n", stat);
 		return -ENXIO;
 	}
 
@@ -201,14 +207,26 @@ int idxd_wq_enable(struct idxd_wq *wq)
 	dev_dbg(dev, "WQ %d enabled\n", wq->id);
 	return 0;
 }
+EXPORT_SYMBOL_GPL(idxd_wq_enable);
 
-int idxd_wq_disable(struct idxd_wq *wq, bool reset_config)
+int idxd_wq_disable(struct idxd_wq *wq, bool reset_config, u32 *status)
 {
 	struct idxd_device *idxd = wq->idxd;
 	struct device *dev = &idxd->pdev->dev;
-	u32 status, operand;
+	u32 stat, operand;
 
 	dev_dbg(dev, "Disabling WQ %d\n", wq->id);
+	if (status)
+		*status = 0;
+
+	/*
+	 * When the wq is in LOCKED state, it means it is disabled but
+	 * also at the same time is "enabled" as far as the user is
+	 * concerned. So a call to disable the hardware can be
+	 * skipped.
+	 */
+	if (wq->state == IDXD_WQ_LOCKED)
+		goto out;
 
 	if (wq->state != IDXD_WQ_ENABLED) {
 		dev_dbg(dev, "WQ %d in wrong state: %d\n", wq->id, wq->state);
@@ -216,35 +234,59 @@ int idxd_wq_disable(struct idxd_wq *wq, bool reset_config)
 	}
 
 	operand = BIT(wq->id % 16) | ((wq->id / 16) << 16);
-	idxd_cmd_exec(idxd, IDXD_CMD_DISABLE_WQ, operand, &status);
+	idxd_cmd_exec(idxd, IDXD_CMD_DISABLE_WQ, operand, &stat);
+
+	if (status)
+		*status = stat;
 
-	if (status != IDXD_CMDSTS_SUCCESS) {
-		dev_dbg(dev, "WQ disable failed: %#x\n", status);
+	if (stat != IDXD_CMDSTS_SUCCESS) {
+		dev_dbg(dev, "WQ disable failed: %#x\n", stat);
 		return -ENXIO;
 	}
 
-	if (reset_config)
-		idxd_wq_disable_cleanup(wq);
-	wq->state = IDXD_WQ_DISABLED;
+out:
+	if (wq_dedicated(wq) && is_idxd_wq_mdev(wq)) {
+		wq->state = IDXD_WQ_LOCKED;
+	} else {
+		if (reset_config && test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+			idxd_wq_disable_cleanup(wq);
+		wq->state = IDXD_WQ_DISABLED;
+	}
 	dev_dbg(dev, "WQ %d disabled\n", wq->id);
 	return 0;
 }
+EXPORT_SYMBOL_GPL(idxd_wq_disable);
 
-void idxd_wq_drain(struct idxd_wq *wq)
+int idxd_wq_drain(struct idxd_wq *wq, u32 *status)
 {
 	struct idxd_device *idxd = wq->idxd;
 	struct device *dev = &idxd->pdev->dev;
-	u32 operand;
+	u32 operand, stat;
+
+	if (status)
+		*status = 0;
 
 	if (wq->state != IDXD_WQ_ENABLED) {
 		dev_dbg(dev, "WQ %d in wrong state: %d\n", wq->id, wq->state);
-		return;
+		return 0;
 	}
 
 	dev_dbg(dev, "Draining WQ %d\n", wq->id);
 	operand = BIT(wq->id % 16) | ((wq->id / 16) << 16);
-	idxd_cmd_exec(idxd, IDXD_CMD_DRAIN_WQ, operand, NULL);
+	idxd_cmd_exec(idxd, IDXD_CMD_DRAIN_WQ, operand, &stat);
+
+	if (status)
+		*status = stat;
+
+	if (stat != IDXD_CMDSTS_SUCCESS) {
+		dev_dbg(dev, "WQ drain failed: %#x\n", stat);
+		return -ENXIO;
+	}
+
+	dev_dbg(dev, "WQ %d drained\n", wq->id);
+	return 0;
 }
+EXPORT_SYMBOL_GPL(idxd_wq_drain);
 
 void idxd_wq_reset(struct idxd_wq *wq)
 {
@@ -262,6 +304,7 @@ void idxd_wq_reset(struct idxd_wq *wq)
 	idxd_wq_disable_cleanup(wq);
 	wq->state = IDXD_WQ_DISABLED;
 }
+EXPORT_SYMBOL_GPL(idxd_wq_reset);
 
 int idxd_wq_map_portal(struct idxd_wq *wq)
 {
@@ -271,7 +314,7 @@ int idxd_wq_map_portal(struct idxd_wq *wq)
 	resource_size_t start;
 
 	start = pci_resource_start(pdev, IDXD_WQ_BAR);
-	start += idxd_get_wq_portal_full_offset(wq->id, IDXD_PORTAL_LIMITED);
+	start += idxd_get_wq_portal_full_offset(wq->id, IDXD_PORTAL_LIMITED, IDXD_IRQ_MSIX);
 
 	wq->portal = devm_ioremap(dev, start, IDXD_PORTAL_SIZE);
 	if (!wq->portal)
@@ -301,6 +344,35 @@ void idxd_wqs_unmap_portal(struct idxd_device *idxd)
 	}
 }
 
+int idxd_wq_abort(struct idxd_wq *wq, u32 *status)
+{
+	struct idxd_device *idxd = wq->idxd;
+	struct device *dev = &idxd->pdev->dev;
+	u32 operand, stat;
+
+	dev_dbg(dev, "Abort WQ %d\n", wq->id);
+	if (wq->state != IDXD_WQ_ENABLED) {
+		dev_dbg(dev, "WQ %d not active\n", wq->id);
+		return -ENXIO;
+	}
+
+	operand = BIT(wq->id % 16) | ((wq->id / 16) << 16);
+	dev_dbg(dev, "cmd: %u operand: %#x\n", IDXD_CMD_ABORT_WQ, operand);
+	idxd_cmd_exec(idxd, IDXD_CMD_ABORT_WQ, operand, &stat);
+
+	if (status)
+		*status = stat;
+
+	if (stat != IDXD_CMDSTS_SUCCESS) {
+		dev_dbg(dev, "WQ abort failed: %#x\n", stat);
+		return -ENXIO;
+	}
+
+	dev_dbg(dev, "WQ %d aborted\n", wq->id);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(idxd_wq_abort);
+
 int idxd_wq_set_pasid(struct idxd_wq *wq, int pasid)
 {
 	struct idxd_device *idxd = wq->idxd;
@@ -308,7 +380,7 @@ int idxd_wq_set_pasid(struct idxd_wq *wq, int pasid)
 	union wqcfg wqcfg;
 	unsigned int offset;
 
-	rc = idxd_wq_disable(wq, false);
+	rc = idxd_wq_disable(wq, false, NULL);
 	if (rc < 0)
 		return rc;
 
@@ -320,12 +392,13 @@ int idxd_wq_set_pasid(struct idxd_wq *wq, int pasid)
 	iowrite32(wqcfg.bits[WQCFG_PASID_IDX], idxd->reg_base + offset);
 	spin_unlock(&idxd->dev_lock);
 
-	rc = idxd_wq_enable(wq);
+	rc = idxd_wq_enable(wq, NULL);
 	if (rc < 0)
 		return rc;
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(idxd_wq_set_pasid);
 
 int idxd_wq_disable_pasid(struct idxd_wq *wq)
 {
@@ -334,7 +407,7 @@ int idxd_wq_disable_pasid(struct idxd_wq *wq)
 	union wqcfg wqcfg;
 	unsigned int offset;
 
-	rc = idxd_wq_disable(wq, false);
+	rc = idxd_wq_disable(wq, false, NULL);
 	if (rc < 0)
 		return rc;
 
@@ -346,12 +419,13 @@ int idxd_wq_disable_pasid(struct idxd_wq *wq)
 	iowrite32(wqcfg.bits[WQCFG_PASID_IDX], idxd->reg_base + offset);
 	spin_unlock(&idxd->dev_lock);
 
-	rc = idxd_wq_enable(wq);
+	rc = idxd_wq_enable(wq, NULL);
 	if (rc < 0)
 		return rc;
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(idxd_wq_disable_pasid);
 
 static void idxd_wq_disable_cleanup(struct idxd_wq *wq)
 {
@@ -413,6 +487,34 @@ void idxd_wq_quiesce(struct idxd_wq *wq)
 }
 EXPORT_SYMBOL_GPL(idxd_wq_quiesce);
 
+void idxd_wq_setup_pasid(struct idxd_wq *wq, int pasid)
+{
+	struct idxd_device *idxd = wq->idxd;
+	int offset;
+
+	lockdep_assert_held(&idxd->dev_lock);
+
+	/* PASID fields are 8 bytes into the WQCFG register */
+	offset = WQCFG_OFFSET(idxd, wq->id, WQCFG_PASID_IDX);
+	wq->wqcfg->pasid = pasid;
+	iowrite32(wq->wqcfg->bits[WQCFG_PASID_IDX], idxd->reg_base + offset);
+}
+EXPORT_SYMBOL_GPL(idxd_wq_setup_pasid);
+
+void idxd_wq_setup_priv(struct idxd_wq *wq, int priv)
+{
+	struct idxd_device *idxd = wq->idxd;
+	int offset;
+
+	lockdep_assert_held(&idxd->dev_lock);
+
+	/* priv field is 8 bytes into the WQCFG register */
+	offset = WQCFG_OFFSET(idxd, wq->id, WQCFG_PRIV_IDX);
+	wq->wqcfg->priv = !!priv;
+	iowrite32(wq->wqcfg->bits[WQCFG_PRIV_IDX], idxd->reg_base + offset);
+}
+EXPORT_SYMBOL_GPL(idxd_wq_setup_priv);
+
 /* Device control bits */
 static inline bool idxd_is_enabled(struct idxd_device *idxd)
 {
@@ -581,6 +683,7 @@ void idxd_device_drain_pasid(struct idxd_device *idxd, int pasid)
 	idxd_cmd_exec(idxd, IDXD_CMD_DRAIN_PASID, operand, NULL);
 	dev_dbg(dev, "pasid %d drained\n", pasid);
 }
+EXPORT_SYMBOL_GPL(idxd_device_drain_pasid);
 
 int idxd_device_request_int_handle(struct idxd_device *idxd, int idx, int *handle,
 				   enum idxd_interrupt_type irq_type)
@@ -1257,7 +1360,7 @@ int __drv_enable_wq(struct idxd_wq *wq)
 	 */
 	wq->ie = &idxd->irq_entries[wq->id + 1];
 
-	rc = idxd_wq_enable(wq);
+	rc = idxd_wq_enable(wq, NULL);
 	if (rc < 0) {
 		dev_dbg(dev, "wq %d enabling failed: %d\n", wq->id, rc);
 		goto err;
@@ -1274,7 +1377,7 @@ int __drv_enable_wq(struct idxd_wq *wq)
 	return 0;
 
 err_map_portal:
-	rc = idxd_wq_disable(wq, false);
+	rc = idxd_wq_disable(wq, false, NULL);
 	if (rc < 0)
 		dev_dbg(dev, "wq %s disable failed\n", dev_name(wq_confdev(wq)));
 err:
@@ -1291,6 +1394,7 @@ int drv_enable_wq(struct idxd_wq *wq)
 	mutex_unlock(&wq->wq_lock);
 	return rc;
 }
+EXPORT_SYMBOL_GPL(drv_enable_wq);
 
 void __drv_disable_wq(struct idxd_wq *wq)
 {
@@ -1305,7 +1409,7 @@ void __drv_disable_wq(struct idxd_wq *wq)
 
 	idxd_wq_unmap_portal(wq);
 
-	idxd_wq_drain(wq);
+	idxd_wq_drain(wq, NULL);
 	idxd_wq_reset(wq);
 
 	wq->client_count = 0;
@@ -1318,6 +1422,7 @@ void drv_disable_wq(struct idxd_wq *wq)
 	__drv_disable_wq(wq);
 	mutex_unlock(&wq->wq_lock);
 }
+EXPORT_SYMBOL_GPL(drv_disable_wq);
 
 int idxd_device_drv_probe(struct idxd_dev *idxd_dev)
 {
diff --git a/drivers/dma/idxd/idxd.h b/drivers/dma/idxd/idxd.h
index 207283b074c3..62b6bf6e07ae 100644
--- a/drivers/dma/idxd/idxd.h
+++ b/drivers/dma/idxd/idxd.h
@@ -12,6 +12,7 @@
 #include <linux/pci.h>
 #include <linux/ioasid.h>
 #include <linux/perf_event.h>
+#include <linux/vfio_pci_core.h>
 #include <uapi/linux/idxd.h>
 #include "registers.h"
 
@@ -54,12 +55,17 @@ enum idxd_type {
 
 #define IDXD_ENQCMDS_RETRIES	32
 
+struct idxd_device_ops {
+	 void (*notify_error)(struct idxd_wq *wq);
+};
+
 struct idxd_device_driver {
 	const char *name;
 	enum idxd_dev_type *type;
 	int (*probe)(struct idxd_dev *idxd_dev);
 	void (*remove)(struct idxd_dev *idxd_dev);
 	struct device_driver drv;
+	struct idxd_device_ops *ops;
 };
 
 extern struct idxd_device_driver dsa_drv;
@@ -128,17 +134,20 @@ struct idxd_pmu {
 enum idxd_wq_state {
 	IDXD_WQ_DISABLED = 0,
 	IDXD_WQ_ENABLED,
+	IDXD_WQ_LOCKED,
 };
 
 enum idxd_wq_flag {
 	WQ_FLAG_DEDICATED = 0,
 	WQ_FLAG_BLOCK_ON_FAULT,
+	WQ_FLAG_MODE_1,
 };
 
 enum idxd_wq_type {
 	IDXD_WQT_NONE = 0,
 	IDXD_WQT_KERNEL,
 	IDXD_WQT_USER,
+	IDXD_WQT_MDEV,
 };
 
 struct idxd_cdev {
@@ -212,6 +221,7 @@ struct idxd_wq {
 	bool ats_dis;
 
 	void *private_data;
+	struct list_head vdcm_list;
 };
 
 struct idxd_engine {
@@ -242,6 +252,7 @@ enum idxd_device_flag {
 	IDXD_FLAG_CONFIGURABLE = 0,
 	IDXD_FLAG_CMD_RUNNING,
 	IDXD_FLAG_PASID_ENABLED,
+	IDXD_FLAG_IMS_SUPPORTED,
 };
 
 struct idxd_dma_dev {
@@ -285,6 +296,7 @@ struct idxd_device {
 	int irq_cnt;
 	bool request_int_handles;
 
+	u32 ims_offset;
 	u32 msix_perm_offset;
 	u32 wqcfg_offset;
 	u32 grpcfg_offset;
@@ -292,6 +304,7 @@ struct idxd_device {
 
 	u64 max_xfer_bytes;
 	u32 max_batch_size;
+	int ims_size;
 	int max_groups;
 	int max_engines;
 	int max_tokens;
@@ -309,6 +322,14 @@ struct idxd_device {
 	struct workqueue_struct *wq;
 	struct work_struct work;
 
+	struct irq_domain *ims_domain;
+	struct vfio_pci_core_device vfio_pdev;
+	struct kref mdev_kref;
+	struct mutex kref_lock;
+	bool mdev_host_init;
+	int *new_handles;
+	struct ida vdev_ida;
+
 	struct idxd_pmu *idxd_pmu;
 };
 
@@ -405,6 +426,11 @@ extern struct device_type idxd_wq_device_type;
 extern struct device_type idxd_engine_device_type;
 extern struct device_type idxd_group_device_type;
 
+static inline bool is_idxd_wq_mdev(struct idxd_wq *wq)
+{
+	return (wq->type == IDXD_WQT_MDEV);
+}
+
 static inline bool is_dsa_dev(struct idxd_dev *idxd_dev)
 {
 	return idxd_dev->type == IDXD_DEV_DSA;
@@ -472,15 +498,17 @@ enum idxd_interrupt_type {
 	IDXD_IRQ_IMS,
 };
 
-static inline int idxd_get_wq_portal_offset(enum idxd_portal_prot prot)
+static inline int idxd_get_wq_portal_offset(enum idxd_portal_prot prot,
+					    enum idxd_interrupt_type irq_type)
 {
-	return prot * 0x1000;
+	return prot * 0x1000 + irq_type * 0x2000;
 }
 
 static inline int idxd_get_wq_portal_full_offset(int wq_id,
-						 enum idxd_portal_prot prot)
+						 enum idxd_portal_prot prot,
+						 enum idxd_interrupt_type irq_type)
 {
-	return ((wq_id * 4) << PAGE_SHIFT) + idxd_get_wq_portal_offset(prot);
+	return ((wq_id * 4) << PAGE_SHIFT) + idxd_get_wq_portal_offset(prot, irq_type);
 }
 
 #define IDXD_PORTAL_MASK	(PAGE_SIZE - 1)
@@ -570,9 +598,9 @@ int idxd_device_release_int_handle(struct idxd_device *idxd, int handle,
 void idxd_wqs_unmap_portal(struct idxd_device *idxd);
 int idxd_wq_alloc_resources(struct idxd_wq *wq);
 void idxd_wq_free_resources(struct idxd_wq *wq);
-int idxd_wq_enable(struct idxd_wq *wq);
-int idxd_wq_disable(struct idxd_wq *wq, bool reset_config);
-void idxd_wq_drain(struct idxd_wq *wq);
+int idxd_wq_enable(struct idxd_wq *wq, u32 *status);
+int idxd_wq_disable(struct idxd_wq *wq, bool reset_config, u32 *status);
+int idxd_wq_drain(struct idxd_wq *wq, u32 *status);
 void idxd_wq_reset(struct idxd_wq *wq);
 int idxd_wq_map_portal(struct idxd_wq *wq);
 void idxd_wq_unmap_portal(struct idxd_wq *wq);
@@ -583,6 +611,9 @@ void idxd_wq_quiesce(struct idxd_wq *wq);
 int idxd_wq_init_percpu_ref(struct idxd_wq *wq);
 void idxd_wq_free_irq(struct idxd_wq *wq);
 int idxd_wq_enable_irq(struct idxd_wq *wq);
+int idxd_wq_abort(struct idxd_wq *wq, u32 *status);
+void idxd_wq_setup_pasid(struct idxd_wq *wq, int pasid);
+void idxd_wq_setup_priv(struct idxd_wq *wq, int priv);
 
 /* submission */
 int idxd_submit_desc(struct idxd_wq *wq, struct idxd_desc *desc);
diff --git a/drivers/dma/idxd/init.c b/drivers/dma/idxd/init.c
index e38de3315d66..7fc6e73a872b 100644
--- a/drivers/dma/idxd/init.c
+++ b/drivers/dma/idxd/init.c
@@ -179,6 +179,7 @@ static int idxd_setup_wqs(struct idxd_device *idxd)
 		init_waitqueue_head(&wq->err_queue);
 		init_completion(&wq->wq_dead);
 		init_completion(&wq->wq_resurrect);
+		INIT_LIST_HEAD(&wq->vdcm_list);
 		wq->max_xfer_bytes = WQ_DEFAULT_MAX_XFER;
 		wq->max_batch_size = WQ_DEFAULT_MAX_BATCH;
 		wq->enqcmds_retries = IDXD_ENQCMDS_RETRIES;
@@ -370,6 +371,24 @@ static void idxd_read_table_offsets(struct idxd_device *idxd)
 	dev_dbg(dev, "IDXD MSIX Permission Offset: %#x\n", idxd->msix_perm_offset);
 	idxd->perfmon_offset = offsets.perfmon * IDXD_TABLE_MULT;
 	dev_dbg(dev, "IDXD Perfmon Offset: %#x\n", idxd->perfmon_offset);
+	idxd->ims_offset = offsets.ims * IDXD_TABLE_MULT;
+	dev_dbg(dev, "IDXD IMS Offset: %#x\n", idxd->ims_offset);
+}
+
+static void idxd_check_ims(struct idxd_device *idxd)
+{
+	struct pci_dev *pdev = idxd->pdev;
+
+	/* verify that we have IMS vectors supported by device */
+	if (idxd->hw.gen_cap.max_ims_mult) {
+		idxd->ims_size = idxd->hw.gen_cap.max_ims_mult * 256ULL;
+		dev_dbg(&pdev->dev, "IMS size: %u\n", idxd->ims_size);
+		set_bit(IDXD_FLAG_IMS_SUPPORTED, &idxd->flags);
+		dev_dbg(&pdev->dev, "IMS supported for device\n");
+		return;
+	}
+
+	dev_dbg(&pdev->dev, "IMS unsupported for device\n");
 }
 
 static void idxd_read_caps(struct idxd_device *idxd)
@@ -394,6 +413,7 @@ static void idxd_read_caps(struct idxd_device *idxd)
 	dev_dbg(dev, "max xfer size: %llu bytes\n", idxd->max_xfer_bytes);
 	idxd->max_batch_size = 1U << idxd->hw.gen_cap.max_batch_shift;
 	dev_dbg(dev, "max batch size: %u\n", idxd->max_batch_size);
+	idxd_check_ims(idxd);
 	if (idxd->hw.gen_cap.config_en)
 		set_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags);
 
diff --git a/drivers/dma/idxd/irq.c b/drivers/dma/idxd/irq.c
index f822a980e7c1..38cc11e2e37f 100644
--- a/drivers/dma/idxd/irq.c
+++ b/drivers/dma/idxd/irq.c
@@ -52,7 +52,7 @@ static void idxd_device_reinit(struct work_struct *work)
 		struct idxd_wq *wq = idxd->wqs[i];
 
 		if (wq->state == IDXD_WQ_ENABLED) {
-			rc = idxd_wq_enable(wq);
+			rc = idxd_wq_enable(wq, NULL);
 			if (rc < 0) {
 				dev_warn(dev, "Unable to re-enable wq %s\n",
 					 dev_name(wq_confdev(wq)));
diff --git a/drivers/dma/idxd/registers.h b/drivers/dma/idxd/registers.h
index 8e396698c22b..411619caeece 100644
--- a/drivers/dma/idxd/registers.h
+++ b/drivers/dma/idxd/registers.h
@@ -90,6 +90,9 @@ struct opcap {
 	u64 bits[4];
 };
 
+#define OPCAP_OFS(op) (op - (0x40 * (op >> 6)))
+#define OPCAP_BIT(op) (BIT_ULL(OPCAP_OFS(op)))
+
 #define IDXD_OPCAP_OFFSET		0x40
 
 #define IDXD_TABLE_OFFSET		0x60
@@ -170,6 +173,7 @@ union idxd_command_reg {
 	};
 	u32 bits;
 } __packed;
+#define IDXD_CMD_INT_MASK		0x80000000
 
 enum idxd_cmd {
 	IDXD_CMD_ENABLE_DEVICE = 1,
@@ -186,6 +190,7 @@ enum idxd_cmd {
 	IDXD_CMD_ABORT_PASID,
 	IDXD_CMD_REQUEST_INT_HANDLE,
 	IDXD_CMD_RELEASE_INT_HANDLE,
+	IDXD_CMD_REVOKED_HANDLES_PROCESSED,
 };
 
 #define CMD_INT_HANDLE_IMS		0x10000
@@ -200,7 +205,8 @@ union cmdsts_reg {
 	};
 	u32 bits;
 } __packed;
-#define IDXD_CMDSTS_ACTIVE		0x80000000
+#define IDXD_CMDS_ACTIVE_BIT		31
+#define IDXD_CMDSTS_ACTIVE		BIT(IDXD_CMDS_ACTIVE_BIT)
 #define IDXD_CMDSTS_ERR_MASK		0xff
 #define IDXD_CMDSTS_RES_SHIFT		8
 
@@ -232,10 +238,11 @@ enum idxd_cmdsts_err {
 	/* disable device errors */
 	IDXD_CMDSTS_ERR_DIS_DEV_EN = 0x31,
 	/* disable WQ, drain WQ, abort WQ, reset WQ */
-	IDXD_CMDSTS_ERR_DEV_NOT_EN,
+	IDXD_CMDSTS_ERR_WQ_NOT_EN,
 	/* request interrupt handle */
 	IDXD_CMDSTS_ERR_INVAL_INT_IDX = 0x41,
 	IDXD_CMDSTS_ERR_NO_HANDLE,
+	IDXD_CMDSTS_ERR_INVAL_INT_IDX_RELEASE,
 };
 
 #define IDXD_CMDCAP_OFFSET		0xb0
@@ -283,6 +290,11 @@ union msix_perm {
 	u32 bits;
 } __packed;
 
+#define IDXD_MSIX_PERM_MASK	0xfffff00c
+#define IDXD_MSIX_PERM_IGNORE	0x3
+#define MSIX_ENTRY_MASK_INT	0x1
+#define MSIX_ENTRY_CTRL_BYTE	12
+
 union group_flags {
 	struct {
 		u32 tc_a:3;
@@ -352,9 +364,19 @@ union wqcfg {
 	u32 bits[8];
 } __packed;
 
-#define WQCFG_PASID_IDX                2
+enum idxd_wq_hw_state {
+	IDXD_WQ_DEV_DISABLED = 0,
+	IDXD_WQ_DEV_ENABLED,
+	IDXD_WQ_DEV_BUSY,
+};
+
+#define WQCFG_PASID_IDX		2
+#define WQCFG_PRIV_IDX		2
 #define WQCFG_OCCUP_IDX		6
 
+#define WQCFG_MODE_DEDICATED	1
+#define WQCFG_MODE_SHARED	0
+
 #define WQCFG_OCCUP_MASK	0xffff
 
 /*
diff --git a/drivers/dma/idxd/sysfs.c b/drivers/dma/idxd/sysfs.c
index 1af8bf5c9b18..61351951bdc9 100644
--- a/drivers/dma/idxd/sysfs.c
+++ b/drivers/dma/idxd/sysfs.c
@@ -14,6 +14,7 @@ static char *idxd_wq_type_names[] = {
 	[IDXD_WQT_NONE]		= "none",
 	[IDXD_WQT_KERNEL]	= "kernel",
 	[IDXD_WQT_USER]		= "user",
+	[IDXD_WQT_MDEV]		= "mdev",
 };
 
 /* IDXD engine attributes */
@@ -438,6 +439,8 @@ static ssize_t wq_state_show(struct device *dev,
 		return sysfs_emit(buf, "disabled\n");
 	case IDXD_WQ_ENABLED:
 		return sysfs_emit(buf, "enabled\n");
+	case IDXD_WQ_LOCKED:
+		return sysfs_emit(buf, "locked\n");
 	}
 
 	return sysfs_emit(buf, "unknown\n");
@@ -718,6 +721,8 @@ static ssize_t wq_type_show(struct device *dev,
 		return sysfs_emit(buf, "%s\n", idxd_wq_type_names[IDXD_WQT_KERNEL]);
 	case IDXD_WQT_USER:
 		return sysfs_emit(buf, "%s\n", idxd_wq_type_names[IDXD_WQT_USER]);
+	case IDXD_WQT_MDEV:
+		return sysfs_emit(buf, "%s\n", idxd_wq_type_names[IDXD_WQT_MDEV]);
 	case IDXD_WQT_NONE:
 	default:
 		return sysfs_emit(buf, "%s\n", idxd_wq_type_names[IDXD_WQT_NONE]);
@@ -743,6 +748,8 @@ static ssize_t wq_type_store(struct device *dev,
 		wq->type = IDXD_WQT_KERNEL;
 	else if (sysfs_streq(buf, idxd_wq_type_names[IDXD_WQT_USER]))
 		wq->type = IDXD_WQT_USER;
+	else if (sysfs_streq(buf, idxd_wq_type_names[IDXD_WQT_MDEV]))
+		wq->type = IDXD_WQT_MDEV;
 	else
 		return -EINVAL;
 
diff --git a/drivers/vfio/mdev/Kconfig b/drivers/vfio/mdev/Kconfig
index 646dbed44eb2..6b2fa8a194ef 100644
--- a/drivers/vfio/mdev/Kconfig
+++ b/drivers/vfio/mdev/Kconfig
@@ -8,3 +8,12 @@ config VFIO_MDEV
 	  See Documentation/driver-api/vfio-mediated-device.rst for more details.
 
 	  If you don't know what do here, say N.
+
+config VFIO_MDEV_IDXD
+	tristate "VFIO Mediated device driver for Intel IDXD"
+	depends on VFIO && VFIO_MDEV && X86_64
+	select IMS_MSI_ARRAY
+	default n
+	help
+	  VFIO based mediated device driver for
+	  Intel Accelerator Devices driver.
diff --git a/drivers/vfio/mdev/Makefile b/drivers/vfio/mdev/Makefile
index ff9ecd802125..a2660c3edf5b 100644
--- a/drivers/vfio/mdev/Makefile
+++ b/drivers/vfio/mdev/Makefile
@@ -3,3 +3,4 @@
 mdev-y := mdev_core.o mdev_sysfs.o mdev_driver.o vfio_mdev.o
 
 obj-$(CONFIG_VFIO_MDEV) += mdev.o
+obj-$(CONFIG_VFIO_MDEV_IDXD) += idxd/
diff --git a/drivers/vfio/mdev/idxd/Makefile b/drivers/vfio/mdev/idxd/Makefile
new file mode 100644
index 000000000000..ab55032b5486
--- /dev/null
+++ b/drivers/vfio/mdev/idxd/Makefile
@@ -0,0 +1,4 @@
+ccflags-y += -I$(srctree)/drivers/dma/idxd -DDEFAULT_SYMBOL_NAMESPACE=IDXD
+
+obj-$(CONFIG_VFIO_MDEV_IDXD) += idxd_mdev.o
+idxd_mdev-y := mdev.o vdev.o mdev_host.o
diff --git a/drivers/vfio/mdev/idxd/mdev.c b/drivers/vfio/mdev/idxd/mdev.c
new file mode 100644
index 000000000000..69ba0f67052a
--- /dev/null
+++ b/drivers/vfio/mdev/idxd/mdev.c
@@ -0,0 +1,2410 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2019,2020 Intel Corporation. All rights rsvd. */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/device.h>
+#include <linux/sched/task.h>
+#include <linux/sched/mm.h>
+#include <linux/io-64-nonatomic-lo-hi.h>
+#include <linux/mm.h>
+#include <linux/mmu_context.h>
+#include <linux/vfio.h>
+#include <linux/mdev.h>
+#include <linux/msi.h>
+#include <linux/intel-iommu.h>
+#include <linux/intel-svm.h>
+#include <linux/kvm_host.h>
+#include <linux/eventfd.h>
+#include <linux/circ_buf.h>
+#include <linux/irqchip/irq-ims-msi.h>
+#include <uapi/linux/idxd.h>
+#include "registers.h"
+#include "idxd.h"
+#include "../mdev_private.h"
+#include "mdev.h"
+
+static u64 idxd_pci_config[] = {
+	0x0010000000008086ULL,
+	0x0080000008800000ULL,
+	0x000000000000000cULL,
+	0x000000000000000cULL,
+	0x0000000000000000ULL,
+	0x2010808600000000ULL,
+	0x0000004000000000ULL,
+	0x000000ff00000000ULL,
+	0x0000060000015011ULL, /* MSI-X capability, hardcoded 2 entries, Encoded as N-1 */
+	0x0000070000000000ULL,
+	0x0000000000920010ULL, /* PCIe capability */
+	0x0000000000000000ULL,
+	0x0000000000000000ULL,
+	0x0000000000000000ULL,
+	0x0070001000000000ULL,
+	0x0000000000000000ULL,
+	0x0000000000000000ULL,
+	0x0000000000000000ULL,
+};
+
+static u64 idxd_pci_ext_cap[] = {
+	0x000000611101000fULL, /* ATS capability */
+	0x0000000000000000ULL,
+	0x8100000012010013ULL, /* Page Request capability */
+	0x0000000000000001ULL,
+	0x000014040001001bULL, /* PASID capability */
+	0x0000000000000000ULL,
+	0x0181808600010023ULL, /* Scalable IOV capability */
+	0x0000000100000005ULL,
+	0x0000000000000001ULL,
+	0x0000000000000000ULL,
+};
+
+static int idxd_vdcm_set_irqs(struct vdcm_idxd *vidxd, uint32_t flags,
+			      unsigned int index, unsigned int start,
+			      unsigned int count, void *data);
+static int vidxd_register_ioasid_notifier(struct vdcm_idxd *vidxd);
+
+struct idxd_ioasid_work {
+	struct work_struct work;
+	struct idxd_wq *wq;
+	u32 guest_pasid;
+	u32 host_pasid;
+};
+
+static const char idxd_dsa_1dwq_name[] = "dsa-1dwq-v1";
+static const char idxd_iax_1dwq_name[] = "iax-1dwq-v1";
+static const char idxd_dsa_1swq_name[] = "dsa-1swq-v1";
+static const char idxd_iax_1swq_name[] = "iax-1swq-v1";
+
+static int idxd_vdcm_get_irq_count(struct mdev_device *mdev, int type)
+{
+	struct vdcm_idxd *vidxd = mdev_get_drvdata(mdev);
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+
+	/*
+	 * Even though the number of MSIX vectors supported are not tied to number of
+	 * wqs being exported, the current design is to allow 1 vector per WQ for guest.
+	 * So here we end up with num of wqs plus 1 that handles the misc interrupts.
+	 */
+	if (type == VFIO_PCI_MSI_IRQ_INDEX || type == VFIO_PCI_MSIX_IRQ_INDEX)
+		return VIDXD_MAX_MSIX_VECS;
+	else if (type == VFIO_PCI_REQ_IRQ_INDEX)
+		return 1;
+	else if (type >= VFIO_PCI_NUM_IRQS &&
+		 type < VFIO_PCI_NUM_IRQS + vfio_pdev->num_ext_irqs)
+		return 1;
+
+	return 0;
+}
+
+static void idxd_wq_ioasid_work(struct work_struct *work)
+{
+	struct idxd_ioasid_work *iwork = container_of(work, struct idxd_ioasid_work, work);
+	struct idxd_wq *wq = iwork->wq;
+
+	if (wq->state != IDXD_WQ_ENABLED)
+		return;
+
+	idxd_device_drain_pasid(wq->idxd, iwork->guest_pasid);
+	ioasid_put(NULL, iwork->host_pasid);
+	kfree(iwork);
+}
+
+static int idxd_mdev_ioasid_event(struct notifier_block *nb, unsigned long event, void *data)
+{
+	struct idxd_vdev *vdev = container_of(nb, struct idxd_vdev, pasid_nb);
+	struct mdev_device *mdev = vdev->mdev;
+	struct vdcm_idxd *vidxd = mdev_get_drvdata(mdev);
+	struct idxd_wq *wq = vidxd->wq;
+	struct ioasid_nb_args *args = (struct ioasid_nb_args *)data;
+	struct idxd_ioasid_work *iwork;
+
+	if (event == IOASID_NOTIFY_FREE) {
+		dev_dbg(mdev_dev(mdev), "ioasid free event\n");
+
+		if (wq_dedicated(wq))
+			return NOTIFY_DONE;
+
+		if (wq->state != IDXD_WQ_ENABLED)
+			return NOTIFY_DONE;
+
+		iwork = kmalloc(sizeof(*iwork), GFP_ATOMIC);
+		if (!iwork)
+			return notifier_from_errno(-ENOMEM);
+		iwork->wq = wq;
+		iwork->guest_pasid = args->spid;
+		iwork->host_pasid = args->id;
+		INIT_WORK(&iwork->work, idxd_wq_ioasid_work);
+		ioasid_queue_work(&iwork->work);
+		return NOTIFY_OK;
+	}
+
+	return NOTIFY_OK;
+}
+
+int idxd_mdev_get_pasid(struct mdev_device *mdev, u32 *pasid)
+{
+	struct vfio_group *vfio_group;
+	struct iommu_domain *iommu_domain;
+	struct device *dev = mdev_dev(mdev);
+	struct device *iommu_device = mdev_get_iommu_device(mdev);
+	struct vdcm_idxd *vidxd = mdev_get_drvdata(mdev);
+	int mdev_pasid;
+
+	if (!vidxd->ivdev.vfio_group) {
+		dev_warn(dev, "Missing vfio_group.\n");
+		return -EINVAL;
+	}
+
+	vfio_group = vidxd->ivdev.vfio_group;
+
+	iommu_domain = vfio_group_iommu_domain(vfio_group);
+	if (IS_ERR_OR_NULL(iommu_domain))
+		goto err;
+
+	mdev_pasid = iommu_aux_get_pasid(iommu_domain, iommu_device);
+	if (mdev_pasid < 0)
+		goto err;
+
+	*pasid = (u32)mdev_pasid;
+	return 0;
+
+ err:
+	vfio_group_put_external_user(vfio_group);
+	vidxd->ivdev.vfio_group = NULL;
+	return -EFAULT;
+}
+
+int idxd_mdev_get_host_pasid(struct mdev_device *mdev, u32 gpasid, u32 *pasid)
+{
+	struct ioasid_set *ioasid_set;
+	struct mm_struct *mm;
+
+	mm = get_task_mm(current);
+	if (!mm)
+		return -ENXIO;
+
+	ioasid_set = ioasid_find_mm_set(mm);
+	if (!ioasid_set) {
+		mmput(mm);
+		return -ENXIO;
+	}
+
+	*pasid = ioasid_find_by_spid(ioasid_set, gpasid, true);
+	mmput(mm);
+	if (*pasid == INVALID_IOASID)
+		return -ENXIO;
+
+	return 0;
+}
+
+static inline void reset_vconfig(struct vdcm_idxd *vidxd)
+{
+	u16 *devid = (u16 *)(vidxd->cfg + PCI_DEVICE_ID);
+	struct idxd_device *idxd = vidxd->idxd;
+
+	memset(vidxd->cfg, 0, VIDXD_MAX_CFG_SPACE_SZ);
+	memcpy(vidxd->cfg, idxd_pci_config, sizeof(idxd_pci_config));
+
+	if (idxd->data->type == IDXD_TYPE_DSA)
+		*devid = PCI_DEVICE_ID_INTEL_DSA_SPR0;
+	else if (idxd->data->type == IDXD_TYPE_IAX)
+		*devid = PCI_DEVICE_ID_INTEL_IAX_SPR0;
+
+	memcpy(vidxd->cfg + 0x100, idxd_pci_ext_cap, sizeof(idxd_pci_ext_cap));
+}
+
+static inline void reset_vmmio(struct vdcm_idxd *vidxd)
+{
+	memset(&vidxd->bar0, 0, VIDXD_MAX_MMIO_SPACE_SZ);
+}
+
+static void idxd_vdcm_init(struct vdcm_idxd *vidxd)
+{
+	struct idxd_wq *wq = vidxd->wq;
+
+	INIT_LIST_HEAD(&vidxd->vwq.head);
+
+	reset_vconfig(vidxd);
+	reset_vmmio(vidxd);
+
+	vidxd->bar_size[0] = VIDXD_BAR0_SIZE;
+	vidxd->bar_size[1] = VIDXD_BAR2_SIZE;
+
+	vidxd_mmio_init(vidxd);
+
+	if (wq_dedicated(wq) && wq->state == IDXD_WQ_ENABLED) {
+		idxd_wq_disable(wq, false, NULL);
+		wq->state = IDXD_WQ_LOCKED;
+	}
+}
+
+static void  vidxd_unregister_ioasid_notifier(struct vdcm_idxd *vidxd)
+{
+	struct idxd_vdev *vdev = &vidxd->ivdev;
+	struct ioasid_mm_entry *mm_entry, *n;
+	struct mm_struct *mm;
+
+	mm = get_task_mm(current);
+	if (!mm)
+		return;
+
+	mutex_lock(&vdev->ioasid_lock);
+
+	list_for_each_entry_safe(mm_entry, n, &vdev->mm_list, node) {
+		if (mm_entry->mm == mm) {
+			list_del(&mm_entry->node);
+			kfree(mm_entry);
+			ioasid_unregister_notifier_mm(mm, &vidxd->ivdev.pasid_nb);
+			break;
+		}
+	}
+
+	mutex_unlock(&vdev->ioasid_lock);
+	mmput(mm);
+}
+
+static int vidxd_source_pause_device(struct vdcm_idxd *vidxd)
+{
+	int i;
+	int rc;
+	u32 status;
+
+	if (vidxd->paused)
+		return 0;
+
+	mutex_lock(&vidxd->mig_submit_lock);
+	/* The VMM is expected to have unmap the portals. So once we drain
+	 * there shouldn't be any work directly submited from the VM */
+	vidxd->paused = true;
+	mutex_unlock(&vidxd->mig_submit_lock);
+
+	/* For DWQs, pausing the vDSA can always be done by Drain WQ command.
+	 * For SWQs, pausing the vDSA may mean Drain PASID if the SWQ is shared
+	 * with other VMs. We will need to do Drain PASID for each PASID
+	 * allocated to the VM which may take a long time. As an optimization,
+	 * we may do Drain PASID if no of PASIDs for the VM is below certain
+	 * number and do Drain WQ otherwise.
+	 */
+	/* Drain WQ(s) to make sure no more outstanding work in the dev */
+	/* TODO: Currently support for only 1 WQ per VDev */
+	for (i = 0; i < vidxd->num_wqs; i++) {
+		rc = idxd_wq_drain(vidxd->wq, &status);
+
+		if (rc < 0) {
+			pr_info("%s: failed rc %d\n", __func__, rc);
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static void vidxd_free_resources (struct vdcm_idxd *vidxd)
+{
+	int i;
+
+        /* Free the queued descriptors */
+        for (i = 0; i < vidxd->num_wqs; i++) {
+                struct idxd_wq_desc_elem *el, *tmp;
+		struct idxd_virtual_wq *vwq = &vidxd->vwq;
+
+                list_for_each_entry_safe(el, tmp, &vwq->head, link) {
+                        list_del(&el->link);
+                        vwq->ndescs--;
+                        kfree(el);
+                }
+        }
+
+}
+
+static void vidxd_source_prepare_for_migration(struct vdcm_idxd *vidxd)
+{
+	int i;
+	struct vfio_pci_core_device *vdev = &vidxd->vfio_pdev;
+	struct vfio_device_migration_info *mig_info =
+		(struct vfio_device_migration_info *)vdev->mig_pages;
+	u8 *data_ptr = (u8 *)vdev->mig_pages;
+	unsigned int offset =  mig_info->data_offset;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct idxd_virtual_wq *vwq;
+
+	memcpy(data_ptr + offset, vidxd->cfg, sizeof(vidxd->cfg));
+	offset += sizeof(vidxd->cfg);
+	memcpy(data_ptr + offset, (u8 *)vidxd->bar_val, sizeof(vidxd->bar_val));
+	offset += sizeof(vidxd->bar_val);
+	memcpy(data_ptr + offset, (u8 *)vidxd->bar_size,
+					sizeof(vidxd->bar_size));
+	offset += sizeof(vidxd->bar_size);
+	memcpy(data_ptr + offset, (u8 *)&vidxd->bar0, sizeof(vidxd->bar0));
+	offset += sizeof(vidxd->bar0);
+
+	/* Save the queued descriptors */
+	for (i = 0; i < vidxd->num_wqs; i++) {
+		struct idxd_wq_desc_elem *el;
+
+		vwq = &vidxd->vwq;
+		memcpy(data_ptr + offset, (u8 *)&vwq->ndescs, sizeof(vwq->ndescs));
+		offset += sizeof(vwq->ndescs);
+		list_for_each_entry(el, &vwq->head, link) {
+			dev_dbg(dev, "Saving descriptor at offset %x\n", offset);
+			memcpy(data_ptr + offset, (u8 *)el, sizeof(*el));
+			offset += sizeof(*el);
+		}
+	}
+
+	/* Save int handle info */
+	for (i = 1; i < VIDXD_MAX_MSIX_VECS; i++) {
+		u32 ims_idx = dev_msi_hwirq(dev, i - 1);
+
+		/* Save the current handle in use */
+		dev_dbg(dev, "Saving handle %d at offset %x\n", ims_idx, offset);
+		memcpy(data_ptr + offset, (u8 *)&ims_idx, sizeof(ims_idx));
+		offset += sizeof(ims_idx);
+	}
+
+	mig_info->data_size = offset - mig_info->data_offset;
+	mig_info->pending_bytes = offset - mig_info->data_offset;
+
+	dev_dbg(dev, "%s, mig_info->pending_bytes: 0x%llx, data_size: 0x%llx\n",
+		__func__, mig_info->pending_bytes, mig_info->data_size);
+}
+
+static void vidxd_dest_prepare_for_migration(struct vdcm_idxd *vidxd)
+{
+
+}
+
+static int vidxd_resume_wq_state(struct vdcm_idxd *vidxd)
+{
+	struct idxd_wq *wq;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct idxd_device *idxd = vidxd->idxd;
+	union wqcfg *vwqcfg, *wqcfg;
+	bool priv;
+	int wq_id;
+	int rc = 0;
+	u8 *bar0 = vidxd->bar0;
+
+	dev_dbg(dev, "%s:%d numwqs %d\n", __func__, __LINE__, vidxd->num_wqs);
+	/* TODO: Currently support for only 1 WQ per VDev */
+	for (wq_id = 0; wq_id < vidxd->num_wqs; wq_id++) {
+		wq = vidxd->wq;
+		dev_dbg(dev, "%s:%d wq %px\n", __func__, __LINE__, wq);
+		vwqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET);
+		wqcfg = wq->wqcfg;
+
+		if (vidxd_state(vidxd) != 1 || vwqcfg->wq_state != 1) {
+			/* either VDEV or vWQ is disabled */
+			if (wq_dedicated(wq) && wq->state == IDXD_WQ_ENABLED)
+				idxd_wq_disable(wq, false, NULL);
+			continue;
+		} else {
+			unsigned long flags;
+			printk("vidxd re-enable wq %u:%u\n", wq_id, wq->id);
+
+			/* If dedicated WQ and PASID is not enabled, program
+			 * the default PASID in the WQ PASID register */
+			if (wq_dedicated(wq) && vwqcfg->mode_support) {
+				int wq_pasid, gpasid = -1;
+
+				if (vwqcfg->pasid_en) {
+					gpasid = vwqcfg->pasid;
+					priv = vwqcfg->priv;
+					rc = idxd_mdev_get_host_pasid(mdev,
+						gpasid, &wq_pasid);
+				} else {
+					rc = idxd_mdev_get_pasid(mdev,
+						&wq_pasid);
+					priv = true;
+				}
+
+				if (wq_pasid >= 0) {
+					u32 status;
+
+					wqcfg->bits[WQCFG_PASID_IDX] &=
+								~GENMASK(29, 8);
+					wqcfg->priv = priv;
+					wqcfg->pasid_en = 1;
+					wqcfg->pasid = wq_pasid;
+					dev_dbg(dev, "pasid %d:%d in wq %d\n",
+						gpasid, wq_pasid, wq->id);
+					spin_lock_irqsave(&idxd->dev_lock,
+									flags);
+					idxd_wq_setup_pasid(wq, wq_pasid);
+					idxd_wq_setup_priv(wq, priv);
+					spin_unlock_irqrestore(&idxd->dev_lock,
+									flags);
+					idxd_wq_enable(wq, &rc);
+					if (status) {
+						dev_err(dev, "resume wq failed\n");
+						break;;
+					}
+				}
+			} else if (!wq_dedicated(wq) && vwqcfg->mode_support) {
+				wqcfg->bits[WQCFG_PASID_IDX] &= ~GENMASK(29, 8);
+				wqcfg->pasid_en = 1;
+				wqcfg->mode = 0;
+				spin_lock_irqsave(&idxd->dev_lock, flags);
+				idxd_wq_setup_pasid(wq, 0);
+				spin_unlock_irqrestore(&idxd->dev_lock, flags);
+				idxd_wq_enable(wq, &rc);
+				if (rc) {
+					dev_err(dev, "resume wq %d failed\n",
+							wq->id);
+					break;
+				}
+			}
+		}
+	}
+	return rc;
+}
+
+static unsigned int vidxd_dest_load_state(struct vdcm_idxd *vidxd)
+{
+	struct vfio_pci_core_device *vdev = &vidxd->vfio_pdev;
+	struct vfio_device_migration_info *mig_info =
+		(struct vfio_device_migration_info *)vdev->mig_pages;
+	u8	*data_ptr = (u8 *)vdev->mig_pages;
+	unsigned int offset =  mig_info->data_offset;
+
+	pr_info("%s, data_size: %llx, data_offset: 0x%llx\n", __func__,
+			mig_info->data_size, mig_info->data_offset);
+
+	/* restore the state data to device */
+	memcpy(vidxd->cfg, data_ptr + offset, sizeof(vidxd->cfg));
+	offset += sizeof(vidxd->cfg);
+	memcpy((u8 *)vidxd->bar_val, data_ptr + offset, sizeof(vidxd->bar_val));
+	offset += sizeof(vidxd->bar_val);
+	memcpy((u8 *)vidxd->bar_size, data_ptr + offset,
+					sizeof(vidxd->bar_size));
+	offset += sizeof(vidxd->bar_size);
+	memcpy((u8 *)&vidxd->bar0, data_ptr + offset, sizeof(vidxd->bar0));
+	offset += sizeof(vidxd->bar0);
+	//memcpy((u8 *)ims, data_ptr + offset, sizeof(vidxd->ims));
+	//offset += sizeof(vidxd->ims);
+
+	printk("Offset %x\n", offset);
+	return offset;
+}
+
+static int vidxd_dest_int_handle_revocation (struct vdcm_idxd *vidxd,
+		unsigned int *offset)
+{
+	struct vfio_pci_core_device *vdev = &vidxd->vfio_pdev;
+	u8 *data_ptr = (u8 *)vdev->mig_pages;
+	u8 *bar0 = vidxd->bar0;
+	int i;
+	int rc = 0;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	bool int_handle_revoked = false;
+
+	/* Restore int handle info */
+	for (i = 1; i < VIDXD_MAX_MSIX_VECS; i++) {
+		u32 perm_val, auxval;
+		u32 gpasid, pasid;
+		bool paside;
+		int ims_idx = dev_msi_hwirq(dev, i - 1);
+		int irq = dev_msi_irq_vector(dev, i - 1);
+		u32 revoked_handle;
+
+		memcpy((u8 *)&revoked_handle, data_ptr + *offset,
+					sizeof(revoked_handle));
+		*offset += sizeof(revoked_handle);
+
+		pr_info("%s: %d new handle %x old handle %x\n",
+				__func__, i, ims_idx, revoked_handle);
+
+		if (revoked_handle != ims_idx) {
+			/* Int Handle Revoked */
+			int_handle_revoked = true;
+		}
+
+		perm_val = *(u32 *)(bar0 + VIDXD_MSIX_PERM_OFFSET + i * 8);
+
+		paside = (perm_val >> 3) & 1;
+		gpasid = (perm_val >> 12) & 0xfffff;
+
+		if (paside)
+			rc = idxd_mdev_get_host_pasid(vidxd->ivdev.mdev, gpasid, &pasid);
+		else
+			rc = idxd_mdev_get_pasid(vidxd->ivdev.mdev, &pasid);
+		if (rc < 0)
+			return rc;
+
+		auxval = ims_ctrl_pasid_aux(pasid, true);
+
+		rc = irq_set_auxdata(irq, IMS_AUXDATA_CONTROL_WORD, auxval);
+		pr_info("%s: auxval %x rc %d\n", __func__, auxval, rc);
+		if (rc < 0) {
+			pr_info("set ims pasid failed rc %d\n", rc);
+			break;
+		}
+	}
+
+	if (int_handle_revoked)
+                vidxd_notify_revoked_handles(vidxd);
+
+	return rc;
+}
+
+static int vidxd_resubmit_pending_descs (struct vdcm_idxd *vidxd,
+		unsigned int *offset)
+{
+	struct vfio_pci_core_device *vdev = &vidxd->vfio_pdev;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	u8 *data_ptr = (u8 *)vdev->mig_pages;
+	struct idxd_virtual_wq *vwq;
+	struct idxd_wq *wq;
+	int i;
+
+	/* Submit the queued descriptors. The WQ state
+	 * has been resumed by this point
+	 */
+	for (i = 0; i < vidxd->num_wqs; i++) {
+		void __iomem *portal;
+		struct idxd_wq_desc_elem el;
+		vwq = &vidxd->vwq;
+		wq = vidxd->wq;
+
+		memcpy((u8 *)&vwq->ndescs, data_ptr + *offset, sizeof(vwq->ndescs));
+		*offset += sizeof(vwq->ndescs);
+
+		for (; vwq->ndescs > 0; vwq->ndescs--) {
+			printk("Descriptor at offset %x\n", *offset);
+
+			memcpy((u8 *)&el, data_ptr + *offset, sizeof(el));
+			*offset += sizeof(el);
+
+			portal = wq->portal;
+			pr_info("submitting a desc to WQ %d:%d ded %d\n",
+					i, wq->id, wq_dedicated(wq));
+			if (wq_dedicated(wq)) {
+				iosubmit_cmds512(portal, el.work_desc, 1);
+			} else {
+				int rc;
+				struct dsa_hw_desc *hw =
+					(struct dsa_hw_desc *)el.work_desc;
+				int hpasid, gpasid = hw->pasid;
+
+				/* Translate the gpasid in the descriptor */
+				rc = idxd_mdev_get_host_pasid(mdev,
+						gpasid, &hpasid);
+				if (rc < 0) {
+					pr_info("gpasid->hpasid trans failed\n");
+					continue;
+				}
+				hw->pasid = hpasid;
+				rc = enqcmds(portal, el.work_desc);
+				if (rc < 0) {
+					pr_info("%s: enqcmds failed\n", __func__);
+					continue;
+				}
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int vidxd_dest_complete_migration(struct vdcm_idxd *vidxd)
+{
+	int rc = 0;
+	unsigned int offset;
+
+	offset = vidxd_dest_load_state(vidxd);
+
+	rc = vidxd_resume_wq_state(vidxd);
+
+	if (rc) {
+		pr_info("vidxd resume wq state failed %d\n", rc);
+		return rc;
+	}
+
+	rc = vidxd_resubmit_pending_descs(vidxd, &offset);
+
+	if (rc) {
+		pr_info("vidxd pending descs handling failed %d\n", rc);
+		return rc;
+	}
+
+	rc = vidxd_dest_int_handle_revocation(vidxd, &offset);
+
+	if (rc) {
+		pr_info("vidxd int handle revocation handling failed %d\n", rc);
+		return rc;
+	}
+
+	return rc;
+}
+
+static int vidxd_migration_state_change(struct vfio_pci_core_device *vfio_vdev,
+		u32 new_state)
+{
+	struct vdcm_idxd *vidxd = container_of(vfio_vdev, struct vdcm_idxd, vfio_pdev);
+	struct vfio_device_migration_info *mig_info =
+		(struct vfio_device_migration_info *) vfio_vdev->mig_pages;
+	int ret = 0;
+
+	pr_info("%s, VFIO_DEVICE_STATE_MASK: 0x%x, new_state: 0x%x\n",
+			__func__, VFIO_DEVICE_STATE_MASK, new_state);
+	if (new_state & (~(VFIO_DEVICE_STATE_MASK))) {
+		pr_info("%s, invalid new device state, 0x%x!!\n", __func__, new_state);
+		return -EINVAL;
+	}
+
+	switch (new_state) {
+	case 0:
+		pr_info("%s, __STOPPED !!\n", __func__);
+		vidxd_free_resources(vidxd);
+		break;
+	case VFIO_DEVICE_STATE_RUNNING:
+		pr_info("%s, VFIO_DEVICE_STATE_RUNNING!! old state %x\n",
+			__func__, mig_info->device_state);
+		if (mig_info->device_state & VFIO_DEVICE_STATE_RESUMING)
+			vidxd_dest_complete_migration(vidxd);
+		break;
+	case VFIO_DEVICE_STATE_SAVING | VFIO_DEVICE_STATE_RUNNING:
+		pr_info("%s, VFIO_DEVICE_STATE_SAVING | VFIO_DEVICE_STATE_RUNNING!!\n", __func__);
+
+		break;
+	case VFIO_DEVICE_STATE_SAVING:
+		pr_info("%s, VFIO_DEVICE_STATE_SAVING!!\n", __func__);
+		/* Prepared the state data for migration */
+		if (!(mig_info->device_state & VFIO_DEVICE_STATE_RUNNING))
+			vidxd_source_prepare_for_migration(vidxd);
+
+		/* Pause the virtual device. The vCPUs are still running.
+		 * This happens just before the VM is paused. The vDEV
+		 * is already in slow path */
+		if (mig_info->device_state & VFIO_DEVICE_STATE_RUNNING)
+			vidxd_source_pause_device(vidxd);
+		break;
+	case VFIO_DEVICE_STATE_RESUMING:
+		/* Prepared the state restore for migration */
+		vidxd_dest_prepare_for_migration(vidxd);
+		pr_info("%s, VFIO_DEVICE_STATE_RESUMING!!\n", __func__);
+		break;
+	default:
+		pr_info("%s, not handled new device state: 0x%x\n", __func__, new_state);
+		ret = -EINVAL;
+	}
+	return ret;
+}
+
+static struct vfio_pci_migops vidxd_migops = {
+	.state_change	= vidxd_migration_state_change,
+};
+
+static struct idxd_wq *find_any_dwq(struct idxd_device *idxd, struct vdcm_idxd_type *type)
+{
+	int i;
+	struct idxd_wq *wq;
+	unsigned long flags;
+
+	switch (type->type) {
+	case IDXD_MDEV_TYPE_DSA_1_DWQ:
+		if (idxd->data->type != IDXD_TYPE_DSA)
+			return NULL;
+		break;
+	case IDXD_MDEV_TYPE_IAX_1_DWQ:
+		if (idxd->data->type != IDXD_TYPE_IAX)
+			return NULL;
+		break;
+	default:
+		return NULL;
+	}
+
+	spin_lock_irqsave(&idxd->dev_lock, flags);
+	for (i = 0; i < idxd->max_wqs; i++) {
+		wq = idxd->wqs[i];
+
+		if (wq->state != IDXD_WQ_ENABLED && wq->state != IDXD_WQ_LOCKED)
+			continue;
+
+		if (!is_idxd_wq_mdev(wq))
+			continue;
+
+		if (!wq_dedicated(wq))
+			continue;
+
+		if (idxd_wq_refcount(wq) != 0)
+			continue;
+
+		spin_unlock_irqrestore(&idxd->dev_lock, flags);
+		mutex_lock(&wq->wq_lock);
+		idxd_wq_get(wq);
+		mutex_unlock(&wq->wq_lock);
+		return wq;
+	}
+
+	spin_unlock_irqrestore(&idxd->dev_lock, flags);
+	return NULL;
+}
+
+static int swq_lowest_client_count(struct idxd_device *idxd)
+{
+	struct idxd_wq *wq;
+	int i, count = -ENODEV;
+
+	lockdep_assert_held(&idxd->dev_lock);
+	for (i = 0; i < idxd->max_wqs; i++) {
+		wq = idxd->wqs[i];
+
+		if (wq->state != IDXD_WQ_ENABLED)
+			continue;
+
+		if (!is_idxd_wq_mdev(wq))
+			continue;
+
+		if (wq_dedicated(wq))
+			continue;
+
+		if (count == -ENODEV)
+			count = idxd_wq_refcount(wq);
+		else if (count > idxd_wq_refcount(wq))
+			count = idxd_wq_refcount(wq);
+	}
+
+	return count;
+}
+
+static struct idxd_wq *find_any_swq(struct idxd_device *idxd, struct vdcm_idxd_type *type)
+{
+	int i, count;
+	struct idxd_wq *wq;
+	unsigned long flags;
+
+	switch (type->type) {
+	case IDXD_MDEV_TYPE_DSA_1_SWQ:
+		if (idxd->data->type != IDXD_TYPE_DSA)
+			return NULL;
+		break;
+	case IDXD_MDEV_TYPE_IAX_1_SWQ:
+		if (idxd->data->type != IDXD_TYPE_IAX)
+			return NULL;
+		break;
+	default:
+		return NULL;
+	}
+
+	spin_lock_irqsave(&idxd->dev_lock, flags);
+	count = swq_lowest_client_count(idxd);
+	if (count < 0)
+		goto out;
+
+	for (i = 0; i < idxd->max_wqs; i++) {
+		wq = idxd->wqs[i];
+
+		if (wq->state != IDXD_WQ_ENABLED)
+			continue;
+
+		if (!is_idxd_wq_mdev(wq))
+			continue;
+
+		if (wq_dedicated(wq))
+			continue;
+
+		/*
+		 * Attempt to load balance the shared wq by round robin until on the lowest
+		 * ref count for the wq.
+		 */
+		if (idxd_wq_refcount(wq) != count)
+			continue;
+
+		spin_unlock_irqrestore(&idxd->dev_lock, flags);
+		mutex_lock(&wq->wq_lock);
+		idxd_wq_get(wq);
+		mutex_unlock(&wq->wq_lock);
+		return wq;
+	}
+
+ out:
+	spin_unlock_irqrestore(&idxd->dev_lock, flags);
+	return NULL;
+}
+
+extern const struct vfio_pci_regops vfio_pci_dma_fault_regops;
+
+static struct vdcm_idxd *vdcm_vidxd_create(struct idxd_device *idxd, struct mdev_device *mdev,
+					   struct vdcm_idxd_type *type)
+{
+	struct vdcm_idxd *vidxd;
+	struct device *dev = mdev_dev(mdev);
+	struct idxd_wq *wq = NULL;
+	int rc;
+
+	switch (type->type) {
+	case IDXD_MDEV_TYPE_DSA_1_DWQ:
+	case IDXD_MDEV_TYPE_IAX_1_DWQ:
+		wq = find_any_dwq(idxd, type);
+		break;
+	case IDXD_MDEV_TYPE_DSA_1_SWQ:
+	case IDXD_MDEV_TYPE_IAX_1_SWQ:
+		wq = find_any_swq(idxd, type);
+		break;
+	default:
+		return ERR_PTR(-ENODEV);
+	}
+
+	if (!wq)
+		return ERR_PTR(-ENODEV);
+
+	vidxd = kzalloc(sizeof(*vidxd), GFP_KERNEL);
+	if (!vidxd) {
+		rc = -ENOMEM;
+		goto err;
+	}
+
+	mutex_init(&vidxd->dev_lock);
+	vidxd->idxd = idxd;
+	vidxd->ivdev.mdev = mdev;
+	vidxd->wq = wq;
+	mdev_set_drvdata(mdev, vidxd);
+	vidxd->type = type;
+	vidxd->num_wqs = VIDXD_MAX_WQS;
+	dev_set_msi_domain(dev, idxd->ims_domain);
+	mutex_init(&vidxd->ivdev.ioasid_lock);
+	INIT_LIST_HEAD(&vidxd->ivdev.mm_list);
+
+	idxd_vdcm_init(vidxd);
+
+	mutex_init(&vidxd->vfio_pdev.igate);
+	vidxd->vfio_pdev.pdev = idxd->pdev;
+	rc = vfio_pci_dma_fault_init(&vidxd->vfio_pdev, false);
+	if (rc < 0) {
+		dev_err(dev, "dma fault region init failed\n");
+		kfree(vidxd);
+		goto err;
+	}
+
+	mdev_set_iommu_fault_data(mdev, &vidxd->vfio_pdev);
+
+	vidxd->vfio_pdev.migops = &vidxd_migops;
+	rc = vfio_pci_migration_init(&vidxd->vfio_pdev, VIDXD_STATE_BUFFER_SIZE);
+	if (rc)
+		pr_err("%s, idxd migration region init failed!!!\n", __func__);
+	else
+		pr_info("%s, idxd migration region init successfully!!!\n", __func__);
+
+	return vidxd;
+
+ err:
+	mutex_lock(&wq->wq_lock);
+	idxd_wq_put(wq);
+	mutex_unlock(&wq->wq_lock);
+	return ERR_PTR(rc);
+}
+
+static struct vdcm_idxd_type idxd_mdev_types[IDXD_MDEV_TYPES] = {
+	{
+		.name = idxd_dsa_1dwq_name,
+		.type = IDXD_MDEV_TYPE_DSA_1_DWQ,
+	},
+	{
+		.name = idxd_iax_1dwq_name,
+		.type = IDXD_MDEV_TYPE_IAX_1_DWQ,
+	},
+	{
+		.name = idxd_dsa_1swq_name,
+		.type = IDXD_MDEV_TYPE_DSA_1_SWQ,
+	},
+	{
+		.name = idxd_iax_1swq_name,
+		.type = IDXD_MDEV_TYPE_IAX_1_SWQ,
+	},
+};
+
+static struct vdcm_idxd_type *idxd_vdcm_get_type(struct mdev_device *mdev)
+{
+	return &idxd_mdev_types[mdev_get_type_group_id(mdev)];
+}
+
+static const struct vfio_device_ops idxd_mdev_ops;
+
+static int idxd_vdcm_probe(struct mdev_device *mdev)
+{
+	struct vdcm_idxd *vidxd;
+	struct vdcm_idxd_type *type;
+	struct device *dev, *parent;
+	struct idxd_device *idxd;
+	struct idxd_wq *wq;
+	int rc;
+
+	parent = mdev_parent_dev(mdev);
+	idxd = dev_get_drvdata(parent);
+	dev = mdev_dev(mdev);
+	mdev_set_iommu_device(mdev, parent);
+	type = idxd_vdcm_get_type(mdev);
+
+	vidxd = vdcm_vidxd_create(idxd, mdev, type);
+	if (IS_ERR(vidxd)) {
+		dev_err(dev, "failed to create vidxd: %ld\n", PTR_ERR(vidxd));
+		return PTR_ERR(vidxd);
+	}
+
+	vfio_init_group_dev(&vidxd->vdev, &mdev->dev, &idxd_mdev_ops);
+	wq = vidxd->wq;
+	dev_set_drvdata(dev, vidxd);
+	rc = vfio_register_group_dev(&vidxd->vdev);
+	if (rc < 0) {
+		mutex_lock(&wq->wq_lock);
+		idxd_wq_put(wq);
+		mutex_unlock(&wq->wq_lock);
+		kfree(vidxd);
+		return rc;
+	}
+
+	mutex_lock(&wq->wq_lock);
+	list_add(&vidxd->list, &wq->vdcm_list);
+	mutex_unlock(&wq->wq_lock);
+	dev_dbg(dev, "mdev creation success: %s\n", dev_name(mdev_dev(mdev)));
+
+	return 0;
+}
+
+static void idxd_vdcm_remove(struct mdev_device *mdev)
+{
+	struct vdcm_idxd *vidxd = mdev_get_drvdata(mdev);
+	struct idxd_device *idxd = vidxd->idxd;
+	struct device *dev = &idxd->pdev->dev;
+	struct idxd_wq *wq = vidxd->wq;
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	int i;
+
+	dev_dbg(dev, "%s: removing for wq %d\n", __func__, vidxd->wq->id);
+
+	for (i = 0; i < vfio_pdev->num_regions; i++)
+		vfio_pdev->region[i].ops->release(vfio_pdev, &vfio_pdev->region[i]);
+	vfio_pdev->num_regions = 0;
+	kfree(vfio_pdev->region);
+	vfio_pdev->region = NULL;
+
+	for (i = 0; i < vfio_pdev->num_ext_irqs; i++)
+		vfio_pci_set_ext_irq_trigger(vfio_pdev,
+					 VFIO_IRQ_SET_DATA_NONE | VFIO_IRQ_SET_ACTION_TRIGGER,
+					 VFIO_PCI_NUM_IRQS + i, 0, 0, NULL);
+	vfio_pdev->num_ext_irqs = 0;
+	kfree(vfio_pdev->ext_irqs);
+	vfio_pdev->ext_irqs = NULL;
+
+	mutex_lock(&wq->wq_lock);
+	list_del(&vidxd->list);
+	idxd_wq_put(wq);
+	mutex_unlock(&wq->wq_lock);
+
+	vfio_unregister_group_dev(&vidxd->vdev);
+
+	kfree(vidxd);
+}
+
+static int idxd_vdcm_open(struct vfio_device *vdev)
+{
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+	int rc = -EINVAL;
+	struct vdcm_idxd_type *type = vidxd->type;
+	struct device *dev = vdev->dev;
+	struct vfio_group *vfio_group;
+
+	dev_dbg(dev, "%s: type: %d\n", __func__, type->type);
+
+	vfio_group = vfio_group_get_external_user_from_dev(dev);
+	if (IS_ERR_OR_NULL(vfio_group)) {
+		rc = -EFAULT;
+		goto out;
+	}
+
+	rc = vidxd_register_ioasid_notifier(vidxd);
+	if (rc < 0)
+		goto ioasid_err;
+
+	mutex_lock(&vidxd->dev_lock);
+	if (vidxd->refcount)
+		goto ioasid_err;
+
+	vidxd->ivdev.vfio_group = vfio_group;
+	vidxd->refcount++;
+
+	mutex_unlock(&vidxd->dev_lock);
+	return 0;
+
+ ioasid_err:
+	vfio_group_put_external_user(vfio_group);
+ out:
+	mutex_unlock(&vidxd->dev_lock);
+	return rc;
+}
+
+static void idxd_vdcm_close(struct vfio_device *vdev)
+{
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+
+	mutex_lock(&vidxd->dev_lock);
+	if (!vidxd->refcount)
+		goto out;
+
+	vidxd_unregister_ioasid_notifier(vidxd);
+	idxd_vdcm_set_irqs(vidxd, VFIO_IRQ_SET_DATA_NONE | VFIO_IRQ_SET_ACTION_TRIGGER,
+			   VFIO_PCI_MSIX_IRQ_INDEX, 0, 0, NULL);
+
+	if (vidxd->ivdev.vfio_group) {
+		vfio_group_put_external_user(vidxd->ivdev.vfio_group);
+		vidxd->ivdev.vfio_group = NULL;
+	}
+
+	/* Re-initialize the VIDXD to a pristine state for re-use */
+	idxd_vdcm_init(vidxd);
+	vidxd->refcount--;
+	vidxd->paused = false;
+ out:
+	mutex_unlock(&vidxd->dev_lock);
+}
+
+static int vidxd_register_ioasid_notifier(struct vdcm_idxd *vidxd)
+{
+	struct idxd_vdev *vdev = &vidxd->ivdev;
+	struct ioasid_mm_entry *mm_entry;
+	struct mm_struct *mm;
+	int rc;
+
+	mm = get_task_mm(current);
+	if (!mm)
+		return -ENODEV;
+
+	mutex_lock(&vdev->ioasid_lock);
+	list_for_each_entry(mm_entry, &vdev->mm_list, node) {
+		if (mm_entry->mm == mm) {
+			mutex_unlock(&vdev->ioasid_lock);
+			return 0;
+		}
+	}
+
+	mm_entry = kzalloc(sizeof(*mm_entry), GFP_KERNEL);
+	if (!mm_entry) {
+		rc = -ENOMEM;
+		goto err_alloc;
+	}
+
+	mm_entry->mm = mm;
+
+	vidxd->ivdev.pasid_nb.priority = IOASID_PRIO_DEVICE;
+	vidxd->ivdev.pasid_nb.notifier_call = idxd_mdev_ioasid_event;
+	rc = ioasid_register_notifier_mm(mm, &vidxd->ivdev.pasid_nb);
+	mmput(mm);
+	if (rc < 0)
+		goto err_ioasid;
+
+	list_add(&mm_entry->node, &vdev->mm_list);
+	mutex_unlock(&vdev->ioasid_lock);
+
+	return 0;
+
+ err_ioasid:
+	kfree(mm_entry);
+ err_alloc:
+	mutex_unlock(&vdev->ioasid_lock);
+	mmput(mm);
+	return rc;
+}
+
+static ssize_t idxd_vdcm_rw(struct vfio_device *vdev, char *buf, size_t count, loff_t *ppos,
+			    enum idxd_vdcm_rw mode)
+{
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+	unsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);
+	u64 pos = *ppos & VFIO_PCI_OFFSET_MASK;
+	struct device *dev = vdev->dev;
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	int rc = -EINVAL;
+
+	if (index >= VFIO_PCI_NUM_REGIONS + vfio_pdev->num_regions) {
+		dev_err(dev, "invalid index: %u\n", index);
+		return -EINVAL;
+	}
+
+	switch (index) {
+	case VFIO_PCI_CONFIG_REGION_INDEX:
+		if (mode == IDXD_VDCM_WRITE)
+			rc = vidxd_cfg_write(vidxd, pos, buf, count);
+		else
+			rc = vidxd_cfg_read(vidxd, pos, buf, count);
+		break;
+	case VFIO_PCI_BAR0_REGION_INDEX:
+	case VFIO_PCI_BAR1_REGION_INDEX:
+		if (mode == IDXD_VDCM_WRITE)
+			rc = vidxd_mmio_write(vidxd, vidxd->bar_val[0] + pos, buf, count);
+		else
+			rc = vidxd_mmio_read(vidxd, vidxd->bar_val[0] + pos, buf, count);
+		break;
+	case VFIO_PCI_BAR2_REGION_INDEX:
+	case VFIO_PCI_BAR3_REGION_INDEX:
+		if (mode == IDXD_VDCM_WRITE) {
+			rc = vidxd_portal_mmio_write(vidxd,
+				vidxd->bar_val[1] + pos, buf, count);
+		} else {
+			rc = vidxd_portal_mmio_read(vidxd,
+				vidxd->bar_val[1] + pos, buf, count);
+		}
+		break;
+
+	case VFIO_PCI_BAR4_REGION_INDEX:
+	case VFIO_PCI_BAR5_REGION_INDEX:
+	case VFIO_PCI_VGA_REGION_INDEX:
+	case VFIO_PCI_ROM_REGION_INDEX:
+		dev_err(dev, "unsupported region: %u\n", index);
+		break;
+
+	default:
+		dev_dbg(dev, "vendor specific region: %u\n", index);
+		index -= VFIO_PCI_NUM_REGIONS;
+		return vfio_pdev->region[index].ops->rw(vfio_pdev, buf, count, ppos, mode);
+	}
+
+	return rc == 0 ? count : rc;
+}
+
+static ssize_t idxd_vdcm_read(struct vfio_device *vdev, char __user *buf, size_t count,
+			      loff_t *ppos)
+{
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+	unsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);
+	unsigned int done = 0;
+	int rc;
+
+	switch (index) {
+	case VFIO_PCI_CONFIG_REGION_INDEX:
+	case VFIO_PCI_BAR0_REGION_INDEX:
+	case VFIO_PCI_BAR1_REGION_INDEX:
+	case VFIO_PCI_BAR2_REGION_INDEX:
+	case VFIO_PCI_BAR3_REGION_INDEX:
+	case VFIO_PCI_BAR4_REGION_INDEX:
+	case VFIO_PCI_BAR5_REGION_INDEX:
+	case VFIO_PCI_VGA_REGION_INDEX:
+	case VFIO_PCI_ROM_REGION_INDEX:
+		break;
+	default: {
+		struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+		struct device *dev = vdev->dev;
+
+		dev_dbg(dev, "vendor specific region: %u\n", index);
+		index -= VFIO_PCI_NUM_REGIONS;
+		return vfio_pdev->region[index].ops->rw(vfio_pdev, buf, count, ppos, false);
+	} /* end default */
+	} /* end switch(index) */
+
+	mutex_lock(&vidxd->dev_lock);
+	while (count) {
+		size_t filled;
+
+		if (count >= 4 && !(*ppos % 4)) {
+			u32 val;
+
+			rc = idxd_vdcm_rw(vdev, (char *)&val, sizeof(val),
+					  ppos, IDXD_VDCM_READ);
+			if (rc <= 0)
+				goto read_err;
+
+			if (copy_to_user(buf, &val, sizeof(val)))
+				goto read_err;
+
+			filled = 4;
+		} else if (count >= 2 && !(*ppos % 2)) {
+			u16 val;
+
+			rc = idxd_vdcm_rw(vdev, (char *)&val, sizeof(val),
+					  ppos, IDXD_VDCM_READ);
+			if (rc <= 0)
+				goto read_err;
+
+			if (copy_to_user(buf, &val, sizeof(val)))
+				goto read_err;
+
+			filled = 2;
+		} else {
+			u8 val;
+
+			rc = idxd_vdcm_rw(vdev, &val, sizeof(val), ppos,
+					  IDXD_VDCM_READ);
+			if (rc <= 0)
+				goto read_err;
+
+			if (copy_to_user(buf, &val, sizeof(val)))
+				goto read_err;
+
+			filled = 1;
+		}
+
+		count -= filled;
+		done += filled;
+		*ppos += filled;
+		buf += filled;
+	}
+
+	mutex_unlock(&vidxd->dev_lock);
+	return done;
+
+ read_err:
+	mutex_unlock(&vidxd->dev_lock);
+	return -EFAULT;
+}
+
+static ssize_t idxd_vdcm_write(struct vfio_device *vdev, const char __user *buf, size_t count,
+			       loff_t *ppos)
+{
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+	unsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);
+	unsigned int done = 0;
+	int rc;
+
+	switch (index) {
+	case VFIO_PCI_CONFIG_REGION_INDEX:
+	case VFIO_PCI_BAR0_REGION_INDEX:
+	case VFIO_PCI_BAR1_REGION_INDEX:
+	case VFIO_PCI_BAR2_REGION_INDEX:
+	case VFIO_PCI_BAR3_REGION_INDEX:
+	case VFIO_PCI_BAR4_REGION_INDEX:
+	case VFIO_PCI_BAR5_REGION_INDEX:
+	case VFIO_PCI_VGA_REGION_INDEX:
+	case VFIO_PCI_ROM_REGION_INDEX:
+		break;
+	default: {
+		struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+		struct device *dev = vdev->dev;
+
+		dev_dbg(dev, "vendor specific region: %u\n", index);
+		index -= VFIO_PCI_NUM_REGIONS;
+		return vfio_pdev->region[index].ops->rw(vfio_pdev, buf, count, ppos, true);
+	} /* end default */
+	} /* end switch(index) */
+
+	mutex_lock(&vidxd->dev_lock);
+	while (count) {
+		size_t filled;
+
+		if (count >= 4 && !(*ppos % 4)) {
+			u32 val;
+
+			if (copy_from_user(&val, buf, sizeof(val)))
+				goto write_err;
+
+			rc = idxd_vdcm_rw(vdev, (char *)&val, sizeof(val),
+					  ppos, IDXD_VDCM_WRITE);
+			if (rc <= 0)
+				goto write_err;
+
+			filled = 4;
+		} else if (count >= 2 && !(*ppos % 2)) {
+			u16 val;
+
+			if (copy_from_user(&val, buf, sizeof(val)))
+				goto write_err;
+
+			rc = idxd_vdcm_rw(vdev, (char *)&val,
+					  sizeof(val), ppos, IDXD_VDCM_WRITE);
+			if (rc <= 0)
+				goto write_err;
+
+			filled = 2;
+		} else {
+			u8 val;
+
+			if (copy_from_user(&val, buf, sizeof(val)))
+				goto write_err;
+
+			rc = idxd_vdcm_rw(vdev, &val, sizeof(val),
+					  ppos, IDXD_VDCM_WRITE);
+			if (rc <= 0)
+				goto write_err;
+
+			filled = 1;
+		}
+
+		count -= filled;
+		done += filled;
+		*ppos += filled;
+		buf += filled;
+	}
+
+	mutex_unlock(&vidxd->dev_lock);
+	return done;
+
+write_err:
+	mutex_unlock(&vidxd->dev_lock);
+	return -EFAULT;
+}
+
+static int idxd_vdcm_mmap(struct vfio_device *vdev, struct vm_area_struct *vma)
+{
+	unsigned int wq_idx, index;
+	unsigned long req_size, pgoff = 0, offset;
+	pgprot_t pg_prot;
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+	struct idxd_device *idxd = vidxd->idxd;
+	struct idxd_wq *wq = vidxd->wq;
+	enum idxd_portal_prot virt_portal, phys_portal;
+	phys_addr_t base = pci_resource_start(idxd->pdev, IDXD_WQ_BAR);
+	struct device *dev = vdev->dev;
+
+	if (!(vma->vm_flags & VM_SHARED))
+		return -EINVAL;
+
+	index = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);
+	if (index >= VFIO_PCI_NUM_REGIONS) {
+		int regnum = index - VFIO_PCI_NUM_REGIONS;
+		struct vfio_pci_region *region = vidxd->vfio_pdev.region + regnum;
+
+		if (region && region->ops && region->ops->mmap &&
+		    (region->flags & VFIO_REGION_INFO_FLAG_MMAP))
+			return region->ops->mmap(&vidxd->vfio_pdev, region, vma);
+
+		return -EINVAL;
+	}
+
+	pg_prot = vma->vm_page_prot;
+	req_size = vma->vm_end - vma->vm_start;
+	if (req_size > PAGE_SIZE)
+		return -EINVAL;
+
+	vma->vm_flags |= VM_DONTCOPY;
+
+	offset = (vma->vm_pgoff << PAGE_SHIFT) &
+		 ((1ULL << VFIO_PCI_OFFSET_SHIFT) - 1);
+
+	wq_idx = offset >> (PAGE_SHIFT + 2);
+	if (wq_idx >= 1) {
+		dev_err(dev, "mapping invalid wq %d off %lx\n",
+			wq_idx, offset);
+		return -EINVAL;
+	}
+
+	/*
+	 * Check and see if the guest wants to map to the limited or unlimited portal.
+	 * The driver will allow mapping to unlimited portal only if the wq is a
+	 * dedicated wq. Otherwise, it goes to limited.
+	 */
+	virt_portal = ((offset >> PAGE_SHIFT) & 0x3) == 1;
+	phys_portal = IDXD_PORTAL_LIMITED;
+	if (virt_portal == IDXD_PORTAL_UNLIMITED && wq_dedicated(wq))
+		phys_portal = IDXD_PORTAL_UNLIMITED;
+
+	/* We always map IMS portals to the guest */
+	pgoff = (base + idxd_get_wq_portal_full_offset(wq->id, phys_portal,
+						       IDXD_IRQ_IMS)) >> PAGE_SHIFT;
+
+	dev_dbg(dev, "mmap %lx %lx %lx %lx\n", vma->vm_start, pgoff, req_size,
+		pgprot_val(pg_prot));
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	vma->vm_pgoff = pgoff;
+
+	return remap_pfn_range(vma, vma->vm_start, pgoff, req_size, pg_prot);
+}
+
+static void vidxd_vdcm_reset(struct vdcm_idxd *vidxd)
+{
+	vidxd_reset(vidxd);
+}
+
+static irqreturn_t idxd_vdcm_msix_handler(int irq, void *arg)
+{
+	struct vfio_pci_irq_ctx *ctx = (struct vfio_pci_irq_ctx *)arg;
+
+	eventfd_signal(ctx->trigger, 1);
+	return IRQ_HANDLED;
+}
+
+static void idxd_vdcm_free_irq (struct vfio_pci_core_device *vfio_pdev, int vector, int irq)
+{
+	u32 auxval;
+	if (irq) {
+		irq_bypass_unregister_producer(&vfio_pdev->ctx[vector].producer);
+		free_irq(irq, &vfio_pdev->ctx[vector]);
+		auxval = ims_ctrl_pasid_aux(0, false);
+		irq_set_auxdata(irq, IMS_AUXDATA_CONTROL_WORD, auxval);
+	}
+	kfree(vfio_pdev->ctx[vector].name);
+	vfio_pdev->ctx[vector].name = NULL;
+}
+
+static int idxd_vdcm_msix_set_vector_signal(struct vdcm_idxd *vidxd, int vector, int fd)
+{
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct eventfd_ctx *trigger;
+	char *name;
+	u32 pasid, auxval;
+	int irq, rc;
+
+	dev_dbg(dev, "%s: set signal %d fd: %d\n", __func__, vector, fd);
+
+	if (vector < 0 || vector >= vfio_pdev->num_ctx) {
+		dev_warn(dev, "%s out of boundary\n", __func__);
+		return -EINVAL;
+	}
+
+	irq = vector ? dev_msi_irq_vector(dev, vector - 1) : 0;
+
+	dev_dbg(dev, "%s: irq: %d\n", __func__, irq);
+
+	if (vfio_pdev->ctx[vector].trigger) {
+		if (irq)
+			irq_bypass_unregister_producer(&vfio_pdev->ctx[vector].producer);
+
+		eventfd_ctx_put(vfio_pdev->ctx[vector].trigger);
+
+		if (fd < 0) {
+			dev_dbg(dev, "%s: trigger already set, freeing\n", __func__);
+			idxd_vdcm_free_irq(vfio_pdev, vector, irq);
+			return 0;
+		}
+		dev_dbg(dev, "%s: trigger already set, changing\n", __func__);
+		trigger = eventfd_ctx_fdget(fd);
+		if (IS_ERR(trigger)) {
+			dev_dbg(dev, "%s: trigger change failed, freeing\n", __func__);
+			idxd_vdcm_free_irq(vfio_pdev, vector, irq);
+			vfio_pdev->ctx[vector].trigger = NULL;
+			return PTR_ERR(trigger);
+		}
+		vfio_pdev->ctx[vector].trigger = trigger;
+
+		if (irq) {
+			/* Update IRQ Bypass Setting */
+			vfio_pdev->ctx[vector].producer.token = trigger;
+			vfio_pdev->ctx[vector].producer.irq = irq;
+			rc = irq_bypass_register_producer(&vfio_pdev->ctx[vector].producer);
+			if (unlikely(rc)) {
+				dev_warn(dev, "irq bypass producer (token %p) registration fails: %d\n",
+					vfio_pdev->ctx[vector].producer.token, rc);
+				vfio_pdev->ctx[vector].producer.token = NULL;
+			}
+		}
+		return 0;
+	}
+
+	if (fd < 0)
+		return 0;
+
+	name = kasprintf(GFP_KERNEL, "vfio-dev-ims[%d](%s)", vector, dev_name(dev));
+	if (!name)
+		return -ENOMEM;
+
+	trigger = eventfd_ctx_fdget(fd);
+	if (IS_ERR(trigger)) {
+		kfree(name);
+		return PTR_ERR(trigger);
+	}
+
+	vfio_pdev->ctx[vector].name = name;
+	vfio_pdev->ctx[vector].trigger = trigger;
+
+	dev_dbg(dev, "%s: trigger: %px\n", __func__, trigger);
+
+	if (!irq) {
+		dev_dbg(dev, "Mediated vector 0 set\n");
+		return 0;
+	}
+
+	rc = idxd_mdev_get_pasid(mdev, &pasid);
+	if (rc < 0) {
+		dev_warn(dev, "%s unable to get pasid, failing\n", __func__);
+		goto err;
+	}
+
+	dev_dbg(dev, "%s: pasid: %d\n", __func__, pasid);
+
+	auxval = ims_ctrl_pasid_aux(pasid, true);
+	rc = irq_set_auxdata(irq, IMS_AUXDATA_CONTROL_WORD, auxval);
+	if (rc < 0) {
+		dev_warn(dev, "%s: set IMS aux data failed: %d\n", __func__, rc);
+		goto err;
+	}
+
+	rc = request_irq(irq, idxd_vdcm_msix_handler, 0, name, &vfio_pdev->ctx[vector]);
+	if (rc < 0) {
+		dev_warn(dev, "%s request_irq() failed\n", __func__);
+		goto irq_err;
+	}
+
+	vfio_pdev->ctx[vector].producer.token = trigger;
+	vfio_pdev->ctx[vector].producer.irq = irq;
+	rc = irq_bypass_register_producer(&vfio_pdev->ctx[vector].producer);
+	if (unlikely(rc)) {
+		dev_warn(dev, "irq bypass producer (token %p) registration fails: %d\n",
+			vfio_pdev->ctx[vector].producer.token, rc);
+		vfio_pdev->ctx[vector].producer.token = NULL;
+	}
+
+	return 0;
+
+ irq_err:
+	auxval = ims_ctrl_pasid_aux(0, false);
+	irq_set_auxdata(irq, IMS_AUXDATA_CONTROL_WORD, auxval);
+ err:
+	kfree(name);
+	vfio_pdev->ctx[vector].name = NULL;
+	eventfd_ctx_put(trigger);
+	vfio_pdev->ctx[vector].trigger = NULL;
+	return rc;
+}
+
+static int idxd_vdcm_msix_set_vector_signals(struct vdcm_idxd *vidxd, u32 start,
+					     u32 count, int *fds)
+{
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	int i, j, rc = 0;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+
+	if (start >= vfio_pdev->num_ctx || start + count > vfio_pdev->num_ctx) {
+		dev_warn(dev, "%s out of boundary\n", __func__);
+		return -EINVAL;
+	}
+
+	for (i = 0, j = start; i < count && !rc; i++, j++) {
+		int fd = fds ? fds[i] : -1;
+
+		dev_dbg(dev, "%s: %s signal %d, fd: %d\n",
+			__func__, (fd == -1) ? "unset" : "set", j, fd);
+		rc = idxd_vdcm_msix_set_vector_signal(vidxd, j, fd);
+	}
+
+	if (rc) {
+		dev_warn(dev, "%s: set signal failed, unwind\n", __func__);
+		for (--j; j >= (int)start; j--)
+			idxd_vdcm_msix_set_vector_signal(vidxd, j, -1);
+	}
+
+	return rc;
+}
+
+static int idxd_vdcm_msix_enable(struct vdcm_idxd *vidxd, int nvec)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	int rc;
+
+	dev_dbg(dev, "%s: nvec: %d\n", __func__, nvec);
+
+	/* There should be at least 1 vectors for idxd */
+	if (nvec < 1)
+		return -EINVAL;
+
+	dev_dbg(dev, "%s: allocating\n", __func__);
+	vfio_pdev->ctx = kcalloc(nvec, sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);
+	if (!vfio_pdev->ctx) {
+		dev_warn(dev, "%s: failed to alloc VFIO irq context\n", __func__);
+		return -ENOMEM;
+	}
+
+	if (nvec > 1) {
+		dev_dbg(dev, "%s: allocate %d IMS\n", __func__, nvec - 1);
+		rc = msi_domain_alloc_irqs(dev_get_msi_domain(dev), dev, nvec - 1);
+		if (rc < 0) {
+			dev_warn(dev, "%s failed to allocate irq on IMS domain: %d\n",
+				 __func__, rc);
+			kfree(vfio_pdev->ctx);
+			return rc;
+		}
+	}
+
+	vfio_pdev->num_ctx = nvec;
+	vfio_pdev->irq_type = VFIO_PCI_MSIX_IRQ_INDEX;
+	return 0;
+}
+
+static int idxd_vdcm_msix_disable(struct vdcm_idxd *vidxd)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct irq_domain *irq_domain;
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+
+	/* Check if somebody already disabled it */
+	if (vfio_pdev->num_ctx == 0)
+		return 0;
+
+	idxd_vdcm_msix_set_vector_signals(vidxd, 0, vfio_pdev->num_ctx, NULL);
+	irq_domain = dev_get_msi_domain(dev);
+	if (irq_domain)
+		msi_domain_free_irqs(irq_domain, dev);
+	kfree(vfio_pdev->ctx);
+	vfio_pdev->num_ctx = 0;
+	vfio_pdev->irq_type = VFIO_PCI_NUM_IRQS;
+	return 0;
+}
+
+static int idxd_vdcm_set_msix_trigger(struct vdcm_idxd *vidxd, u32 index, u32 start,
+				      u32 count, u32 flags, void *data)
+{
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	int rc, i;
+
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+
+	dev_dbg(dev, "%s(index: %d start: %d count: %d flags: %d data: %px\n",
+		__func__, index, start, count, flags, data);
+
+	if (count > VIDXD_MAX_MSIX_VECS)
+		count = VIDXD_MAX_MSIX_VECS;
+
+	if (!count && (flags & VFIO_IRQ_SET_DATA_NONE)) {
+		dev_dbg(dev, "%s disabling\n", __func__);
+		idxd_vdcm_msix_disable(vidxd);
+		return 0;
+	}
+
+	if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {
+		int *fds = data;
+
+		if (vfio_pdev->irq_type == index) {
+			dev_dbg(dev, "%s straight set signal\n", __func__);
+			return idxd_vdcm_msix_set_vector_signals(vidxd, start, count, fds);
+		}
+
+		rc = idxd_vdcm_msix_enable(vidxd, start + count);
+		if (rc < 0)
+			return rc;
+
+		rc = idxd_vdcm_msix_set_vector_signals(vidxd, start, count, fds);
+		if (rc < 0)
+			idxd_vdcm_msix_disable(vidxd);
+
+		return rc;
+	}
+
+	if (start + count > VIDXD_MAX_MSIX_VECS)
+		return -EINVAL;
+
+	for (i = start; i < start + count; i++) {
+		if (!vfio_pdev->ctx[i].trigger)
+			continue;
+		if (flags & VFIO_IRQ_SET_DATA_NONE) {
+			eventfd_signal(vfio_pdev->ctx[i].trigger, 1);
+		} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {
+			u8 *bools = data;
+
+			if (bools[i - start])
+				eventfd_signal(vfio_pdev->ctx[i].trigger, 1);
+		}
+	}
+	return 0;
+}
+
+
+static int idxd_vdcm_set_ctx_trigger_single(struct eventfd_ctx **ctx,
+					    unsigned int count, u32 flags, void *data)
+{
+	/* DATA_NONE/DATA_BOOL enables loopback testing */
+	if (flags & VFIO_IRQ_SET_DATA_NONE) {
+		if (*ctx) {
+			if (count) {
+				eventfd_signal(*ctx, 1);
+			} else {
+				eventfd_ctx_put(*ctx);
+				*ctx = NULL;
+			}
+			return 0;
+		}
+	} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {
+		u8 trigger;
+
+		if (!count)
+			return -EINVAL;
+
+		trigger = *(u8 *)data;
+		if (trigger && *ctx)
+			eventfd_signal(*ctx, 1);
+
+		return 0;
+	} else if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {
+		s32 fd;
+
+		if (!count)
+			return -EINVAL;
+
+		fd = *(s32 *)data;
+		if (fd == -1) {
+			if (*ctx)
+				eventfd_ctx_put(*ctx);
+			*ctx = NULL;
+		} else if (fd >= 0) {
+			struct eventfd_ctx *efdctx;
+
+			efdctx = eventfd_ctx_fdget(fd);
+			if (IS_ERR(efdctx))
+				return PTR_ERR(efdctx);
+
+			if (*ctx)
+				eventfd_ctx_put(*ctx);
+
+			*ctx = efdctx;
+		}
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+static int idxd_vdcm_set_req_trigger(struct mdev_device *mdev, unsigned int index,
+				    unsigned int start, unsigned int count,
+				    u32 flags, void *data)
+{
+	if (index != VFIO_PCI_REQ_IRQ_INDEX || start != 0 || count > 1)
+		return -EINVAL;
+
+	return idxd_vdcm_set_ctx_trigger_single(&mdev->req_trigger, count, flags, data);
+}
+
+static int idxd_vdcm_set_irqs(struct vdcm_idxd *vidxd, uint32_t flags,
+			      unsigned int index, unsigned int start,
+			      unsigned int count, void *data)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+
+	dev_dbg(dev, "%s: flags: %#x index: %#x, start: %#x, count: %#x, data: %px\n",
+		__func__, flags, index, start, count, data);
+
+	switch (index) {
+	case VFIO_PCI_MSIX_IRQ_INDEX:
+		switch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {
+		case VFIO_IRQ_SET_ACTION_MASK:
+		case VFIO_IRQ_SET_ACTION_UNMASK:
+			break;
+		case VFIO_IRQ_SET_ACTION_TRIGGER:
+			return idxd_vdcm_set_msix_trigger(vidxd, index, start, count, flags, data);
+		}
+		break;
+	case VFIO_PCI_REQ_IRQ_INDEX:
+		switch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {
+		case VFIO_IRQ_SET_ACTION_TRIGGER:
+			return idxd_vdcm_set_req_trigger(mdev, index, start, count, flags, data);
+		}
+		break;
+	default:
+		switch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {
+		case VFIO_IRQ_SET_ACTION_TRIGGER:
+			return vfio_pci_set_ext_irq_trigger(vfio_pdev, index, start,
+							    count, flags, data);
+		}
+		break;
+	}
+
+	return -ENOTTY;
+}
+
+static long idxd_vdcm_ioctl(struct vfio_device *vdev, unsigned int cmd,
+			    unsigned long arg)
+{
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+	unsigned long minsz;
+	int rc = -EINVAL;
+	struct device *dev = vdev->dev;
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+
+	dev_dbg(dev, "vidxd %p ioctl, cmd: %d\n", vidxd, cmd);
+
+	mutex_lock(&vidxd->dev_lock);
+	if (cmd == VFIO_DEVICE_GET_INFO) {
+		struct vfio_device_info info;
+
+		minsz = offsetofend(struct vfio_device_info, num_irqs);
+
+		if (copy_from_user(&info, (void __user *)arg, minsz)) {
+			rc = -EFAULT;
+			goto out;
+		}
+
+		if (info.argsz < minsz) {
+			rc = -EINVAL;
+			goto out;
+		}
+
+		info.flags = VFIO_DEVICE_FLAGS_PCI;
+		info.flags |= VFIO_DEVICE_FLAGS_RESET;
+		info.num_regions = VFIO_PCI_NUM_REGIONS + vfio_pdev->num_regions;
+		info.num_irqs = VFIO_PCI_NUM_IRQS + vfio_pdev->num_ext_irqs;
+
+		if (copy_to_user((void __user *)arg, &info, minsz))
+			rc = -EFAULT;
+		else
+			rc = 0;
+		goto out;
+	} else if (cmd == VFIO_DEVICE_GET_REGION_INFO) {
+		struct vfio_region_info info;
+		struct vfio_info_cap caps = { .buf = NULL, .size = 0 };
+		struct vfio_region_info_cap_sparse_mmap *sparse = NULL;
+		size_t size;
+		int nr_areas = 1;
+		int cap_type_id = 0;
+
+		minsz = offsetofend(struct vfio_region_info, offset);
+
+		if (copy_from_user(&info, (void __user *)arg, minsz)) {
+			rc = -EFAULT;
+			goto out;
+		}
+
+		if (info.argsz < minsz) {
+			rc = -EINVAL;
+			goto out;
+		}
+
+		switch (info.index) {
+		case VFIO_PCI_CONFIG_REGION_INDEX:
+			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
+			info.size = VIDXD_MAX_CFG_SPACE_SZ;
+			info.flags = VFIO_REGION_INFO_FLAG_READ | VFIO_REGION_INFO_FLAG_WRITE;
+			break;
+		case VFIO_PCI_BAR0_REGION_INDEX:
+			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
+			info.size = vidxd->bar_size[info.index];
+			if (!info.size) {
+				info.flags = 0;
+				break;
+			}
+
+			info.flags = VFIO_REGION_INFO_FLAG_READ | VFIO_REGION_INFO_FLAG_WRITE;
+			break;
+		case VFIO_PCI_BAR1_REGION_INDEX:
+			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
+			info.size = 0;
+			info.flags = 0;
+			break;
+		case VFIO_PCI_BAR2_REGION_INDEX:
+			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
+			info.flags = VFIO_REGION_INFO_FLAG_CAPS | VFIO_REGION_INFO_FLAG_MMAP |
+				     VFIO_REGION_INFO_FLAG_READ | VFIO_REGION_INFO_FLAG_WRITE |
+				     VFIO_REGION_INFO_FLAG_DYNAMIC_TRAP;
+			info.size = vidxd->bar_size[1];
+
+			/*
+			 * Every WQ has two areas for unlimited and limited
+			 * MSI-X portals. IMS portals are not reported. For shared
+			 * WQ, we will only allow limited portal.
+			 */
+			nr_areas = wq_dedicated(vidxd->wq) ? 2 : 1;
+
+			size = sizeof(*sparse) + (nr_areas * sizeof(*sparse->areas));
+			sparse = kzalloc(size, GFP_KERNEL);
+			if (!sparse) {
+				rc = -ENOMEM;
+				goto out;
+			}
+
+			sparse->header.id = VFIO_REGION_INFO_CAP_SPARSE_MMAP;
+			sparse->header.version = 1;
+			sparse->nr_areas = nr_areas;
+			cap_type_id = VFIO_REGION_INFO_CAP_SPARSE_MMAP;
+
+			/* Unlimited portal */
+			if (wq_dedicated(vidxd->wq)) {
+				sparse->areas[0].offset = 0;
+				sparse->areas[0].size = PAGE_SIZE;
+				sparse->areas[1].offset = PAGE_SIZE;
+				sparse->areas[1].size = PAGE_SIZE;
+			} else {
+			/* Limited portal */
+				sparse->areas[0].offset = PAGE_SIZE;
+				sparse->areas[0].size = PAGE_SIZE;
+			}
+
+			break;
+
+		case VFIO_PCI_BAR3_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:
+			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
+			info.size = 0;
+			info.flags = 0;
+			dev_dbg(dev, "get region info bar:%d\n", info.index);
+			break;
+
+		case VFIO_PCI_ROM_REGION_INDEX:
+		case VFIO_PCI_VGA_REGION_INDEX:
+			dev_dbg(dev, "get region info index:%d\n", info.index);
+			break;
+		default: {
+			struct vfio_region_info_cap_type cap_type = {
+				.header.id = VFIO_REGION_INFO_CAP_TYPE,
+				.header.version = 1,
+			};
+			int i;
+
+			if (info.index >= VFIO_PCI_NUM_REGIONS + vfio_pdev->num_regions) {
+				rc = -EINVAL;
+				goto out;
+			}
+
+			info.index = array_index_nospec(info.index,
+							VFIO_PCI_NUM_REGIONS +
+							vfio_pdev->num_regions);
+			i = info.index - VFIO_PCI_NUM_REGIONS;
+
+			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
+			info.size = vfio_pdev->region[i].size;
+			info.flags = vfio_pdev->region[i].flags;
+
+			cap_type.type = vfio_pdev->region[i].type;
+			cap_type.subtype = vfio_pdev->region[i].subtype;
+
+			rc = vfio_info_add_capability(&caps, &cap_type.header, sizeof(cap_type));
+			if (rc)
+				goto out;
+
+			if (vfio_pdev->region[i].ops->add_capability) {
+				rc = vfio_pdev->region[i].ops->add_capability(vfio_pdev,
+									  &vfio_pdev->region[i],
+									  &caps);
+				if (rc)
+					goto out;
+			}
+		} /* default */
+		} /* info.index switch */
+
+		if ((info.flags & VFIO_REGION_INFO_FLAG_CAPS) && sparse) {
+			if (cap_type_id == VFIO_REGION_INFO_CAP_SPARSE_MMAP) {
+				rc = vfio_info_add_capability(&caps, &sparse->header,
+							      sizeof(*sparse) + (sparse->nr_areas *
+							      sizeof(*sparse->areas)));
+				kfree(sparse);
+				if (rc)
+					goto out;
+			}
+		}
+
+		if (caps.size) {
+			info.flags |= VFIO_REGION_INFO_FLAG_CAPS;
+			if (info.argsz < sizeof(info) + caps.size) {
+				info.argsz = sizeof(info) + caps.size;
+				info.cap_offset = 0;
+			} else {
+				vfio_info_cap_shift(&caps, sizeof(info));
+				if (copy_to_user((void __user *)arg + sizeof(info),
+						 caps.buf, caps.size)) {
+					kfree(caps.buf);
+					rc = -EFAULT;
+					goto out;
+				}
+				info.cap_offset = sizeof(info);
+			}
+
+			kfree(caps.buf);
+		}
+		if (copy_to_user((void __user *)arg, &info, minsz))
+			rc = -EFAULT;
+		else
+			rc = 0;
+		goto out;
+	} else if (cmd == VFIO_DEVICE_GET_IRQ_INFO) {
+		struct vfio_irq_info info;
+		struct vfio_info_cap caps = {
+			.buf = NULL,
+			.size = 0
+		};
+		unsigned long capsz;
+
+		minsz = offsetofend(struct vfio_irq_info, count);
+		capsz = offsetofend(struct vfio_irq_info, cap_offset);
+
+		if (copy_from_user(&info, (void __user *)arg, minsz)) {
+			rc = -EFAULT;
+			goto out;
+		}
+
+		if (info.argsz < minsz ||
+		    info.index >= VFIO_PCI_NUM_IRQS + vfio_pdev->num_ext_irqs) {
+			rc = -EINVAL;
+			goto out;
+		}
+
+		if (info.argsz >= capsz)
+			minsz = capsz;
+
+		info.flags = VFIO_IRQ_INFO_EVENTFD;
+
+		switch (info.index) {
+		case VFIO_PCI_INTX_IRQ_INDEX:
+		case VFIO_PCI_MSI_IRQ_INDEX:
+		case VFIO_PCI_ERR_IRQ_INDEX:
+			rc = -EINVAL;
+			goto out;
+		case VFIO_PCI_MSIX_IRQ_INDEX:
+		case VFIO_PCI_REQ_IRQ_INDEX:
+			info.flags |= VFIO_IRQ_INFO_NORESIZE;
+			break;
+		default: {
+			struct vfio_irq_info_cap_type cap_type = {
+				.header.id = VFIO_IRQ_INFO_CAP_TYPE,
+				.header.version = 1
+			};
+			int i;
+
+			if (info.index >= VFIO_PCI_NUM_IRQS + vfio_pdev->num_ext_irqs)
+				return -EINVAL;
+			info.index = array_index_nospec(info.index,
+							VFIO_PCI_NUM_IRQS + vfio_pdev->num_ext_irqs);
+			i = info.index - VFIO_PCI_NUM_IRQS;
+
+			info.flags = vfio_pdev->ext_irqs[i].flags;
+			cap_type.type = vfio_pdev->ext_irqs[i].type;
+			cap_type.subtype = vfio_pdev->ext_irqs[i].subtype;
+
+			rc = vfio_info_add_capability(&caps, &cap_type.header, sizeof(cap_type));
+			if (rc)
+				goto out;
+			break;
+		}
+		} /* switch(info.index) */
+
+		info.count = idxd_vdcm_get_irq_count(mdev, info.index);
+		if (caps.size) {
+			info.flags |= VFIO_IRQ_INFO_FLAG_CAPS;
+			if (info.argsz < sizeof(info) + caps.size) {
+				info.argsz = sizeof(info) + caps.size;
+				info.cap_offset = 0;
+			} else {
+				vfio_info_cap_shift(&caps, sizeof(info));
+				if (copy_to_user((void __user *)arg + sizeof(info), caps.buf,
+						 caps.size)) {
+					kfree(caps.buf);
+					return -EFAULT;
+				}
+				info.cap_offset = sizeof(info);
+			}
+			kfree(caps.buf);
+		}
+
+		rc = copy_to_user((void __user *)arg, &info, minsz);
+		rc = rc ? -EFAULT : 0;
+		goto out;
+	} else if (cmd == VFIO_DEVICE_SET_IRQS) {
+		struct vfio_irq_set hdr;
+		u8 *data = NULL;
+		size_t data_size = 0;
+		int max;
+
+		minsz = offsetofend(struct vfio_irq_set, count);
+
+		if (copy_from_user(&hdr, (void __user *)arg, minsz)) {
+			rc = -EFAULT;
+			goto out;
+		}
+
+		max = idxd_vdcm_get_irq_count(mdev, hdr.index);
+		rc = vfio_set_irqs_validate_and_prepare(&hdr, max,
+							VFIO_PCI_NUM_IRQS +
+							vfio_pdev->num_ext_irqs,
+							&data_size);
+		if (rc) {
+			dev_err(dev, "intel:vfio_set_irqs_validate_and_prepare failed\n");
+			goto out;
+		}
+
+		if (data_size) {
+			data = memdup_user((void __user *)(arg + minsz), data_size);
+			if (IS_ERR(data)) {
+				rc = PTR_ERR(data);
+				goto out;
+			}
+		}
+		mutex_lock(&vidxd->vfio_pdev.igate);
+		rc = idxd_vdcm_set_irqs(vidxd, hdr.flags, hdr.index, hdr.start, hdr.count, data);
+		mutex_unlock(&vidxd->vfio_pdev.igate);
+		kfree(data);
+		goto out;
+	} else if (cmd == VFIO_DEVICE_RESET) {
+		vidxd_vdcm_reset(vidxd);
+	}
+
+ out:
+	mutex_unlock(&vidxd->dev_lock);
+	return rc;
+}
+
+static void idxd_vdcm_mdev_request(struct vfio_device *vdev, unsigned int count)
+{
+	struct vdcm_idxd *vidxd = vdev_to_vidxd(vdev);
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+
+	mutex_lock(&vfio_pdev->igate);
+	if (mdev->req_trigger) {
+		if (!(count % 10))
+			dev_info_ratelimited(mdev_dev(mdev),
+					     "Relaying device request to user (#%u)\n",
+					     count);
+		eventfd_signal(mdev->req_trigger, 1);
+	} else if (count == 0) {
+		dev_warn(mdev_dev(mdev),
+			 "No device request channel registered, blocked until released by user\n");
+	}
+
+	mutex_unlock(&vfio_pdev->igate);
+}
+
+static ssize_t name_show(struct mdev_type *mtype, struct mdev_type_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%s\n", idxd_mdev_types[mtype_get_type_group_id(mtype)].name);
+}
+static MDEV_TYPE_ATTR_RO(name);
+
+static int find_available_mdev_instances(struct idxd_device *idxd, struct vdcm_idxd_type *type)
+{
+	int count = 0, i;
+	unsigned long flags;
+
+	switch (type->type) {
+	case IDXD_MDEV_TYPE_DSA_1_DWQ:
+	case IDXD_MDEV_TYPE_DSA_1_SWQ:
+		if (idxd->data->type != IDXD_TYPE_DSA)
+			return 0;
+		break;
+	case IDXD_MDEV_TYPE_IAX_1_DWQ:
+	case IDXD_MDEV_TYPE_IAX_1_SWQ:
+		if (idxd->data->type != IDXD_TYPE_IAX)
+			return 0;
+		break;
+	default:
+		return 0;
+	}
+
+	spin_lock_irqsave(&idxd->dev_lock, flags);
+	for (i = 0; i < idxd->max_wqs; i++) {
+		struct idxd_wq *wq;
+
+		wq = idxd->wqs[i];
+
+		if (wq->state != IDXD_WQ_ENABLED)
+			continue;
+
+		if (!is_idxd_wq_mdev(wq))
+			continue;
+
+		switch (type->type) {
+		case IDXD_MDEV_TYPE_DSA_1_DWQ:
+		case IDXD_MDEV_TYPE_IAX_1_DWQ:
+			if (wq_dedicated(wq) && !idxd_wq_refcount(wq))
+				count++;
+			break;
+		case IDXD_MDEV_TYPE_DSA_1_SWQ:
+		case IDXD_MDEV_TYPE_IAX_1_SWQ:
+			if (!wq_dedicated(wq))
+				count++;
+			break;
+		default:
+			return 0;
+		}
+	}
+	spin_unlock_irqrestore(&idxd->dev_lock, flags);
+
+	return count;
+}
+
+static ssize_t available_instances_show(struct mdev_type *mtype,
+					struct mdev_type_attribute *attr,
+					char *buf)
+{
+	struct device *dev = mtype_get_parent_dev(mtype);
+	struct idxd_device *idxd = dev_get_drvdata(dev);
+	int count;
+	struct vdcm_idxd_type *type;
+
+	type = &idxd_mdev_types[mtype_get_type_group_id(mtype)];
+	count = find_available_mdev_instances(idxd, type);
+
+	return sprintf(buf, "%d\n", count);
+}
+static MDEV_TYPE_ATTR_RO(available_instances);
+
+static ssize_t device_api_show(struct mdev_type *mtype, struct mdev_type_attribute *attr,
+			       char *buf)
+{
+	return sprintf(buf, "%s\n", VFIO_DEVICE_API_PCI_STRING);
+}
+static MDEV_TYPE_ATTR_RO(device_api);
+
+static struct attribute *idxd_mdev_types_attrs[] = {
+	&mdev_type_attr_name.attr,
+	&mdev_type_attr_device_api.attr,
+	&mdev_type_attr_available_instances.attr,
+	NULL,
+};
+
+static struct attribute_group idxd_mdev_type_dsa_group0 = {
+	.name = idxd_dsa_1dwq_name,
+	.attrs = idxd_mdev_types_attrs,
+};
+
+static struct attribute_group idxd_mdev_type_iax_group0 = {
+	.name = idxd_iax_1dwq_name,
+	.attrs = idxd_mdev_types_attrs,
+};
+
+static struct attribute_group idxd_mdev_type_dsa_group1 = {
+	.name = idxd_dsa_1swq_name,
+	.attrs = idxd_mdev_types_attrs,
+};
+
+static struct attribute_group idxd_mdev_type_iax_group1 = {
+	.name = idxd_iax_1swq_name,
+	.attrs = idxd_mdev_types_attrs,
+};
+
+static struct attribute_group *idxd_mdev_type_groups[] = {
+	&idxd_mdev_type_dsa_group0,
+	&idxd_mdev_type_iax_group0,
+	&idxd_mdev_type_dsa_group1,
+	&idxd_mdev_type_iax_group1,
+	NULL,
+};
+
+static const struct vfio_device_ops idxd_mdev_ops = {
+	.name = "vfio-mdev",
+	.open_device = idxd_vdcm_open,
+	.close_device = idxd_vdcm_close,
+	.read = idxd_vdcm_read,
+	.write = idxd_vdcm_write,
+	.mmap = idxd_vdcm_mmap,
+	.ioctl = idxd_vdcm_ioctl,
+	.request = idxd_vdcm_mdev_request,
+};
+
+static struct mdev_driver idxd_vdcm_driver = {
+	.driver = {
+		.name = "idxd-mdev",
+		.owner = THIS_MODULE,
+		.mod_name = KBUILD_MODNAME,
+	},
+	.probe = idxd_vdcm_probe,
+	.remove = idxd_vdcm_remove,
+};
+
+static const struct mdev_parent_ops idxd_parent_ops = {
+	.owner = THIS_MODULE,
+	.device_driver = &idxd_vdcm_driver,
+	.supported_type_groups = idxd_mdev_type_groups,
+};
+
+static int idxd_mdev_drv_probe(struct idxd_dev *idxd_dev)
+{
+	struct device *dev = &idxd_dev->conf_dev;
+	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+	struct idxd_device *idxd = wq->idxd;
+	int rc;
+
+	if (idxd->state != IDXD_DEV_ENABLED)
+		return -ENXIO;
+
+	mutex_lock(&wq->wq_lock);
+	wq->type = IDXD_WQT_MDEV;
+
+	rc = __drv_enable_wq(wq);
+	mutex_unlock(&wq->wq_lock);
+	if (rc < 0)
+		return rc;
+
+	mutex_lock(&idxd->kref_lock);
+	/*
+	 * If kref == 1, that means there are no mdev clients and mdev has
+	 * not been registered.
+	 */
+	if (!idxd->mdev_host_init) {
+		kref_init(&idxd->mdev_kref);
+		rc = idxd_mdev_host_init(idxd, &idxd_parent_ops);
+		if (rc < 0) {
+			mutex_unlock(&idxd->kref_lock);
+			drv_disable_wq(wq);
+			dev_warn(dev, "mdev device init failed!\n");
+			return -ENXIO;
+		}
+	} else {
+		kref_get(&idxd->mdev_kref);
+	}
+	mutex_unlock(&idxd->kref_lock);
+
+	get_device(dev);
+	dev_info(dev, "wq %s enabled\n", dev_name(dev));
+	return 0;
+}
+
+static void idxd_mdev_drv_remove(struct idxd_dev *idxd_dev)
+{
+	struct device *dev = &idxd_dev->conf_dev;
+	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+	struct idxd_device *idxd = wq->idxd;
+
+
+	mutex_lock(&wq->wq_lock);
+	__drv_disable_wq(wq);
+
+	if (wq->state == IDXD_WQ_DISABLED) {
+		mutex_unlock(&wq->wq_lock);
+		return;
+	}
+
+	if (wq->state == IDXD_WQ_LOCKED)
+		wq->state = IDXD_WQ_DISABLED;
+	mutex_unlock(&wq->wq_lock);
+
+	mutex_lock(&idxd->kref_lock);
+	if (idxd->mdev_host_init)
+		kref_put(&idxd->mdev_kref, idxd_mdev_host_release);
+	mutex_unlock(&idxd->kref_lock);
+	put_device(dev);
+	dev_info(dev, "wq %s disabled\n", dev_name(dev));
+}
+
+static struct idxd_device_ops mdev_wq_ops = {
+	.notify_error = idxd_wq_vidxd_send_errors,
+};
+
+static enum idxd_dev_type dev_types[] = {
+	IDXD_DEV_WQ,
+	IDXD_DEV_NONE,
+};
+
+static struct idxd_device_driver idxd_mdev_driver = {
+	.probe = idxd_mdev_drv_probe,
+	.remove = idxd_mdev_drv_remove,
+	.name = "mdev",
+	.type = dev_types,
+	.ops = &mdev_wq_ops,
+};
+
+static int __init idxd_mdev_init(void)
+{
+	int rc;
+
+	rc = idxd_driver_register(&idxd_mdev_driver);
+	if (rc < 0)
+		return rc;
+
+	rc = mdev_register_driver(&idxd_vdcm_driver);
+	if (rc < 0) {
+		idxd_driver_unregister(&idxd_mdev_driver);
+		return rc;
+	}
+
+	return 0;
+}
+
+static void __exit idxd_mdev_exit(void)
+{
+	mdev_unregister_driver(&idxd_vdcm_driver);
+	idxd_driver_unregister(&idxd_mdev_driver);
+}
+
+module_init(idxd_mdev_init);
+module_exit(idxd_mdev_exit);
+
+MODULE_IMPORT_NS(IDXD);
+MODULE_SOFTDEP("pre: idxd");
+MODULE_SOFTDEP("pre: mdev");
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_ALIAS_IDXD_DEVICE(0);
diff --git a/drivers/vfio/mdev/idxd/mdev.h b/drivers/vfio/mdev/idxd/mdev.h
new file mode 100644
index 000000000000..775f4927b2e3
--- /dev/null
+++ b/drivers/vfio/mdev/idxd/mdev.h
@@ -0,0 +1,179 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2020 Intel Corporation. All rights rsvd. */
+
+#ifndef _IDXD_MDEV_H_
+#define _IDXD_MDEV_H_
+
+#include <linux/vfio.h>
+#include <linux/vfio_pci_core.h>
+
+/* two 64-bit BARs implemented */
+#define VIDXD_MAX_BARS 2
+#define VIDXD_MAX_CFG_SPACE_SZ 4096
+#define VIDXD_MAX_MMIO_SPACE_SZ 8192
+#define VIDXD_MSIX_TBL_SZ_OFFSET 0x42
+#define VIDXD_CAP_CTRL_SZ 0x100
+#define VIDXD_GRP_CTRL_SZ 0x100
+#define VIDXD_WQ_CTRL_SZ 0x100
+#define VIDXD_WQ_OCPY_INT_SZ 0x20
+#define VIDXD_MSIX_TBL_SZ 0x90
+#define VIDXD_MSIX_PERM_TBL_SZ 0x48
+
+#define VIDXD_MSIX_TABLE_OFFSET 0x600
+#define VIDXD_MSIX_PERM_OFFSET 0x300
+#define VIDXD_GRPCFG_OFFSET 0x400
+#define VIDXD_WQCFG_OFFSET 0x500
+#define VIDXD_IMS_OFFSET 0x1000
+
+#define VIDXD_BAR0_SIZE  0x2000
+#define VIDXD_BAR2_SIZE  0x2000
+#define VIDXD_MAX_MSIX_ENTRIES  (VIDXD_MSIX_TBL_SZ / 0x10)
+#define VIDXD_MAX_WQS	1
+#define VIDXD_MAX_MSIX_VECS	2
+
+#define VIDXD_ATS_OFFSET 0x100
+#define VIDXD_PRS_OFFSET 0x110
+#define VIDXD_PASID_OFFSET 0x120
+#define VIDXD_MSIX_PBA_OFFSET 0x700
+
+#define VIDXD_STATE_BUFFER_SIZE (4 * PAGE_SIZE)
+#define VIDXD_MAX_INTS 65536
+
+struct ioasid_mm_entry {
+	struct mm_struct *mm;
+	struct list_head node;
+};
+
+#define IDXD_DESC_SIZE sizeof(struct dsa_hw_desc)
+
+#define VIDXD_MAX_PORTALS 64
+
+struct idxd_wq_desc_elem {
+	enum idxd_portal_prot portal_prot;
+	u8  work_desc[IDXD_DESC_SIZE];
+	struct list_head link;
+};
+
+struct idxd_wq_portal {
+	u8 data[IDXD_DESC_SIZE];
+	unsigned int count;
+};
+
+struct idxd_virtual_wq {
+	unsigned int ndescs;
+	struct list_head head;
+	struct idxd_wq_portal portals[VIDXD_MAX_PORTALS];
+};
+
+struct idxd_vdev {
+	struct mdev_device *mdev;
+	struct vfio_group *vfio_group;
+	struct notifier_block pasid_nb;
+	struct mutex ioasid_lock;
+	struct list_head mm_list;
+};
+
+struct vdcm_idxd {
+	struct vfio_device vdev;
+	struct idxd_device *idxd;
+	struct idxd_wq *wq;
+	struct idxd_virtual_wq vwq;
+	struct idxd_vdev ivdev;
+	struct vdcm_idxd_type *type;
+	int num_wqs;
+
+	/* For VM use case */
+	u64 bar_val[VIDXD_MAX_BARS];
+	u64 bar_size[VIDXD_MAX_BARS];
+	u8 cfg[VIDXD_MAX_CFG_SPACE_SZ];
+	u8 bar0[VIDXD_MAX_MMIO_SPACE_SZ];
+	struct list_head list;
+	struct mutex dev_lock; /* lock for vidxd resources */
+	struct mutex mig_submit_lock;
+	bool paused;
+
+	int refcount;
+	struct vfio_pci_core_device vfio_pdev;
+};
+
+#define vdev_to_vidxd(vdev) container_of(vdev, struct vdcm_idxd, vdev)
+
+static inline struct vdcm_idxd *to_vidxd(struct idxd_vdev *vdev)
+{
+	return container_of(vdev, struct vdcm_idxd, vdev);
+}
+
+#define IDXD_MDEV_NAME_LEN 64
+
+enum idxd_mdev_type {
+	IDXD_MDEV_TYPE_NONE = -1,
+	IDXD_MDEV_TYPE_DSA_1_DWQ = 0,
+	IDXD_MDEV_TYPE_IAX_1_DWQ,
+	IDXD_MDEV_TYPE_DSA_1_SWQ,
+	IDXD_MDEV_TYPE_IAX_1_SWQ,
+};
+
+#define IDXD_MDEV_WQ_TYPES 	2
+#define IDXD_MDEV_TYPES		(IDXD_TYPE_MAX * IDXD_MDEV_WQ_TYPES)
+
+struct vdcm_idxd_type {
+	const char *name;
+	enum idxd_mdev_type type;
+	unsigned int avail_instance;
+};
+
+enum idxd_vdcm_rw {
+	IDXD_VDCM_READ = 0,
+	IDXD_VDCM_WRITE,
+};
+
+static inline u64 get_reg_val(void *buf, int size)
+{
+	u64 val = 0;
+
+	switch (size) {
+	case 8:
+		val = *(u64 *)buf;
+		break;
+	case 4:
+		val = *(u32 *)buf;
+		break;
+	case 2:
+		val = *(u16 *)buf;
+		break;
+	case 1:
+		val = *(u8 *)buf;
+		break;
+	}
+
+	return val;
+}
+
+static inline u8 vidxd_state(struct vdcm_idxd *vidxd)
+{
+	union gensts_reg *gensts = (union gensts_reg *)(vidxd->bar0 + IDXD_GENSTATS_OFFSET);
+
+	return gensts->state;
+}
+
+int idxd_mdev_host_init(struct idxd_device *idxd, const struct mdev_parent_ops *ops);
+void idxd_mdev_host_release(struct kref *kref);
+int idxd_mdev_get_pasid(struct mdev_device *mdev, u32 *pasid);
+int idxd_mdev_get_host_pasid(struct mdev_device *mdev, u32 gpasid, u32 *pasid);
+int vidxd_mmio_read(struct vdcm_idxd *vidxd, u64 pos, void *buf, unsigned int size);
+int vidxd_mmio_write(struct vdcm_idxd *vidxd, u64 pos, void *buf, unsigned int size);
+int vidxd_cfg_read(struct vdcm_idxd *vidxd, unsigned int pos, void *buf, unsigned int count);
+int vidxd_cfg_write(struct vdcm_idxd *vidxd, unsigned int pos, void *buf, unsigned int size);
+void vidxd_mmio_init(struct vdcm_idxd *vidxd);
+void vidxd_reset(struct vdcm_idxd *vidxd);
+void vidxd_send_interrupt(struct vdcm_idxd *vidxd, int msix_idx);
+void idxd_wq_vidxd_send_errors(struct idxd_wq *wq);
+
+int vidxd_portal_mmio_read(struct vdcm_idxd *vidxd, u64 pos, void *buf,
+				unsigned int size);
+int vidxd_portal_mmio_write(struct vdcm_idxd *vidxd, u64 pos, void *buf,
+				unsigned int size);
+
+void vidxd_notify_revoked_handles (struct vdcm_idxd *vidxd);
+
+#endif
diff --git a/drivers/vfio/mdev/idxd/mdev_host.c b/drivers/vfio/mdev/idxd/mdev_host.c
new file mode 100644
index 000000000000..b05858e22759
--- /dev/null
+++ b/drivers/vfio/mdev/idxd/mdev_host.c
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) Intel Corporation. All rights rsvd. */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/iommu.h>
+#include <linux/mdev.h>
+#include <linux/irqdomain.h>
+#include <linux/irqchip/irq-ims-msi.h>
+#include <uapi/linux/idxd.h>
+#include "idxd.h"
+#include "mdev.h"
+
+extern const struct vfio_pci_regops vfio_pci_dma_fault_regops;
+
+int idxd_mdev_host_init(struct idxd_device *idxd, const struct mdev_parent_ops *ops)
+{
+	struct device *dev = &idxd->pdev->dev;
+	struct ims_array_info ims_info;
+	int rc;
+
+	if (!test_bit(IDXD_FLAG_IMS_SUPPORTED, &idxd->flags))
+		return -EOPNOTSUPP;
+
+	rc = iommu_dev_enable_feature(dev, IOMMU_DEV_FEAT_AUX);
+	if (rc < 0) {
+		dev_warn(dev, "Failed to enable aux-domain: %d\n", rc);
+		return rc;
+	}
+
+	ims_info.max_slots = idxd->ims_size;
+	ims_info.slots = idxd->reg_base + idxd->ims_offset;
+	idxd->ims_domain = pci_ims_array_create_msi_irq_domain(idxd->pdev, &ims_info);
+	if (!idxd->ims_domain) {
+		dev_warn(dev, "Fail to acquire IMS domain\n");
+		iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_AUX);
+		return -ENODEV;
+	}
+
+	rc = mdev_register_device(dev, ops);
+	if (rc < 0) {
+		dev_warn(dev, "mdev register failed\n");
+		irq_domain_remove(idxd->ims_domain);
+		iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_AUX);
+		return rc;
+	}
+
+	mutex_init(&idxd->vfio_pdev.igate);
+	idxd->vfio_pdev.pdev = idxd->pdev;
+	rc = vfio_pci_dma_fault_init(&idxd->vfio_pdev, true);
+	if (rc < 0) {
+		dev_err(dev, "dma fault region init failed\n");
+		irq_domain_remove(idxd->ims_domain);
+		iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_AUX);
+		mdev_unregister_device(dev);
+		return rc;
+	}
+
+	idxd->mdev_host_init = true;
+	return 0;
+}
+
+void idxd_mdev_host_release(struct kref *kref)
+{
+	struct idxd_device *idxd = container_of(kref, struct idxd_device, mdev_kref);
+	struct device *dev = &idxd->pdev->dev;
+	struct vfio_pci_core_device *vfio_pdev = &idxd->vfio_pdev;
+	int i;
+
+	if (!idxd->mdev_host_init)
+		return;
+
+	WARN_ON(iommu_unregister_device_fault_handler(dev));
+
+	for (i = 0; i < vfio_pdev->num_regions; i++)
+		vfio_pdev->region[i].ops->release(vfio_pdev, &vfio_pdev->region[i]);
+	vfio_pdev->num_regions = 0;
+	kfree(vfio_pdev->region);
+	vfio_pdev->region = NULL;
+	iommu_unregister_device_fault_handler(&idxd->pdev->dev);
+	for (i = 0; i < vfio_pdev->num_ext_irqs; i++)
+		vfio_pci_set_ext_irq_trigger(vfio_pdev,
+					     VFIO_IRQ_SET_DATA_NONE | VFIO_IRQ_SET_ACTION_TRIGGER,
+					     VFIO_PCI_NUM_IRQS + i, 0, 0, NULL);
+	vfio_pdev->num_ext_irqs = 0;
+	kfree(vfio_pdev->ext_irqs);
+	vfio_pdev->ext_irqs = NULL;
+
+	irq_domain_remove(idxd->ims_domain);
+	mdev_unregister_device(dev);
+	iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_AUX);
+	idxd->mdev_host_init = false;
+}
diff --git a/drivers/vfio/mdev/idxd/vdev.c b/drivers/vfio/mdev/idxd/vdev.c
new file mode 100644
index 000000000000..42019cfaa371
--- /dev/null
+++ b/drivers/vfio/mdev/idxd/vdev.c
@@ -0,0 +1,1397 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2019,2020 Intel Corporation. All rights rsvd. */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/device.h>
+#include <linux/sched/task.h>
+#include <linux/io-64-nonatomic-lo-hi.h>
+#include <linux/mm.h>
+#include <linux/mmu_context.h>
+#include <linux/vfio.h>
+#include <linux/mdev.h>
+#include <linux/msi.h>
+#include <linux/intel-iommu.h>
+#include <linux/intel-svm.h>
+#include <linux/kvm_host.h>
+#include <linux/eventfd.h>
+#include <linux/irqchip/irq-ims-msi.h>
+#include <uapi/linux/idxd.h>
+#include "registers.h"
+#include "idxd.h"
+#include "../mdev_private.h"
+#include "mdev.h"
+
+static void vidxd_do_command(struct vdcm_idxd *vidxd, u32 val);
+
+void vidxd_send_interrupt(struct vdcm_idxd *vidxd, int msix_idx)
+{
+	struct vfio_pci_core_device *vfio_pdev = &vidxd->vfio_pdev;
+
+	eventfd_signal(vfio_pdev->ctx[msix_idx].trigger, 1);
+}
+
+static void vidxd_report_error(struct vdcm_idxd *vidxd, unsigned int error)
+{
+	u8 *bar0 = vidxd->bar0;
+	union sw_err_reg *swerr = (union sw_err_reg *)(bar0 + IDXD_SWERR_OFFSET);
+	union genctrl_reg *genctrl;
+	bool send = false;
+
+	if (!swerr->valid) {
+		memset(swerr, 0, sizeof(*swerr));
+		swerr->valid = 1;
+		swerr->error = error;
+		send = true;
+	} else if (swerr->valid && !swerr->overflow) {
+		swerr->overflow = 1;
+	}
+
+	genctrl = (union genctrl_reg *)(bar0 + IDXD_GENCTRL_OFFSET);
+	if (send && genctrl->softerr_int_en) {
+		u32 *intcause = (u32 *)(bar0 + IDXD_INTCAUSE_OFFSET);
+
+		*intcause |= IDXD_INTC_ERR;
+		vidxd_send_interrupt(vidxd, 0);
+	}
+}
+
+void vidxd_notify_revoked_handles (struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+	u32 *intcause = (u32 *)(bar0 + IDXD_INTCAUSE_OFFSET);
+
+	*intcause |= IDXD_INTC_INT_HANDLE_REVOKED;
+	pr_info("informating guest about revoked handles\n");
+	vidxd_send_interrupt(vidxd, 0);
+}
+
+static int vidxd_set_ims_pasid(struct vdcm_idxd *vidxd, int index, bool pasid_en, u32 gpasid)
+{
+	struct device *dev = mdev_dev(vidxd->ivdev.mdev);
+	u64 auxval;
+	u32 pasid;
+	int irq;
+	int rc;
+
+	irq = dev_msi_irq_vector(dev, index);
+
+	if (pasid_en)
+		rc = idxd_mdev_get_host_pasid(vidxd->ivdev.mdev, gpasid, &pasid);
+	else
+		rc = idxd_mdev_get_pasid(vidxd->ivdev.mdev, &pasid);
+	if (rc < 0)
+		return rc;
+	auxval = ims_ctrl_pasid_aux(pasid, 1);
+	return irq_set_auxdata(irq, IMS_AUXDATA_CONTROL_WORD, auxval);
+
+}
+
+int vidxd_mmio_write(struct vdcm_idxd *vidxd, u64 pos, void *buf, unsigned int size)
+{
+	u32 offset = pos & (vidxd->bar_size[0] - 1);
+	u8 *bar0 = vidxd->bar0;
+	struct device *dev = mdev_dev(vidxd->ivdev.mdev);
+
+	dev_dbg(dev, "vidxd mmio W %d %x %x: %llx\n", vidxd->wq->id, size,
+		offset, get_reg_val(buf, size));
+
+	if (((size & (size - 1)) != 0) || (offset & (size - 1)) != 0)
+		return -EINVAL;
+
+	/* If we don't limit this, we potentially can write out of bound */
+	if (size > sizeof(u32))
+		return -EINVAL;
+
+	switch (offset) {
+	case IDXD_GENCFG_OFFSET ... IDXD_GENCFG_OFFSET + 3:
+		/* Write only when device is disabled. */
+		if (vidxd_state(vidxd) == IDXD_DEVICE_STATE_DISABLED)
+			memcpy(bar0 + offset, buf, size);
+		break;
+
+	case IDXD_GENCTRL_OFFSET:
+		memcpy(bar0 + offset, buf, size);
+		break;
+
+	case IDXD_INTCAUSE_OFFSET:
+		*(u32 *)&bar0[offset] &= ~(get_reg_val(buf, 4));
+		break;
+
+	case IDXD_CMD_OFFSET: {
+		u32 *cmdsts = (u32 *)(bar0 + IDXD_CMDSTS_OFFSET);
+		u32 val = get_reg_val(buf, size);
+
+		if (size != sizeof(u32))
+			return -EINVAL;
+
+		/* Check and set command in progress */
+		if (test_and_set_bit(IDXD_CMDS_ACTIVE_BIT, (unsigned long *)cmdsts) == 0)
+			vidxd_do_command(vidxd, val);
+		else
+			vidxd_report_error(vidxd, DSA_ERR_CMD_REG);
+		break;
+	}
+
+	case IDXD_SWERR_OFFSET:
+		/* W1C */
+		bar0[offset] &= ~(get_reg_val(buf, 1) & GENMASK(1, 0));
+		break;
+
+	case VIDXD_WQCFG_OFFSET ... VIDXD_WQCFG_OFFSET + VIDXD_WQ_CTRL_SZ - 1: {
+		union wqcfg *wqcfg;
+		int wq_id = (offset - VIDXD_WQCFG_OFFSET) / 0x20;
+		int subreg = offset & 0x1c;
+		u32 new_val;
+
+		if (wq_id >= VIDXD_MAX_WQS)
+			break;
+
+		/* FIXME: Need to sanitize for RO Config WQ mode 1 */
+		wqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET + wq_id * 0x20);
+		if (size >= 4) {
+			new_val = get_reg_val(buf, 4);
+		} else {
+			u32 tmp1, tmp2, shift, mask;
+
+			switch (subreg) {
+			case 4:
+				tmp1 = wqcfg->bits[1];
+				break;
+			case 8:
+				tmp1 = wqcfg->bits[2];
+				break;
+			case 12:
+				tmp1 = wqcfg->bits[3];
+				break;
+			case 16:
+				tmp1 = wqcfg->bits[4];
+				break;
+			case 20:
+				tmp1 = wqcfg->bits[5];
+				break;
+			default:
+				tmp1 = 0;
+			}
+
+			tmp2 = get_reg_val(buf, size);
+			shift = (offset & 0x03U) * 8;
+			mask = ((1U << size * 8) - 1u) << shift;
+			new_val = (tmp1 & ~mask) | (tmp2 << shift);
+		}
+
+		if (subreg == 8) {
+			if (wqcfg->wq_state == 0) {
+				wqcfg->bits[2] &= 0xfe;
+				wqcfg->bits[2] |= new_val & 0xffffff01;
+			}
+		}
+
+		break;
+	} /* WQCFG */
+
+	case VIDXD_GRPCFG_OFFSET ...  VIDXD_GRPCFG_OFFSET + VIDXD_GRP_CTRL_SZ - 1:
+		/* Nothing is written. Should be all RO */
+		break;
+
+	case VIDXD_MSIX_TABLE_OFFSET ...  VIDXD_MSIX_TABLE_OFFSET + VIDXD_MSIX_TBL_SZ - 1: {
+		int index = (offset - VIDXD_MSIX_TABLE_OFFSET) / 0x10;
+		u8 *msix_entry = &bar0[VIDXD_MSIX_TABLE_OFFSET + index * 0x10];
+		u64 *pba = (u64 *)(bar0 + VIDXD_MSIX_PBA_OFFSET);
+		u8 ctrl;
+
+		ctrl = msix_entry[MSIX_ENTRY_CTRL_BYTE];
+		memcpy(bar0 + offset, buf, size);
+		/* Handle clearing of UNMASK bit */
+		if (!(msix_entry[MSIX_ENTRY_CTRL_BYTE] & MSIX_ENTRY_MASK_INT) &&
+		    ctrl & MSIX_ENTRY_MASK_INT)
+			if (test_and_clear_bit(index, (unsigned long *)pba))
+				vidxd_send_interrupt(vidxd, index);
+		break;
+	}
+
+	case VIDXD_MSIX_PERM_OFFSET ...  VIDXD_MSIX_PERM_OFFSET + VIDXD_MSIX_PERM_TBL_SZ - 1: {
+		int index, rc;
+		u32 msix_perm;
+		u32 pasid, pasid_en;
+
+		if (size != sizeof(u32) || !IS_ALIGNED(offset, sizeof(u64))) {
+			dev_warn(dev, "XXX unaligned MSIX PERM access\n");
+			break;
+		}
+
+		index = (offset - VIDXD_MSIX_PERM_OFFSET) / 8;
+		msix_perm = get_reg_val(buf, sizeof(u32)) & 0xfffff00d;
+		memcpy(bar0 + offset, buf, size);
+		/*
+		 * index 0 for MSIX is emulated for misc interrupts. The MSIX indices from
+		 * 1...N are backed by IMS. Here we would pass in index - 1, which is 0 for
+		 * the first one
+		 */
+		if (index > 0) {
+			pasid_en = (msix_perm >> 3) & 1;
+
+			/*
+			 * When vSVA is turned on, this is the only place where the guest PASID
+			 * can be retrieved by the host. The guest driver writes the PASID to the
+			 * MSIX permission entry. In turn the vdcm will translate this to the
+			 * IMS entry.
+			 */
+
+			if (pasid_en) {
+				pasid = (msix_perm >> 12) & 0xfffff;
+				if (!pasid)
+					break;
+			}
+			rc = vidxd_set_ims_pasid(vidxd, index - 1, pasid_en, pasid);
+			if (rc < 0)
+				return rc;
+		}
+		break;
+	}
+	} /* offset */
+
+	return 0;
+}
+
+int vidxd_portal_mmio_read(struct vdcm_idxd *vidxd, u64 pos, void *buf,
+                                unsigned int size)
+{
+	u32 offset = pos & (vidxd->bar_size[1] - 1);
+	struct device *dev = mdev_dev(vidxd->ivdev.mdev);
+
+	BUG_ON((size & (size - 1)) != 0);
+	BUG_ON(size > 8);
+	BUG_ON((offset & (size - 1)) != 0);
+
+	memset(buf, 0xff, size);
+
+	dev_dbg(dev, "vidxd portal mmio R %d %x %x: %llx\n",
+		vidxd->wq->id, size, offset, get_reg_val(buf, size));
+	return 0;
+}
+
+int vidxd_portal_mmio_write(struct vdcm_idxd *vidxd, u64 pos, void *buf,
+				unsigned int size)
+{
+	struct device *dev = mdev_dev(vidxd->ivdev.mdev);
+	u32 offset = pos & (vidxd->bar_size[1] - 1);
+	uint16_t wq_id = offset >> 14;
+	uint16_t portal_id, portal_offset;
+	struct idxd_virtual_wq *vwq;
+	struct idxd_wq *wq;
+	struct idxd_wq_portal *portal;
+	enum idxd_portal_prot portal_prot = IDXD_PORTAL_UNLIMITED;
+
+	BUG_ON((size & (size - 1)) != 0);
+	BUG_ON(size > 64);
+	BUG_ON((offset & (size - 1)) != 0);
+
+	dev_dbg(dev, "vidxd portal mmio W %d %x %x: %llx\n", vidxd->wq->id, size,
+			offset, get_reg_val(buf, size));
+
+	if (wq_id >= vidxd->num_wqs) {
+		printk("DSA portal write: Invalid wq  %d\n", wq_id);
+	}
+
+	vwq = &vidxd->vwq;
+	wq = vidxd->wq;
+
+	if (!wq_dedicated(wq) || (((offset >> PAGE_SHIFT) & 0x3) == 1))
+		portal_prot = IDXD_PORTAL_LIMITED;
+
+	portal_id = (offset & 0xFFF) >> 6;
+	portal_offset = offset & 0x3F;
+
+	portal = &vwq->portals[portal_id];
+
+	portal->count += size;
+	memcpy(&portal->data[portal_offset], buf, size);
+
+	if (portal->count == IDXD_DESC_SIZE) {
+		struct idxd_wq_desc_elem *elem;
+		u64 *p = (u64 *)portal->data;
+		printk("desc: %016llx %016llx  %016llx %016llx %016llx %016llx %016llx %016llx\n",
+				p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7]);
+
+		if (wq_dedicated(wq) && vwq->ndescs == wq->size) {
+			printk("can't submit more descriptors than WQ size. Dropping.\n");
+			memset(&portal->data, 0, IDXD_DESC_SIZE);
+			portal->count = 0;
+			return 0;
+		}
+
+		mutex_lock(&vidxd->mig_submit_lock);
+		if (vidxd->paused) {
+			elem = kmalloc(sizeof(struct idxd_wq_desc_elem),
+					GFP_KERNEL);
+
+			if (elem == NULL) {
+				printk("kmalloc failed\n");
+				return 0;
+			}
+			printk("queuing the desc\n");
+			memcpy(elem->work_desc, portal->data, IDXD_DESC_SIZE);
+			elem->portal_prot = portal_prot;
+
+			list_add_tail(&elem->link, &vwq->head);
+			vwq->ndescs++;
+               } else {
+			void __iomem *wq_portal;
+
+			wq_portal = wq->portal;
+			printk("submitting a desc to WQ %d ded %d\n", wq->id,
+					wq_dedicated(wq));
+			if (wq_dedicated(wq)) {
+				iosubmit_cmds512(wq_portal, (struct dsa_hw_desc *)p, 1);
+			} else {
+				int rc;
+				rc = enqcmds(wq_portal, (struct dsa_hw_desc *)p);
+				if (rc < 0) {
+					pr_info("%s: enqcmds failed\n", __func__);
+					return rc;
+				}
+			}
+		}
+		mutex_unlock(&vidxd->mig_submit_lock);
+		memset(&portal->data, 0, IDXD_DESC_SIZE);
+		portal->count = 0;
+	}
+
+	return 0;
+}
+
+int vidxd_mmio_read(struct vdcm_idxd *vidxd, u64 pos, void *buf, unsigned int size)
+{
+	u32 offset = pos & (vidxd->bar_size[0] - 1);
+	struct device *dev = mdev_dev(vidxd->ivdev.mdev);
+
+	memcpy(buf, vidxd->bar0 + offset, size);
+
+	dev_dbg(dev, "vidxd mmio R %d %x %x: %llx\n",
+		vidxd->wq->id, size, offset, get_reg_val(buf, size));
+	return 0;
+}
+
+int vidxd_cfg_read(struct vdcm_idxd *vidxd, unsigned int pos, void *buf, unsigned int count)
+{
+	u32 offset = pos & 0xfff;
+	struct device *dev = mdev_dev(vidxd->ivdev.mdev);
+
+	memcpy(buf, &vidxd->cfg[offset], count);
+
+	dev_dbg(dev, "vidxd pci R %d %x %x: %llx\n",
+		vidxd->wq->id, count, offset, get_reg_val(buf, count));
+
+	return 0;
+}
+
+/*
+ * Much of the emulation code has been borrowed from Intel i915 cfg space
+ * emulation code.
+ * drivers/gpu/drm/i915/gvt/cfg_space.c:
+ */
+
+/*
+ * Bitmap for writable bits (RW or RW1C bits, but cannot co-exist in one
+ * byte) byte by byte in standard pci configuration space. (not the full
+ * 256 bytes.)
+ */
+static const u8 pci_cfg_space_rw_bmp[PCI_INTERRUPT_LINE + 4] = {
+	[PCI_COMMAND]		= 0xff, 0x07,
+	[PCI_STATUS]		= 0x00, 0xf9, /* the only one RW1C byte */
+	[PCI_CACHE_LINE_SIZE]	= 0xff,
+	[PCI_BASE_ADDRESS_0 ... PCI_CARDBUS_CIS - 1] = 0xff,
+	[PCI_ROM_ADDRESS]	= 0x01, 0xf8, 0xff, 0xff,
+	[PCI_INTERRUPT_LINE]	= 0xff,
+};
+
+static void _pci_cfg_mem_write(struct vdcm_idxd *vidxd, unsigned int off, u8 *src,
+			       unsigned int bytes)
+{
+	u8 *cfg_base = vidxd->cfg;
+	u8 mask, new, old;
+	int i = 0;
+
+	for (; i < bytes && (off + i < sizeof(pci_cfg_space_rw_bmp)); i++) {
+		mask = pci_cfg_space_rw_bmp[off + i];
+		old = cfg_base[off + i];
+		new = src[i] & mask;
+
+		/**
+		 * The PCI_STATUS high byte has RW1C bits, here
+		 * emulates clear by writing 1 for these bits.
+		 * Writing a 0b to RW1C bits has no effect.
+		 */
+		if (off + i == PCI_STATUS + 1)
+			new = (~new & old) & mask;
+
+		cfg_base[off + i] = (old & ~mask) | new;
+	}
+
+	/* For other configuration space directly copy as it is. */
+	if (i < bytes)
+		memcpy(cfg_base + off + i, src + i, bytes - i);
+}
+
+static inline void _write_pci_bar(struct vdcm_idxd *vidxd, u32 offset, u32 val, bool low)
+{
+	u32 *pval;
+
+	/* BAR offset should be 32 bits algiend */
+	offset = rounddown(offset, 4);
+	pval = (u32 *)(vidxd->cfg + offset);
+
+	if (low) {
+		/*
+		 * only update bit 31 - bit 4,
+		 * leave the bit 3 - bit 0 unchanged.
+		 */
+		*pval = (val & GENMASK(31, 4)) | (*pval & GENMASK(3, 0));
+	} else {
+		*pval = val;
+	}
+}
+
+static int _pci_cfg_bar_write(struct vdcm_idxd *vidxd, unsigned int offset, void *p_data,
+			      unsigned int bytes)
+{
+	u32 new = *(u32 *)(p_data);
+	bool lo = IS_ALIGNED(offset, 8);
+	u64 size;
+	unsigned int bar_id;
+
+	/*
+	 * Power-up software can determine how much address
+	 * space the device requires by writing a value of
+	 * all 1's to the register and then reading the value
+	 * back. The device will return 0's in all don't-care
+	 * address bits.
+	 */
+	if (new == 0xffffffff) {
+		switch (offset) {
+		case PCI_BASE_ADDRESS_0:
+		case PCI_BASE_ADDRESS_1:
+		case PCI_BASE_ADDRESS_2:
+		case PCI_BASE_ADDRESS_3:
+			bar_id = (offset - PCI_BASE_ADDRESS_0) / 8;
+			size = vidxd->bar_size[bar_id];
+			_write_pci_bar(vidxd, offset, size >> (lo ? 0 : 32), lo);
+			break;
+		default:
+			/* Unimplemented BARs */
+			_write_pci_bar(vidxd, offset, 0x0, false);
+		}
+	} else {
+		switch (offset) {
+		case PCI_BASE_ADDRESS_0:
+		case PCI_BASE_ADDRESS_1:
+		case PCI_BASE_ADDRESS_2:
+		case PCI_BASE_ADDRESS_3:
+			_write_pci_bar(vidxd, offset, new, lo);
+			break;
+		default:
+			break;
+		}
+	}
+	return 0;
+}
+
+int vidxd_cfg_write(struct vdcm_idxd *vidxd, unsigned int pos, void *buf, unsigned int size)
+{
+	struct device *dev = &vidxd->idxd->pdev->dev;
+	u8 *cfg = vidxd->cfg;
+	u32 offset = pos & 0xfff;
+	u64 val;
+
+	if (size > 4)
+		return -EINVAL;
+
+	if (pos + size > VIDXD_MAX_CFG_SPACE_SZ)
+		return -EINVAL;
+
+	dev_dbg(dev, "vidxd pci W %d %x %x: %llx\n", vidxd->wq->id, size, pos,
+		get_reg_val(buf, size));
+
+	/* First check if it's PCI_COMMAND */
+	if (IS_ALIGNED(pos, 2) && pos == PCI_COMMAND) {
+		bool new_bme;
+		bool bme;
+
+		if (size > 2)
+			return -EINVAL;
+
+		new_bme = !!(get_reg_val(buf, 2) & PCI_COMMAND_MASTER);
+		bme = !!(vidxd->cfg[pos] & PCI_COMMAND_MASTER);
+		_pci_cfg_mem_write(vidxd, pos, buf, size);
+
+		/* Flag error if turning off BME while device is enabled */
+		if ((bme && !new_bme) && vidxd_state(vidxd) == IDXD_DEVICE_STATE_ENABLED)
+			vidxd_report_error(vidxd, DSA_ERR_PCI_CFG);
+		return 0;
+	}
+
+	switch (pos) {
+	case PCI_BASE_ADDRESS_0 ... PCI_BASE_ADDRESS_5:
+		if (!IS_ALIGNED(pos, 4))
+			return -EINVAL;
+		return _pci_cfg_bar_write(vidxd, pos, buf, size);
+
+	case VIDXD_ATS_OFFSET + 4:
+		if (size < 4)
+			break;
+		offset += 2;
+		buf = buf + 2;
+		size -= 2;
+		fallthrough;
+
+	case VIDXD_ATS_OFFSET + 6:
+		memcpy(&cfg[offset], buf, size);
+		break;
+
+	case VIDXD_PRS_OFFSET + 4: {
+		u8 old_val, new_val;
+
+		val = get_reg_val(buf, 1);
+		old_val = cfg[VIDXD_PRS_OFFSET + 4];
+		new_val = val & 1;
+
+		cfg[offset] = new_val;
+		if (old_val == 0 && new_val == 1) {
+			/*
+			 * Clear Stopped, Response Failure,
+			 * and Unexpected Response.
+			 */
+			*(u16 *)&cfg[VIDXD_PRS_OFFSET + 6] &= ~(u16)(0x0103);
+		}
+
+		if (size < 4)
+			break;
+
+		offset += 2;
+		buf = (u8 *)buf + 2;
+		size -= 2;
+		fallthrough;
+	}
+
+	case VIDXD_PRS_OFFSET + 6:
+		cfg[offset] &= ~(get_reg_val(buf, 1) & 3);
+		break;
+
+	case VIDXD_PRS_OFFSET + 12 ... VIDXD_PRS_OFFSET + 15:
+		memcpy(&cfg[offset], buf, size);
+		break;
+
+	case VIDXD_PASID_OFFSET + 4:
+		if (size < 4)
+			break;
+		offset += 2;
+		buf = buf + 2;
+		size -= 2;
+		fallthrough;
+
+	case VIDXD_PASID_OFFSET + 6:
+		cfg[offset] = get_reg_val(buf, 1) & 5;
+		break;
+
+	default:
+		_pci_cfg_mem_write(vidxd, pos, buf, size);
+	}
+	return 0;
+}
+
+static void vidxd_mmio_init_grpcap(struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+	union group_cap_reg *grp_cap = (union group_cap_reg *)(bar0 + IDXD_GRPCAP_OFFSET);
+
+	/* single group for current implementation */
+	grp_cap->token_en = 0;
+	grp_cap->token_limit = 0;
+	grp_cap->total_tokens = 0;
+	grp_cap->num_groups = 1;
+}
+
+static void vidxd_mmio_init_grpcfg(struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+	struct grpcfg *grpcfg = (struct grpcfg *)(bar0 + VIDXD_GRPCFG_OFFSET);
+	struct idxd_wq *wq = vidxd->wq;
+	struct idxd_group *group = wq->group;
+	int i;
+
+	/*
+	 * At this point, we are only exporting a single workqueue for
+	 * each mdev. So we need to just fake it as first workqueue
+	 * and also mark the available engines in this group.
+	 */
+
+	/* Set single workqueue and the first one */
+	grpcfg->wqs[0] = BIT(0);
+	grpcfg->engines = 0;
+	for (i = 0; i < group->num_engines; i++)
+		grpcfg->engines |= BIT(i);
+	grpcfg->flags.bits = group->grpcfg.flags.bits;
+}
+
+static void vidxd_mmio_init_wqcap(struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+	struct idxd_wq *wq = vidxd->wq;
+	union wq_cap_reg *wq_cap = (union wq_cap_reg *)(bar0 + IDXD_WQCAP_OFFSET);
+
+	wq_cap->occupancy_int = 0;
+	wq_cap->occupancy = 0;
+	wq_cap->priority = 0;
+	wq_cap->total_wq_size = wq->size;
+	wq_cap->num_wqs = VIDXD_MAX_WQS;
+	wq_cap->wq_ats_support = 0;
+	if (wq_dedicated(wq))
+		wq_cap->dedicated_mode = 1;
+	else
+		wq_cap->shared_mode = 1;
+}
+
+static void vidxd_mmio_init_wqcfg(struct vdcm_idxd *vidxd)
+{
+	struct idxd_device *idxd = vidxd->idxd;
+	struct idxd_wq *wq = vidxd->wq;
+	u8 *bar0 = vidxd->bar0;
+	union wqcfg *wqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET);
+
+	wqcfg->wq_size = wq->size;
+	wqcfg->wq_thresh = wq->threshold;
+
+	if (wq_dedicated(wq))
+		wqcfg->mode = WQCFG_MODE_DEDICATED;
+	else if (device_pasid_enabled(idxd))
+		wqcfg->pasid_en = 1;
+
+	wqcfg->bof = wq->wqcfg->bof;
+
+	wqcfg->priority = wq->priority;
+	wqcfg->max_xfer_shift = idxd->hw.gen_cap.max_xfer_shift;
+	wqcfg->max_batch_shift = idxd->hw.gen_cap.max_batch_shift;
+	wqcfg->mode_support = 1;
+}
+
+static void vidxd_mmio_init_engcap(struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+	union engine_cap_reg *engcap = (union engine_cap_reg *)(bar0 + IDXD_ENGCAP_OFFSET);
+	struct idxd_wq *wq = vidxd->wq;
+	struct idxd_group *group = wq->group;
+
+	engcap->num_engines = group->num_engines;
+}
+
+static void vidxd_mmio_init_gencap(struct vdcm_idxd *vidxd)
+{
+	struct idxd_device *idxd = vidxd->idxd;
+	u8 *bar0 = vidxd->bar0;
+	union gen_cap_reg *gencap = (union gen_cap_reg *)(bar0 + IDXD_GENCAP_OFFSET);
+
+	gencap->bits = idxd->hw.gen_cap.bits;
+	gencap->config_en = 0;
+	gencap->max_ims_mult = 0;
+	gencap->cmd_cap = 1;
+	if (device_pasid_enabled(idxd))
+		gencap->block_on_fault = 1;
+}
+
+static void vidxd_mmio_init_cmdcap(struct vdcm_idxd *vidxd)
+{
+	struct idxd_device *idxd = vidxd->idxd;
+	u8 *bar0 = vidxd->bar0;
+	u32 *cmdcap = (u32 *)(bar0 + IDXD_CMDCAP_OFFSET);
+
+	if (idxd->hw.cmd_cap)
+		*cmdcap = idxd->hw.cmd_cap;
+	else
+		*cmdcap = 0x1ffe;
+
+	*cmdcap |= BIT(IDXD_CMD_REQUEST_INT_HANDLE) | BIT(IDXD_CMD_RELEASE_INT_HANDLE) |
+			BIT(IDXD_CMD_REVOKED_HANDLES_PROCESSED);
+}
+
+static void vidxd_mmio_init_opcap(struct vdcm_idxd *vidxd)
+{
+	struct idxd_device *idxd = vidxd->idxd;
+	u64 opcode;
+	u8 *bar0 = vidxd->bar0;
+	u64 *opcap = (u64 *)(bar0 + IDXD_OPCAP_OFFSET);
+
+	if (idxd->data->type == IDXD_TYPE_DSA) {
+		opcode = BIT_ULL(DSA_OPCODE_NOOP) | BIT_ULL(DSA_OPCODE_BATCH) |
+			 BIT_ULL(DSA_OPCODE_DRAIN) | BIT_ULL(DSA_OPCODE_MEMMOVE) |
+			 BIT_ULL(DSA_OPCODE_MEMFILL) | BIT_ULL(DSA_OPCODE_COMPARE) |
+			 BIT_ULL(DSA_OPCODE_COMPVAL) | BIT_ULL(DSA_OPCODE_CR_DELTA) |
+			 BIT_ULL(DSA_OPCODE_AP_DELTA) | BIT_ULL(DSA_OPCODE_DUALCAST) |
+			 BIT_ULL(DSA_OPCODE_CRCGEN) | BIT_ULL(DSA_OPCODE_COPY_CRC) |
+			 BIT_ULL(DSA_OPCODE_DIF_CHECK) | BIT_ULL(DSA_OPCODE_DIF_INS) |
+			 BIT_ULL(DSA_OPCODE_DIF_STRP) | BIT_ULL(DSA_OPCODE_DIF_UPDT) |
+			 BIT_ULL(DSA_OPCODE_CFLUSH);
+		*opcap = opcode;
+	} else if (idxd->data->type == IDXD_TYPE_IAX) {
+		opcode = BIT_ULL(IAX_OPCODE_NOOP) | BIT_ULL(IAX_OPCODE_DRAIN) |
+			 BIT_ULL(IAX_OPCODE_MEMMOVE);
+		*opcap = opcode;
+		opcap++;
+		opcode = OPCAP_BIT(IAX_OPCODE_DECOMPRESS) | OPCAP_BIT(IAX_OPCODE_COMPRESS) |
+			 OPCAP_BIT(IAX_OPCODE_CRC64) | OPCAP_BIT(IAX_OPCODE_ZERO_DECOMP_32) |
+			 OPCAP_BIT(IAX_OPCODE_ZERO_DECOMP_16) | OPCAP_BIT(IAX_OPCODE_DECOMP_32) |
+			 OPCAP_BIT(IAX_OPCODE_DECOMP_16) | OPCAP_BIT(IAX_OPCODE_SCAN) |
+			 OPCAP_BIT(IAX_OPCODE_SET_MEMBER) | OPCAP_BIT(IAX_OPCODE_EXTRACT) |
+			 OPCAP_BIT(IAX_OPCODE_SELECT) | OPCAP_BIT(IAX_OPCODE_RLE_BURST) |
+			 OPCAP_BIT(IAX_OPCDE_FIND_UNIQUE) | OPCAP_BIT(IAX_OPCODE_EXPAND);
+		*opcap = opcode;
+	}
+}
+
+static void vidxd_mmio_init_version(struct vdcm_idxd *vidxd)
+{
+	struct idxd_device *idxd = vidxd->idxd;
+	u32 *version;
+
+	version = (u32 *)vidxd->bar0;
+	*version = idxd->hw.version;
+}
+
+static void vidxd_mmio_reset(struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+
+	memset(bar0 + IDXD_GENCFG_OFFSET, 0, 4);
+	memset(bar0 + IDXD_GENCTRL_OFFSET, 0, 4);
+	memset(bar0 + IDXD_GENSTATS_OFFSET, 0, 4);
+	memset(bar0 + IDXD_INTCAUSE_OFFSET, 0, 4);
+	memset(bar0 + IDXD_INTCAUSE_OFFSET, 0, 4);
+	memset(bar0 + VIDXD_MSIX_PBA_OFFSET, 0, 1);
+	memset(bar0 + VIDXD_MSIX_PERM_OFFSET, 0, VIDXD_MSIX_PERM_TBL_SZ);
+
+	vidxd_mmio_init_grpcfg(vidxd);
+	vidxd_mmio_init_wqcfg(vidxd);
+}
+
+void vidxd_mmio_init(struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+	union offsets_reg *offsets;
+
+	memset(vidxd->bar0, 0, VIDXD_BAR0_SIZE);
+
+	vidxd_mmio_init_version(vidxd);
+	vidxd_mmio_init_gencap(vidxd);
+	vidxd_mmio_init_wqcap(vidxd);
+	vidxd_mmio_init_grpcap(vidxd);
+	vidxd_mmio_init_engcap(vidxd);
+	vidxd_mmio_init_opcap(vidxd);
+
+	offsets = (union offsets_reg *)(bar0 + IDXD_TABLE_OFFSET);
+	offsets->grpcfg = VIDXD_GRPCFG_OFFSET / 0x100;
+	offsets->wqcfg = VIDXD_WQCFG_OFFSET / 0x100;
+	offsets->msix_perm = VIDXD_MSIX_PERM_OFFSET / 0x100;
+
+	vidxd_mmio_init_cmdcap(vidxd);
+	memset(bar0 + VIDXD_MSIX_PERM_OFFSET, 0, VIDXD_MSIX_PERM_TBL_SZ);
+	vidxd_mmio_init_grpcfg(vidxd);
+	vidxd_mmio_init_wqcfg(vidxd);
+}
+
+static void idxd_complete_command(struct vdcm_idxd *vidxd, enum idxd_cmdsts_err val)
+{
+	u8 *bar0 = vidxd->bar0;
+	u32 *cmd = (u32 *)(bar0 + IDXD_CMD_OFFSET);
+	u32 *cmdsts = (u32 *)(bar0 + IDXD_CMDSTS_OFFSET);
+	u32 *intcause = (u32 *)(bar0 + IDXD_INTCAUSE_OFFSET);
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+
+	*cmdsts = val;
+	dev_dbg(dev, "%s: cmd: %#x  status: %#x\n", __func__, *cmd, val);
+
+	if (*cmd & IDXD_CMD_INT_MASK) {
+		*intcause |= IDXD_INTC_CMD;
+		vidxd_send_interrupt(vidxd, 0);
+	}
+}
+
+static void vidxd_enable(struct vdcm_idxd *vidxd)
+{
+	u8 *bar0 = vidxd->bar0;
+	union gensts_reg *gensts = (union gensts_reg *)(bar0 + IDXD_GENSTATS_OFFSET);
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	bool ats = (*(u16 *)&vidxd->cfg[VIDXD_ATS_OFFSET + 6]) & (1U << 15);
+	bool prs = (*(u16 *)&vidxd->cfg[VIDXD_PRS_OFFSET + 4]) & 1U;
+	bool pasid = (*(u16 *)&vidxd->cfg[VIDXD_PASID_OFFSET + 6]) & 1U;
+
+	dev_dbg(dev, "%s\n", __func__);
+	if (gensts->state == IDXD_DEVICE_STATE_ENABLED)
+		return idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_DEV_ENABLED);
+
+	/* Check PCI configuration */
+	if (!(vidxd->cfg[PCI_COMMAND] & PCI_COMMAND_MASTER))
+		return idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_BUSMASTER_EN);
+
+	if (pasid != prs || (pasid && !ats))
+		return idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_BUSMASTER_EN);
+
+	gensts->state = IDXD_DEVICE_STATE_ENABLED;
+
+	return idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_disable(struct vdcm_idxd *vidxd)
+{
+	struct idxd_wq *wq;
+	union wqcfg *wqcfg;
+	u8 *bar0 = vidxd->bar0;
+	union gensts_reg *gensts = (union gensts_reg *)(bar0 + IDXD_GENSTATS_OFFSET);
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	u32 status;
+
+	dev_dbg(dev, "%s\n", __func__);
+	if (gensts->state == IDXD_DEVICE_STATE_DISABLED) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_DIS_DEV_EN);
+		return;
+	}
+
+	wqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET);
+	wq = vidxd->wq;
+
+	/* If it is a DWQ, need to disable the DWQ as well */
+	if (wq_dedicated(wq)) {
+		idxd_wq_disable(wq, false, &status);
+		if (status) {
+			dev_warn(dev, "vidxd disable (wq disable) failed: %#x\n", status);
+			idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_DIS_DEV_EN);
+			return;
+		}
+	} else {
+		idxd_wq_drain(wq, &status);
+		if (status)
+			dev_warn(dev, "vidxd disable (wq drain) failed: %#x\n", status);
+	}
+
+	wqcfg->wq_state = 0;
+	gensts->state = IDXD_DEVICE_STATE_DISABLED;
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_drain_all(struct vdcm_idxd *vidxd)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct idxd_wq *wq = vidxd->wq;
+
+	dev_dbg(dev, "%s\n", __func__);
+
+	idxd_wq_drain(wq, NULL);
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_wq_drain(struct vdcm_idxd *vidxd, int val)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	u8 *bar0 = vidxd->bar0;
+	union wqcfg *wqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET);
+	struct idxd_wq *wq = vidxd->wq;
+	u32 status;
+
+	dev_dbg(dev, "%s\n", __func__);
+	if (wqcfg->wq_state != IDXD_WQ_DEV_ENABLED) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_WQ_NOT_EN);
+		return;
+	}
+
+	idxd_wq_drain(wq, &status);
+	if (status) {
+		dev_dbg(dev, "wq drain failed: %#x\n", status);
+		idxd_complete_command(vidxd, status);
+		return;
+	}
+
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_abort_all(struct vdcm_idxd *vidxd)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct idxd_wq *wq = vidxd->wq;
+
+	dev_dbg(dev, "%s\n", __func__);
+	if (wq_dedicated(wq))
+		idxd_wq_abort(wq, NULL);
+	else
+		idxd_wq_drain(wq, NULL);
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_wq_abort(struct vdcm_idxd *vidxd, int val)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	u8 *bar0 = vidxd->bar0;
+	union wqcfg *wqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET);
+	struct idxd_wq *wq = vidxd->wq;
+	u32 status;
+
+	dev_dbg(dev, "%s\n", __func__);
+	if (wqcfg->wq_state != IDXD_WQ_DEV_ENABLED) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_WQ_NOT_EN);
+		return;
+	}
+
+	if (wq_dedicated(wq))
+		idxd_wq_abort(wq, &status);
+	else
+		idxd_wq_drain(wq, &status);
+	if (status) {
+		dev_dbg(dev, "wq abort failed: %#x\n", status);
+		idxd_complete_command(vidxd, status);
+		return;
+	}
+
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+void vidxd_reset(struct vdcm_idxd *vidxd)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	u8 *bar0 = vidxd->bar0;
+	union gensts_reg *gensts = (union gensts_reg *)(bar0 + IDXD_GENSTATS_OFFSET);
+	struct idxd_wq *wq;
+
+	dev_dbg(dev, "%s\n", __func__);
+	gensts->state = IDXD_DEVICE_STATE_DRAIN;
+	wq = vidxd->wq;
+
+	if (wq->state == IDXD_WQ_ENABLED) {
+		if (wq_dedicated(wq)) {
+			idxd_wq_abort(wq, NULL);
+			idxd_wq_disable(wq, false, NULL);
+		} else {
+			idxd_wq_drain(wq, NULL);
+		}
+	}
+
+	vidxd_mmio_reset(vidxd);
+	gensts->state = IDXD_DEVICE_STATE_DISABLED;
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_wq_reset(struct vdcm_idxd *vidxd, int wq_id_mask)
+{
+	struct idxd_wq *wq;
+	u8 *bar0 = vidxd->bar0;
+	union wqcfg *wqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET);
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	u32 status;
+
+	wq = vidxd->wq;
+	dev_dbg(dev, "vidxd reset wq %u:%u\n", 0, wq->id);
+
+	if (wqcfg->wq_state != IDXD_WQ_DEV_ENABLED) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_WQ_NOT_EN);
+		return;
+	}
+
+	if (wq_dedicated(wq)) {
+		idxd_wq_abort(wq, &status);
+		if (status) {
+			dev_dbg(dev, "vidxd reset wq failed to abort: %#x\n", status);
+			idxd_complete_command(vidxd, status);
+			return;
+		}
+
+		idxd_wq_disable(wq, false, &status);
+		if (status) {
+			dev_dbg(dev, "vidxd reset wq failed to disable: %#x\n", status);
+			idxd_complete_command(vidxd, status);
+			return;
+		}
+	} else {
+		idxd_wq_drain(wq, &status);
+		if (status) {
+			dev_dbg(dev, "vidxd reset wq failed to drain: %#x\n", status);
+			idxd_complete_command(vidxd, status);
+			return;
+		}
+	}
+
+	wqcfg->wq_state = IDXD_WQ_DEV_DISABLED;
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_alloc_int_handle(struct vdcm_idxd *vidxd, int operand)
+{
+	bool ims = !!(operand & CMD_INT_HANDLE_IMS);
+	u32 cmdsts;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	int ims_idx, vidx;
+
+	vidx = operand & GENMASK(15, 0);
+
+	dev_dbg(dev, "allocating int handle for %d\n", vidx);
+
+	/* vidx cannot be 0 since that's emulated and does not require IMS handle */
+	if (vidx <= 0 || vidx >= VIDXD_MAX_MSIX_VECS) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_INVAL_INT_IDX);
+		return;
+	}
+
+	if (ims) {
+		dev_warn(dev, "IMS allocation is not implemented yet\n");
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_NO_HANDLE);
+		return;
+	}
+
+	ims_idx = dev_msi_hwirq(dev, vidx - 1);
+	cmdsts = ims_idx << IDXD_CMDSTS_RES_SHIFT;
+	dev_dbg(dev, "requested index %d handle %d\n", vidx, ims_idx);
+	idxd_complete_command(vidxd, cmdsts);
+}
+
+static void vidxd_revoked_handles_processed (struct vdcm_idxd *vidxd,
+		int operand)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct idxd_virtual_wq *vwq = &vidxd->vwq;
+	int idx;
+	u32 status;
+
+        printk("completed revoked int handle\n");
+
+	idxd_complete_command(vidxd, 0);
+
+	BUG_ON(!list_empty(&vwq->head));
+
+	/* Step 1. Drain all the WQs associated with this VM. Currently only 1 */
+	idxd_wq_drain(vidxd->wq, &status);
+
+	if (status)
+		dev_dbg(dev, "wq drain failed: %#x\n", status);
+
+	/* Step 2. Generate a completion interrupt for all int handles */
+	for (idx = 1; idx < VIDXD_MAX_MSIX_VECS; idx++) {
+		dev_dbg(dev, "revoked int handle processed idx %d\n", idx);
+		vidxd_send_interrupt(vidxd, idx);
+	}
+}
+
+static void vidxd_release_int_handle(struct vdcm_idxd *vidxd, int operand)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	bool ims = !!(operand & CMD_INT_HANDLE_IMS);
+	int handle, i;
+	bool found = false;
+
+	handle = operand & GENMASK(15, 0);
+	dev_dbg(dev, "allocating int handle %d\n", handle);
+
+	if (ims) {
+		dev_warn(dev, "IMS allocation is not implemented yet\n");
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_INVAL_INT_IDX_RELEASE);
+		return;
+	}
+
+	/* IMS backed entry start at 1, 0 is emulated vector */
+	for (i = 0; i < VIDXD_MAX_MSIX_VECS - 1; i++) {
+		if (dev_msi_hwirq(dev, i) == handle) {
+			found = true;
+			break;
+		}
+	}
+
+	if (!found) {
+		dev_warn(dev, "Freeing unallocated int handle.\n");
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_INVAL_INT_IDX_RELEASE);
+	}
+
+	dev_dbg(dev, "int handle %d released.\n", handle);
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_wq_enable(struct vdcm_idxd *vidxd, int wq_id)
+{
+	struct idxd_wq *wq;
+	u8 *bar0 = vidxd->bar0;
+	union wq_cap_reg *wqcap;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	struct idxd_device *idxd;
+	union wqcfg *vwqcfg, *wqcfg;
+	int rc;
+	bool wq_pasid_enable;
+	bool pasid_enabled = (*(u16 *)&vidxd->cfg[VIDXD_PASID_OFFSET + 6]) & 1U;
+
+	if (wq_id >= VIDXD_MAX_WQS) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_INVAL_WQIDX);
+		return;
+	}
+
+	idxd = vidxd->idxd;
+	wq = vidxd->wq;
+
+	dev_dbg(dev, "%s: wq %u:%u\n", __func__, wq_id, wq->id);
+
+	vwqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET + wq_id * 32);
+	wqcap = (union wq_cap_reg *)(bar0 + IDXD_WQCAP_OFFSET);
+	wqcfg = wq->wqcfg;
+
+	if (vidxd_state(vidxd) != IDXD_DEVICE_STATE_ENABLED) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_DEV_NOTEN);
+		return;
+	}
+
+	if (vwqcfg->wq_state != IDXD_WQ_DEV_DISABLED) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_WQ_ENABLED);
+		return;
+	}
+
+	if ((!wq_dedicated(wq) && wqcap->shared_mode == 0) ||
+	    (wq_dedicated(wq) && wqcap->dedicated_mode == 0)) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_WQ_MODE);
+		return;
+	}
+
+	if ((!wq_dedicated(wq) && vwqcfg->pasid_en == 0) ||
+	    (vwqcfg->pasid_en && pasid_enabled == 0)) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_PASID_EN);
+		return;
+	}
+
+	wq_pasid_enable = vwqcfg->pasid_en;
+
+	if (wq_dedicated(wq)) {
+		u32 wq_pasid;
+		bool priv;
+
+		if (wq_pasid_enable) {
+			u32 gpasid;
+
+			priv = vwqcfg->priv;
+			gpasid = vwqcfg->pasid;
+			rc = idxd_mdev_get_host_pasid(mdev, gpasid, &wq_pasid);
+		} else {
+			priv = 1;
+			rc = idxd_mdev_get_pasid(mdev, &wq_pasid);
+		}
+		if (rc < 0) {
+			dev_err(dev, "idxd pasid setup failed wq %d: %d\n", wq->id, rc);
+			idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_PASID_EN);
+			return;
+		}
+
+		if (wq_pasid >= 0) {
+			u32 status;
+			unsigned long flags;
+
+			wqcfg->bits[WQCFG_PASID_IDX] &= ~GENMASK(29, 8);
+			wqcfg->priv = priv;
+			wqcfg->pasid_en = 1;
+			wqcfg->pasid = wq_pasid;
+			dev_dbg(dev, "program pasid %d in wq %d\n", wq_pasid, wq->id);
+			spin_lock_irqsave(&idxd->dev_lock, flags);
+			idxd_wq_setup_pasid(wq, wq_pasid);
+			idxd_wq_setup_priv(wq, priv);
+			spin_unlock_irqrestore(&idxd->dev_lock, flags);
+			idxd_wq_enable(wq, &status);
+			if (status) {
+				dev_err(dev, "vidxd enable wq %d failed\n", wq->id);
+				idxd_complete_command(vidxd, status);
+				return;
+			}
+		} else {
+			dev_err(dev, "idxd pasid setup failed wq %d wq_pasid %d\n",
+				wq->id, wq_pasid);
+			idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_PASID_EN);
+			return;
+		}
+	}
+
+	vwqcfg->wq_state = IDXD_WQ_DEV_ENABLED;
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+static void vidxd_wq_disable(struct vdcm_idxd *vidxd, int wq_id_mask)
+{
+	struct idxd_wq *wq;
+	union wqcfg *wqcfg, *vwqcfg;
+	u8 *bar0 = vidxd->bar0;
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+	u32 status;
+
+	wq = vidxd->wq;
+
+	dev_dbg(dev, "vidxd disable wq %u:%u\n", 0, wq->id);
+
+	wqcfg = wq->wqcfg;
+	vwqcfg = (union wqcfg *)(bar0 + VIDXD_WQCFG_OFFSET);
+	if (vwqcfg->wq_state != IDXD_WQ_DEV_ENABLED) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_ERR_WQ_NOT_EN);
+		return;
+	}
+
+	/* If it is a DWQ, need to disable the DWQ as well */
+	if (wq_dedicated(wq)) {
+		struct ioasid_set *ioasid_set;
+		struct mm_struct *mm;
+
+		idxd_wq_disable(wq, false, &status);
+		if (status) {
+			dev_warn(dev, "vidxd disable wq failed: %#x\n", status);
+			idxd_complete_command(vidxd, status);
+			return;
+		}
+
+		if (vwqcfg->pasid_en) {
+			mm = get_task_mm(current);
+			if (!mm) {
+				dev_dbg(dev, "Can't retrieve task mm\n");
+				return;
+			}
+
+			ioasid_set = ioasid_find_mm_set(mm);
+			if (!ioasid_set) {
+				dev_dbg(dev, "Unable to find ioasid_set\n");
+				mmput(mm);
+				return;
+			}
+			mmput(mm);
+			if (!ioasid_put(ioasid_set, wqcfg->pasid))
+				dev_warn(dev, "Unable to put ioasid\n");
+		}
+	} else {
+		idxd_wq_drain(wq, &status);
+		if (status) {
+			dev_warn(dev, "vidxd disable drain wq failed: %#x\n", status);
+			idxd_complete_command(vidxd, status);
+			return;
+		}
+	}
+
+	vwqcfg->wq_state = IDXD_WQ_DEV_DISABLED;
+	idxd_complete_command(vidxd, IDXD_CMDSTS_SUCCESS);
+}
+
+void vidxd_free_ims_entries(struct vdcm_idxd *vidxd)
+{
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+
+	msi_domain_free_irqs(dev_get_msi_domain(dev), dev);
+}
+
+static bool command_supported(struct vdcm_idxd *vidxd, u32 cmd)
+{
+	u8 *bar0 = vidxd->bar0;
+	u32 *cmd_cap = (u32 *)(bar0 + IDXD_CMDCAP_OFFSET);
+
+	return !!(*cmd_cap & BIT(cmd));
+}
+
+static void vidxd_do_command(struct vdcm_idxd *vidxd, u32 val)
+{
+	union idxd_command_reg *reg = (union idxd_command_reg *)(vidxd->bar0 + IDXD_CMD_OFFSET);
+	struct mdev_device *mdev = vidxd->ivdev.mdev;
+	struct device *dev = mdev_dev(mdev);
+
+	reg->bits = val;
+
+	dev_dbg(dev, "%s: cmd code: %u reg: %x\n", __func__, reg->cmd, reg->bits);
+
+	if (!command_supported(vidxd, reg->cmd)) {
+		idxd_complete_command(vidxd, IDXD_CMDSTS_INVAL_CMD);
+		return;
+	}
+
+	switch (reg->cmd) {
+	case IDXD_CMD_ENABLE_DEVICE:
+		vidxd_enable(vidxd);
+		break;
+	case IDXD_CMD_DISABLE_DEVICE:
+		vidxd_disable(vidxd);
+		break;
+	case IDXD_CMD_DRAIN_ALL:
+		vidxd_drain_all(vidxd);
+		break;
+	case IDXD_CMD_ABORT_ALL:
+		vidxd_abort_all(vidxd);
+		break;
+	case IDXD_CMD_RESET_DEVICE:
+		vidxd_reset(vidxd);
+		break;
+	case IDXD_CMD_ENABLE_WQ:
+		vidxd_wq_enable(vidxd, reg->operand);
+		break;
+	case IDXD_CMD_DISABLE_WQ:
+		vidxd_wq_disable(vidxd, reg->operand);
+		break;
+	case IDXD_CMD_DRAIN_WQ:
+		vidxd_wq_drain(vidxd, reg->operand);
+		break;
+	case IDXD_CMD_ABORT_WQ:
+		vidxd_wq_abort(vidxd, reg->operand);
+		break;
+	case IDXD_CMD_RESET_WQ:
+		vidxd_wq_reset(vidxd, reg->operand);
+		break;
+	case IDXD_CMD_REQUEST_INT_HANDLE:
+		vidxd_alloc_int_handle(vidxd, reg->operand);
+		break;
+	case IDXD_CMD_RELEASE_INT_HANDLE:
+		vidxd_release_int_handle(vidxd, reg->operand);
+		break;
+	case IDXD_CMD_REVOKED_HANDLES_PROCESSED:
+		vidxd_revoked_handles_processed(vidxd, reg->operand);
+		break;
+	default:
+		idxd_complete_command(vidxd, IDXD_CMDSTS_INVAL_CMD);
+		break;
+	}
+}
+
+static void vidxd_send_errors(struct vdcm_idxd *vidxd)
+{
+	struct idxd_device *idxd = vidxd->idxd;
+	u8 *bar0 = vidxd->bar0;
+	union sw_err_reg *swerr = (union sw_err_reg *)(bar0 + IDXD_SWERR_OFFSET);
+	union genctrl_reg *genctrl = (union genctrl_reg *)(bar0 + IDXD_GENCTRL_OFFSET);
+	u32 *intcause = (u32 *)(bar0 + IDXD_INTCAUSE_OFFSET);
+	int i;
+
+	lockdep_assert_held(&idxd->dev_lock);
+
+	if (swerr->valid) {
+		if (!swerr->overflow)
+			swerr->overflow = 1;
+		return;
+	}
+
+	for (i = 0; i < 4; i++)
+		swerr->bits[i] = idxd->sw_err.bits[i];
+
+	*intcause |= IDXD_INTC_ERR;
+	if (genctrl->softerr_int_en)
+		vidxd_send_interrupt(vidxd, 0);
+}
+
+void idxd_wq_vidxd_send_errors(struct idxd_wq *wq)
+{
+	struct vdcm_idxd *vidxd;
+
+	list_for_each_entry(vidxd, &wq->vdcm_list, list)
+		vidxd_send_errors(vidxd);
+}
diff --git a/include/uapi/linux/idxd.h b/include/uapi/linux/idxd.h
index bce7c43657d5..19e4e532918f 100644
--- a/include/uapi/linux/idxd.h
+++ b/include/uapi/linux/idxd.h
@@ -131,6 +131,8 @@ enum dsa_completion_status {
 	DSA_COMP_HW_ERR1,
 	DSA_COMP_HW_ERR_DRB,
 	DSA_COMP_TRANSLATION_FAIL,
+	DSA_ERR_PCI_CFG = 0x51,
+	DSA_ERR_CMD_REG,
 };
 
 enum iax_completion_status {
-- 
2.31.1

